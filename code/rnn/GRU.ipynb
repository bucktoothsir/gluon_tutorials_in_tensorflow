{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 门控循环单元（GRU）— 从0开始\n",
    "上一节中，我们介绍了循环神经网络中的梯度计算方法。我们发现，循环神经网络的隐含层变量梯度可能会出现衰减或爆炸。虽然梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。因此，给定一个时间序列，例如文本序列，循环神经网络在实际中其实较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。\n",
    "\n",
    "门控循环神经网络（gated recurrent neural networks）的提出，是为了更好地捕捉时序数据中间隔较大的依赖关系。其中，门控循环单元（gated recurrent unit，简称GRU）是一种常用的门控循环神经网络。它由Cho、van Merrienboer、 Bahdanau和Bengio在2014年被提出。\n",
    "\n",
    "### 门控循环单元\n",
    "我们先介绍门控循环单元的构造。它比循环神经网络中的隐含层构造稍复杂一点。\n",
    "\n",
    "### 重置门和更新门\n",
    "门控循环单元的隐含状态只包含隐含层变量$H$。假定隐含状态长度为$h$，给定时刻t的一个样本数为$n$特征向量维度为$x$的批量数据$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$和上一时刻隐含状态$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$，重置门（reset gate）$\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$和更新门（update gate）$\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$的定义如下：\n",
    "\n",
    "$\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\mathbf{b}_r)$\n",
    "\n",
    "$\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\mathbf{b}_z)$\n",
    "\n",
    "其中的$\\mathbf{W}_{xr}, \\mathbf{W}_{xz} \\in \\mathbb{R}^{x \\times h}$,和$\\mathbf{W}_{hr}, \\mathbf{W}_{hz} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。函数$\\sigma$自变量中的三项相加使用了广播。\n",
    "\n",
    "需要注意的是，重置门和更新门使用了值域为$[0,1]$的函数$\\sigma(x) = 1/(1+\\text{exp}(-x))$。因此，重置门$\\mathbf{R}_t$和更新门$\\mathbf{Z}_t$中每个元素的值域都是$[0,1]$。\n",
    "\n",
    "### 候选隐含状态\n",
    "我们可以通过元素值域在$[0,1]$的更新门和重置门来控制隐含状态中信息的流动：这通常可以应用按元素乘法符$\\odot$。门控循环单元中的候选隐含状态$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$使用了值域在$[−1,1]$的双曲正切函数$\\tanh$做激活函数：\n",
    "\n",
    "$\\tilde{\\mathbf{H}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xh} + \\mathbf{R}_t \\odot \\mathbf{H}_{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h)$\n",
    "\n",
    "其中的$\\mathbf{W}_{xh} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hh} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。\n",
    "\n",
    "需要注意的是，候选隐含状态使用了重置门来控制包含过去时刻信息的上一个隐含状态的流入。如果重置门近似0，上一个隐含状态将被丢弃。因此，重置门提供了丢弃与未来无关的过去隐含状态的机制。\n",
    "\n",
    "### 隐含状态\n",
    "隐含状态$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$的计算使用更新门$\\mathbf{Z}_t$来对上一时刻的隐含状态$\\mathbf{H}_{t-1}$和当前时刻的候选隐含状态$\\tilde{\\mathbf{H}}_t$做组合，公式如下：\n",
    "\n",
    "$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t$\n",
    "\n",
    "需要注意的是，更新门可以控制过去的隐含状态在当前时刻的重要性。如果更新门一直近似1，过去的隐含状态将一直通过时间保存并传递至当前时刻。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "\n",
    "我们对门控循环单元的设计稍作总结：\n",
    "\n",
    "重置门有助于捕捉时序数据中短期的依赖关系。\n",
    "更新门有助于捕捉时序数据中长期的依赖关系。\n",
    "输出层的设计可参照循环神经网络中的描述。\n",
    "\n",
    "### 实验\n",
    "为了实现并展示门控循环单元，我们依然使用周杰伦歌词数据集来训练模型作词。这里除门控循环单元以外的实现已在循环神经网络中介绍。\n",
    "\n",
    "### 数据处理\n",
    "我们先读取并对数据集做简单处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vocab size:', 1465)\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('../../data/')\n",
    "\n",
    "with open('../../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read().decode('utf-8')\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]\n",
    "\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用onehot来将字符索引表示成向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot(positions, vocal_size):\n",
    "    if len(positions.shape) == 0:\n",
    "        positions = np.expand_dims(positions, 0)\n",
    "        positions = np.expand_dims(positions, 1)\n",
    "    if len(positions.shape) == 1:\n",
    "        positions = np.expand_dims(positions, 1)\n",
    "    all_zeros = np.zeros((positions.shape[0], vocab_size))\n",
    "    for i in xrange(positions.shape[0]):\n",
    "        for j in xrange(positions.shape[1]):\n",
    "            all_zeros[i, int(positions[i, j])] = 1\n",
    "        \n",
    "    return all_zeros\n",
    "        \n",
    "one_hot(np.array([0, 2]), vocab_size)\n",
    "\n",
    "def get_inputs(data):\n",
    "    return [one_hot(X, vocab_size) for X in data.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "以下部分对模型参数进行初始化。参数hidden_dim定义了隐含状态的长度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_dim = vocab_size\n",
    "# 隐含状态长度\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "weight_scale = .01\n",
    "\n",
    "\n",
    "with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "    W_xz = tf.get_variable(name='weights_xz', shape=[input_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    W_hz = tf.get_variable(name='weights_hz', shape=[hidden_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_z = tf.get_variable(name='bias_z', shape=[hidden_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "\n",
    "    W_xr = tf.get_variable(name='weights_xr', shape=[input_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    W_hr = tf.get_variable(name='weights_hr', shape=[hidden_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_z = tf.get_variable(name='bias_r', shape=[hidden_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    \n",
    "    W_xh = tf.get_variable(name='weights_xh', shape=[input_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    W_hh = tf.get_variable(name='weights_hh', shape=[hidden_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_h = tf.get_variable(name='bias_h', shape=[hidden_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    \n",
    "\n",
    "    W_hy = tf.get_variable(name='weights_output', shape=[hidden_dim, output_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_y = tf.get_variable(name='bias_output', shape=[output_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "\n",
    "\n",
    "params = [W_xz, W_hz, b_z, W_xr, W_hr, b_z, W_xh, W_hh, b_h, W_hy, b_y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "我们将前面的模型公式翻译成代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_rnn(inputs, H, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hy, b_y = params[0]\n",
    "    outputs = []\n",
    "    num_steps = inputs.get_shape().as_list()[0]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        X = inputs[i]\n",
    "        Z = tf.nn.sigmoid(tf.matmul(X, W_xz) + tf.matmul(H, W_hz) + b_z)\n",
    "        R = tf.nn.sigmoid(tf.matmul(X, W_xr) + tf.matmul(H, W_hr) + b_r)\n",
    "        H_tilda = tf.nn.tanh(tf.matmul(X, W_xh) + R * tf.matmul(H, W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda \n",
    "        Y = tf.matmul(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "下面我们开始训练模型。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。这里采用的是相邻批量采样实验门控循环单元谱写歌词。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps):\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    # 随机化样本\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回num_steps个数据\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        data = np.array(\n",
    "            [_data(j * num_steps) for j in batch_indices])\n",
    "        label = np.array(\n",
    "            [_data(j * num_steps + 1) for j in batch_indices])\n",
    "        yield data.astype(np.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    corpus_indices = np.array((corpus_indices))\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    indices = corpus_indices[0: batch_size * batch_len].reshape((\n",
    "        batch_size, batch_len))\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        i = i * num_steps\n",
    "        data = indices[:, i: i + num_steps]\n",
    "        label = indices[:, i + 1: i + num_steps + 1]\n",
    "        yield data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "rnn/weights_xz\n",
      "rnn/weights_hz\n",
      "rnn/bias_z\n",
      "rnn/weights_xr\n",
      "rnn/weights_hr\n",
      "rnn/bias_r\n",
      "rnn/weights_xh\n",
      "rnn/weights_hh\n",
      "rnn/bias_h\n",
      "rnn/weights_output\n",
      "rnn/bias_output\n",
      "1464.9376\n",
      "1439.6514\n",
      "1249.856\n",
      "649.8489\n",
      "652.86163\n",
      "511.26956\n",
      "649.5704\n",
      "549.7654\n",
      "530.80414\n",
      "418.92908\n",
      "443.1054\n",
      "455.75653\n",
      "436.66254\n",
      "416.843\n",
      "339.69907\n",
      "390.52524\n",
      "495.65125\n",
      "340.27444\n",
      "275.5851\n",
      "306.25516\n",
      "295.50293\n",
      "294.7655\n",
      "283.7825\n",
      "306.8359\n",
      "283.94656\n",
      "302.96762\n",
      "292.34683\n",
      "272.07056\n",
      "231.1862\n",
      "272.39\n",
      "266.0971\n",
      "222.43002\n",
      "245.96857\n",
      "224.33475\n",
      "164.07266\n",
      "155.38518\n",
      "164.51215\n",
      "166.97023\n",
      "135.26433\n",
      "135.62885\n",
      "144.93999\n",
      "156.54453\n",
      "145.53488\n",
      "141.26868\n",
      "116.46877\n",
      "122.53087\n",
      "137.13354\n",
      "110.45613\n",
      "90.34166\n",
      "113.38717\n",
      "94.81563\n",
      "79.833435\n",
      "62.691242\n",
      "71.12315\n",
      "57.73551\n",
      "53.634052\n",
      "52.863514\n",
      "58.326344\n",
      "57.293995\n",
      "48.356567\n",
      "46.911655\n",
      "34.347435\n",
      "44.95937\n",
      "46.96556\n",
      "40.04711\n",
      "40.3418\n",
      "57.008533\n",
      "40.487366\n",
      "26.308485\n",
      "28.486073\n",
      "28.07149\n",
      "19.740408\n",
      "25.616179\n",
      "21.053658\n",
      "20.104101\n",
      "23.428736\n",
      "19.016323\n",
      "14.829099\n",
      "21.882864\n",
      "22.722208\n",
      "13.322555\n",
      "17.549547\n",
      "22.380533\n",
      "20.338774\n",
      "16.782362\n",
      "6.9460087\n",
      "10.311589\n",
      "13.276394\n",
      "10.924375\n",
      "9.539542\n",
      "9.723417\n",
      "11.965799\n",
      "11.989914\n",
      "11.079608\n",
      "11.734436\n",
      "10.104621\n",
      "9.132162\n",
      "11.655289\n",
      "9.152366\n",
      "8.841994\n",
      "7.413091\n",
      "7.648497\n",
      "4.306487\n",
      "5.5429\n",
      "4.674918\n",
      "5.321325\n",
      "6.106869\n",
      "5.7502112\n",
      "5.312621\n",
      "5.816897\n",
      "4.711344\n",
      "4.692552\n",
      "5.682086\n",
      "4.7587123\n",
      "6.7653546\n",
      "5.594224\n",
      "4.505258\n",
      "4.421305\n",
      "4.5886064\n",
      "3.6090312\n",
      "3.4091105\n",
      "2.8327906\n",
      "3.4301326\n",
      "3.7196045\n",
      "3.7024655\n",
      "2.7026055\n",
      "3.6303105\n",
      "3.2259321\n",
      "2.963186\n",
      "2.8127878\n",
      "3.3479438\n",
      "3.7545676\n",
      "2.2528508\n",
      "3.503127\n",
      "2.6462836\n",
      "3.0340116\n",
      "2.283304\n",
      "2.299787\n",
      "2.095093\n",
      "2.1469042\n",
      "2.1284473\n",
      "2.0031574\n",
      "2.191079\n",
      "2.1850035\n",
      "2.4467468\n",
      "2.3383296\n",
      "2.1265016\n",
      "2.0372984\n",
      "2.3719804\n",
      "2.1014493\n",
      "2.0779793\n",
      "2.202065\n",
      "2.6848514\n",
      "1.5600936\n",
      "1.7092818\n",
      "1.7841113\n",
      "1.5968939\n",
      "1.6638592\n",
      "2.0010777\n",
      "1.7217791\n",
      "1.6656927\n",
      "1.6475664\n",
      "1.7733389\n",
      "1.7957488\n",
      "1.5651722\n",
      "1.8102183\n",
      "1.6986411\n",
      "1.8794363\n",
      "2.1538465\n",
      "1.6593891\n",
      "1.5546829\n",
      "1.3765949\n",
      "1.4215251\n",
      "1.4890281\n",
      "1.5054616\n",
      "1.5332634\n",
      "1.5139217\n",
      "1.6846634\n",
      "1.4345611\n",
      "1.4834824\n",
      "1.3895817\n",
      "1.3826497\n",
      "1.4990022\n",
      "1.4335502\n",
      "1.4581473\n",
      "1.4877851\n",
      "1.5089595\n",
      "1.2841046\n",
      "1.3588533\n",
      "1.2918122\n",
      "1.3396184\n",
      "1.2716631\n",
      "1.3186281\n",
      "1.3476472\n",
      "1.3073943\n",
      "1.3168154\n",
      "1.3072845\n",
      "1.278862\n",
      "1.2876105\n",
      "1.3265201\n",
      "1.3331126\n",
      "1.3005193\n",
      "1.4994506\n",
      "1.365188\n",
      "1.2305744\n",
      "1.2514448\n",
      "1.2129406\n",
      "1.209123\n",
      "1.2813616\n",
      "1.213418\n",
      "1.2873288\n",
      "1.208026\n",
      "1.2534083\n",
      "1.2081182\n",
      "1.2296591\n",
      "1.2199322\n",
      "1.1648487\n",
      "1.2479775\n",
      "1.2614474\n",
      "1.2892638\n",
      "1.298985\n",
      "1.1561531\n",
      "1.1616186\n",
      "1.1911479\n",
      "1.1590122\n",
      "1.1930851\n",
      "1.1962903\n",
      "1.1344632\n",
      "1.1659616\n",
      "1.2134653\n",
      "1.224457\n",
      "1.2275838\n",
      "1.1962249\n",
      "1.1628219\n",
      "1.1581824\n",
      "1.2080384\n",
      "1.2192253\n",
      "1.1768997\n",
      "1.1442842\n",
      "1.1399624\n",
      "1.14243\n",
      "1.1651351\n",
      "1.1228815\n",
      "1.1152884\n",
      "1.1401459\n",
      "1.153378\n",
      "1.1553199\n",
      "1.2029408\n",
      "1.1501273\n",
      "1.1209602\n",
      "1.1882459\n",
      "1.1708031\n",
      "1.1388133\n",
      "1.147303\n",
      "1.1914765\n",
      "1.1109681\n",
      "1.1327617\n",
      "1.126681\n",
      "1.1049188\n",
      "1.1221421\n",
      "1.1294947\n",
      "1.1115271\n",
      "1.1157311\n",
      "1.1145467\n",
      "1.1591161\n",
      "1.1124524\n",
      "1.1275699\n",
      "1.1401157\n",
      "1.1292851\n",
      "1.1301693\n",
      "1.1245238\n",
      "1.132837\n",
      "1.1296067\n",
      "1.0880152\n",
      "1.1231743\n",
      "1.0788524\n",
      "1.0885736\n",
      "1.100034\n",
      "1.1158196\n",
      "1.1067007\n",
      "1.1009436\n",
      "1.107904\n",
      "1.0959876\n",
      "1.1347523\n",
      "1.1187356\n",
      "1.117358\n",
      "1.1352413\n",
      "1.1062329\n",
      "1.1206534\n",
      "1.080085\n",
      "1.1077641\n",
      "1.0682119\n",
      "1.1046572\n",
      "1.086793\n",
      "1.1166161\n",
      "1.1086144\n",
      "1.1208974\n",
      "1.1046959\n",
      "1.0968312\n",
      "1.0990999\n",
      "1.1008435\n",
      "1.1283547\n",
      "1.1035165\n",
      "1.1085355\n",
      "1.0883303\n",
      "1.0773497\n",
      "1.0881996\n",
      "1.088361\n",
      "1.1054041\n",
      "1.071149\n",
      "1.086624\n",
      "1.071893\n",
      "1.0664895\n",
      "1.1086379\n",
      "1.0735687\n",
      "1.0930656\n",
      "1.0836858\n",
      "1.074348\n",
      "1.1096907\n",
      "1.1039909\n",
      "1.1084018\n",
      "1.1092225\n",
      "1.0865629\n",
      "1.0760126\n",
      "1.0821183\n",
      "1.0798652\n",
      "1.0634481\n",
      "1.0843986\n",
      "1.0794789\n",
      "1.0828935\n",
      "1.0771648\n",
      "1.1034173\n",
      "1.0619276\n",
      "1.0863761\n",
      "1.0781118\n",
      "1.0958132\n",
      "1.097596\n",
      "1.0967702\n",
      "1.0754515\n",
      "1.1171299\n",
      "1.0712448\n",
      "1.0827873\n",
      "1.0584061\n",
      "1.0590544\n",
      "1.0703806\n",
      "1.0806022\n",
      "1.065485\n",
      "1.0696734\n",
      "1.0794765\n",
      "1.0786358\n",
      "1.0705334\n",
      "1.0868062\n",
      "1.1190592\n",
      "1.0875534\n",
      "1.0902029\n",
      "1.1143984\n",
      "1.1041052\n",
      "1.0800648\n",
      "1.0872743\n",
      "1.0611367\n",
      "1.0706692\n",
      "1.061479\n",
      "1.0927964\n",
      "1.0807875\n",
      "1.0887525\n",
      "1.0814422\n",
      "1.06926\n",
      "1.0740225\n",
      "1.0769455\n",
      "1.0948339\n",
      "1.1055493\n",
      "1.0926471\n",
      "1.0920771\n",
      "1.0700793\n",
      "1.0546244\n",
      "1.0579499\n",
      "1.0594028\n",
      "1.0752372\n",
      "1.0821327\n",
      "1.0771474\n",
      "1.062475\n",
      "1.0641809\n",
      "1.0754867\n",
      "1.0696323\n",
      "1.0663059\n",
      "1.0849555\n",
      "1.0969809\n",
      "1.0819473\n",
      "1.0903878\n",
      "1.1038661\n",
      "1.1288015\n",
      "1.0692539\n",
      "1.0481843\n",
      "1.066479\n",
      "1.085481\n",
      "1.0671779\n",
      "1.0642649\n",
      "1.0736965\n",
      "1.0860945\n",
      "1.0777426\n",
      "1.0949496\n",
      "1.0734575\n",
      "1.0772809\n",
      "1.1123929\n",
      "1.0682573\n",
      "1.1095865\n",
      "1.0787436\n",
      "1.080924\n",
      "1.092055\n",
      "1.063227\n",
      "1.059684\n",
      "1.0546066\n",
      "1.0614624\n",
      "1.0521168\n",
      "1.0944477\n",
      "1.0647846\n",
      "1.0570316\n",
      "1.069432\n",
      "1.0783167\n",
      "1.074204\n",
      "1.0639035\n",
      "1.0820836\n",
      "1.0842913\n",
      "1.0685252\n",
      "1.0738859\n",
      "1.0708604\n",
      "1.054654\n",
      "1.057082\n",
      "1.0418767\n",
      "1.0591785\n",
      "1.0609199\n",
      "1.0741968\n",
      "1.0654482\n",
      "1.0735788\n",
      "1.0853108\n",
      "1.0637693\n",
      "1.0643765\n",
      "1.1086357\n",
      "1.0527514\n",
      "1.0709141\n",
      "1.0750343\n",
      "1.0679389\n",
      "1.0721357\n",
      "1.066995\n",
      "1.0569094\n",
      "1.0490503\n",
      "1.0776116\n",
      "1.0686107\n",
      "1.0732479\n",
      "1.0669136\n",
      "1.0540642\n",
      "1.0742509\n",
      "1.0521103\n",
      "1.0733484\n",
      "1.0632701\n",
      "1.0657457\n",
      "1.0683639\n",
      "1.0792885\n",
      "1.0837843\n",
      "1.0564625\n",
      "1.0569781\n",
      "1.0518442\n",
      "1.0569763\n",
      "1.0879713\n",
      "1.0464824\n",
      "1.073867\n",
      "1.0769298\n",
      "1.0672529\n",
      "1.0581306\n",
      "1.0547459\n",
      "1.0477741\n",
      "1.0720061\n",
      "1.050439\n",
      "1.0731467\n",
      "1.0742551\n",
      "1.0954554\n",
      "1.0695623\n",
      "1.0543894\n",
      "1.0360438\n",
      "1.0651714\n",
      "1.0458789\n",
      "1.0501766\n",
      "1.0537276\n",
      "1.0745411\n",
      "1.0597814\n",
      "1.0650604\n",
      "1.0742882\n",
      "1.0706016\n",
      "1.0557686\n",
      "1.04086\n",
      "1.0702695\n",
      "1.0696301\n",
      "1.0882884\n",
      "1.0780952\n",
      "1.047142\n",
      "1.0548313\n",
      "1.0650007\n",
      "1.0580082\n",
      "1.0559028\n",
      "1.0671854\n",
      "1.053803\n",
      "1.0737385\n",
      "1.0527982\n",
      "1.0584673\n",
      "1.0502983\n",
      "1.0822144\n",
      "1.031323\n",
      "1.0552844\n",
      "1.0471565\n",
      "1.0781412\n",
      "1.0577544\n",
      "1.0415517\n",
      "1.0479861\n",
      "1.0426676\n",
      "1.0437814\n",
      "1.0540967\n",
      "1.0429504\n",
      "1.0853062\n",
      "1.0533177\n",
      "1.0531781\n",
      "1.0564951\n",
      "1.068899\n",
      "1.0859035\n",
      "1.0694735\n",
      "1.0561552\n",
      "1.0724217\n",
      "1.0642681\n",
      "1.045388\n",
      "1.0266864\n",
      "1.0528591\n",
      "1.0542585\n",
      "1.0522997\n",
      "1.060249\n",
      "1.0615261\n",
      "1.0441749\n",
      "1.0737712\n",
      "1.0742087\n",
      "1.068913\n",
      "1.0692947\n",
      "1.0454267\n",
      "1.0411803\n",
      "1.0551634\n",
      "1.0627432\n",
      "1.0644275\n",
      "1.050989\n",
      "1.0449823\n",
      "1.0618271\n",
      "1.0633361\n",
      "1.0629783\n",
      "1.0595658\n",
      "1.0403504\n",
      "1.0373468\n",
      "1.0501575\n",
      "1.058397\n",
      "1.0404016\n",
      "1.0547141\n",
      "1.0729373\n",
      "1.0421916\n",
      "1.0796528\n",
      "1.0437534\n",
      "1.0534958\n",
      "1.0439305\n",
      "1.0353022\n",
      "1.0481015\n",
      "1.0480064\n",
      "1.0509136\n",
      "1.0517915\n",
      "1.0495654\n",
      "1.0543674\n",
      "1.0542456\n",
      "1.0471723\n",
      "1.0571535\n",
      "1.0659217\n",
      "1.0527065\n",
      "1.0543056\n",
      "1.0544283\n",
      "1.0605968\n",
      "1.0608749\n",
      "1.0470126\n",
      "1.0413247\n",
      "1.0432103\n",
      "1.0408889\n",
      "1.0474634\n",
      "1.0545163\n",
      "1.034924\n",
      "1.0586687\n",
      "1.049331\n",
      "1.0607895\n",
      "1.0473404\n",
      "1.0525092\n",
      "1.0574967\n",
      "1.0441517\n",
      "1.052342\n",
      "1.07601\n",
      "1.0577025\n",
      "1.050688\n",
      "1.0403231\n",
      "1.0667121\n",
      "1.0560547\n",
      "1.0443399\n",
      "1.0524187\n",
      "1.0333378\n",
      "1.053325\n",
      "1.0527388\n",
      "1.0416993\n",
      "1.048801\n",
      "1.0528378\n",
      "1.0517727\n",
      "1.0633614\n",
      "1.0350035\n",
      "1.0431981\n",
      "1.0591297\n",
      "1.0444573\n",
      "1.045006\n",
      "1.0337551\n",
      "1.0448208\n",
      "1.0518142\n",
      "1.0472041\n",
      "1.0600659\n",
      "1.0550536\n",
      "1.0367233\n",
      "1.0512589\n",
      "1.0521457\n",
      "1.061116\n",
      "1.0553253\n",
      "1.0539843\n",
      "1.0422678\n",
      "1.0645028\n",
      "1.0515138\n",
      "1.0431876\n",
      "1.0463463\n",
      "1.0279635\n",
      "1.0622231\n",
      "1.0442284\n",
      "1.0584408\n",
      "1.0418385\n",
      "1.0464735\n",
      "1.0522602\n",
      "1.0476639\n",
      "1.0423315\n",
      "1.0566444\n",
      "1.0439856\n",
      "1.0570801\n",
      "1.0656494\n",
      "1.0647132\n",
      "1.0482565\n",
      "1.0458158\n",
      "1.0376053\n",
      "1.0449661\n",
      "1.033749\n",
      "1.0475196\n",
      "1.0508238\n",
      "1.0343084\n",
      "1.0541958\n",
      "1.0597857\n",
      "1.0534226\n",
      "1.0652382\n",
      "1.0396785\n",
      "1.0537245\n",
      "1.0444088\n",
      "1.0565071\n",
      "1.0612719\n",
      "1.0610535\n",
      "1.0280432\n",
      "1.0300093\n",
      "1.0439066\n",
      "1.0496728\n",
      "1.0636004\n",
      "1.0437415\n",
      "1.0463308\n",
      "1.0496832\n",
      "1.0500566\n",
      "1.048987\n",
      "1.0485959\n",
      "1.0565027\n",
      "1.040783\n",
      "1.0551939\n",
      "1.044454\n",
      "1.0485623\n",
      "1.0413374\n"
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "learning_rate = 1e-1\n",
    "max_steps = 10000\n",
    "batch_size = 32\n",
    "train_loss = 0.0\n",
    "train_acc = 0.0\n",
    "is_random_iter=True\n",
    "epochs=40\n",
    "num_steps=35 \n",
    "learning_rate=1e-2\n",
    "batch_size=32\n",
    "\n",
    "#训练\n",
    "print hidden_dim\n",
    "num_inputs = num_outputs = vocab_size\n",
    "input_placeholder = tf.placeholder(tf.float32, [num_steps, None, num_inputs])\n",
    "state_h_placeholder = tf.placeholder(tf.float32, [None, hidden_dim])\n",
    "\n",
    "state_c_placeholder = tf.placeholder(tf.float32, [None, hidden_dim])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [num_steps, None, 1])\n",
    "\n",
    "\n",
    "\n",
    "outputs, state_h = gru_rnn(input_placeholder, state_h_placeholder, params)\n",
    "\n",
    "outputs = tf.concat(outputs, axis=0)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(logits=outputs,  labels=tf.reshape(gt_placeholder, (num_steps*batch_size, 1)))\n",
    "\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "op = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "gradients = tf.gradients(loss, params)\n",
    "\n",
    "#process gradients\n",
    "clipped_gradients, norm = tf.clip_by_global_norm(gradients, 1)\n",
    "\n",
    "train_op = op.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "if is_random_iter:\n",
    "    data_iter = data_iter_random\n",
    "else:\n",
    "    data_iter = data_iter_consecutive\n",
    "for e in range(0, epochs):\n",
    "    # 如使用相邻批量采样，在同一个epoch中，隐含变量只需要在该epoch开始的时候初始化。\n",
    "    if not is_random_iter:\n",
    "        state_h_init = np.zeros(shape=(batch_size, hidden_dim))\n",
    "    train_loss, num_examples = 0, 0\n",
    "\n",
    "    for data, label in data_iter(corpus_indices, batch_size, num_steps):\n",
    "        # 如使用随机批量采样，处理每个随机小批量前都需要初始化隐含变量。\n",
    "        if is_random_iter:\n",
    "            state_h_init = np.zeros(shape=(batch_size, hidden_dim))\n",
    "        feed_dict = {input_placeholder: get_inputs(data), state_h_placeholder: state_h_init, gt_placeholder: np.expand_dims(label.T, axis=-1)}\n",
    "        loss_, state_h_, _ = sess.run([loss, state_h, train_op], feed_dict=feed_dict)\n",
    "        state_h_init = state_h_\n",
    "\n",
    "        print np.exp(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天空\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "num_steps = 35\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "seq1 = '天空'.decode('utf-8')\n",
    "seq2 = '天天'.decode('utf-8')\n",
    "seq3 = '云'.decode('utf-8')\n",
    "seqs = [seq1, seq2, seq3]\n",
    "print seq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn/weights_xz\n",
      "rnn/weights_hz\n",
      "rnn/bias_z\n",
      "rnn/weights_xr\n",
      "rnn/weights_hr\n",
      "rnn/bias_r\n",
      "rnn/weights_xh\n",
      "rnn/weights_hh\n",
      "rnn/bias_h\n",
      "rnn/weights_output\n",
      "rnn/bias_output\n",
      "天空叫我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我疯狂的可爱女人 坏坏的让我疯狂的可爱女人 \n",
      "()\n",
      "天天看着对我有属于我的天 我要一步一步往上爬 在最高点乘着叶片往前飞 任风吹干流过的泪和汗 总有一天我有属于我的天 我要一步一步往上爬 在最高点乘着叶片往前飞 任风吹干流过的泪和汗 总有一天我有属于我的天 \n",
      "()\n",
      "云在我面前 捏成你的形状 随风跟著我 一口一口吃掉忧愁 我出的那天我看着它一定实现 它一定实现 娘子 娘子却依旧每日 折一枝杨柳 你在那里 在小村外的溪边河口默默等著我 娘子依旧每日折一枝杨柳 你在那里 \n",
      "()\n"
     ]
    }
   ],
   "source": [
    "pred_len = 100\n",
    "test_num_steps = 1\n",
    "# 预测\n",
    "num_inputs = num_outputs = vocab_size    \n",
    "test_input_placeholder = tf.placeholder(tf.float32, [test_num_steps, None, num_inputs])\n",
    "test_state_h_placeholder = tf.placeholder(tf.float32, [test_num_steps, hidden_dim])\n",
    "test_state_c_placeholder = tf.placeholder(tf.float32, [test_num_steps, hidden_dim])\n",
    "\n",
    "\n",
    "outputs, state_h = gru_rnn(test_input_placeholder, test_state_h_placeholder, params)\n",
    "\n",
    "outputs = tf.concat(outputs, axis=0)\n",
    "\n",
    "\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "\n",
    "\n",
    "for seq in seqs:\n",
    "    prefix = seq\n",
    "\n",
    "    prefix = prefix.lower()\n",
    "    test_state_h_init = np.zeros((1, hidden_dim))\n",
    "    output_seq = [char_to_idx[prefix[0]]]\n",
    "    for i in range(pred_len + len(prefix)):\n",
    "        X = np.array([output_seq[-1]])\n",
    "        # 在序列中循环迭代隐含变量。\n",
    "        Y, state_h_ = sess.run([outputs, state_h], feed_dict={test_input_placeholder: get_inputs(X), test_state_h_placeholder: test_state_h_init})\n",
    "        test_state_h_init = state_h_\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = np.argmax(Y[0])\n",
    "        output_seq.append(next_input)\n",
    "    print ''.join([idx_to_char[i] for i in output_seq])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
