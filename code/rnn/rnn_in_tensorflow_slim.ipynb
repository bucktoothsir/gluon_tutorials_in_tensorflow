{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络 — 使用tensorflow\n",
    " 本节介绍如何使用tensorflow训练循环神经网络。\n",
    "\n",
    "### Penn Tree Bank (PTB) 数据集\n",
    "我们以单词为基本元素来训练语言模型。Penn Tree Bank（PTB）是一个标准的文本序列数据集。它包括训练集、验证集和测试集。\n",
    "\n",
    "下面我们载入数据集。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile('../../data/ptb.zip', 'r') as zin:\n",
    "    zin.extractall('../../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立词语索引\n",
    "下面定义了Dictionary类来映射词语和索引。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word_to_idx:\n",
    "            self.idx_to_word.append(word)\n",
    "            self.word_to_idx[word] = len(self.idx_to_word) - 1\n",
    "        return self.word_to_idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下的`Corpus`类按照读取的文本数据集建立映射词语和索引的词典，并将文本转换成词语索引的序列。这样，每个文本数据集就变成了`NDArray`格式的整数序列。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path + 'train.txt')\n",
    "        self.valid = self.tokenize(path + 'valid.txt')\n",
    "        self.test = self.tokenize(path + 'test.txt')\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        assert os.path.exists(path)\n",
    "        # 将词语添加至词典。\n",
    "\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        # 将文本转换成词语索引的序列（NDArray格式）。\n",
    "        with open(path, 'r') as f:\n",
    "            indices = np.zeros((tokens,), dtype='int32')\n",
    "            idx = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    indices[idx] = self.dictionary.word_to_idx[word]\n",
    "                    idx += 1\n",
    "        return np.array(indices, dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下词典的大小。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = '../../data/ptb/ptb.'\n",
    "corpus = Corpus(data)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 循环神经网络模型库\n",
    "我们可以定义一个循环神经网络模型库。这样就可以支持各种不同的循环神经网络模型了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class RNNModel():\n",
    "    \"\"\"循环神经网络模型库\"\"\"\n",
    "    def __init__(self, mode, vocab_size, embed_dim, hidden_dim,\n",
    "                 num_layers, dropout=0.5):\n",
    "        with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "            self.drop = dropout\n",
    "            self.embedding_weights = tf.get_variable(\"embedding\", shape=[vocab_size, embed_dim], dtype=tf.float32)\n",
    "            self.mode = mode\n",
    "            self.num_layers = num_layers\n",
    "            self.vocab_size = vocab_size\n",
    "            self.hidden_dim = hidden_dim\n",
    "            \n",
    "            if mode == 'rnn_relu':\n",
    "                self.cells = tf.contrib.rnn.BasicRNNCell(num_units=hidden_dim, activation=tf.nn.relu)\n",
    "            elif mode == 'rnn_tanh':\n",
    "                self.cells = tf.contrib.rnn.BasicRNNCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "            elif mode == 'gru_':\n",
    "                self.cells = tf.contrib.rnn.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, state_is_tuple=True)\n",
    "            elif mode == 'lstm':\n",
    "                self.cells = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, activation=tf.nn.tanh, state_is_tuple=True)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid mode %s. Options are rnn_relu, \"\n",
    "                                 \"rnn_tanh, lstm, and gru\"%mode)        \n",
    "\n",
    "    def forward(self, inputs, state, is_training):\n",
    "        with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "            emb = tf.nn.embedding_lookup(self.embedding_weights, inputs)\n",
    "            '''\n",
    "            if is_training:\n",
    "                emb =  tf.nn.dropout(emb, self.drop)\n",
    "                cells = tf.contrib.rnn.DropoutWrapper(self.cells, output_keep_prob=self.drop)\n",
    "            '''\n",
    "            num_steps = emb.get_shape().as_list()[0]\n",
    "            emb_list = []\n",
    "            for i in range(num_steps):\n",
    "                emb_list.append(emb[i])\n",
    "            cells = self.cells\n",
    "            cells = tf.contrib.rnn.MultiRNNCell([cells] * self.num_layers, state_is_tuple=True)\n",
    "            initial_state = []\n",
    "            for i in range(self.num_layers):\n",
    "                initial_state.append(state[i])\n",
    "            print initial_state\n",
    "\n",
    "            outputs, last_state = tf.nn.static_rnn(cells, emb_list, initial_state=initial_state)\n",
    "            decoded = tf.contrib.layers.fully_connected(outputs, self.vocab_size, activation_fn=None)\n",
    "        return decoded, last_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义参数\n",
    "我们接着定义模型参数。我们选择使用`ReLU`为激活函数的循环神经网络为例。这里我们把`epochs`设为$1$是为了演示方便。\n",
    "\n",
    "### 多层循环神经网络\n",
    "我们通过`num_layers`设置循环神经网络隐含层的层数，例如$2$。\n",
    "\n",
    "对于一个多层循环神经网络，当前时刻隐含层的输入来自同一时刻输入层（如果有）或上一隐含层的输出。每一层的隐含状态只沿着同一层传递。\n",
    "\n",
    "把单层循环神经网络中隐含层的每个单元当做一个函数$f$，这个函数在$t$时刻的输入是$\\mathbf{X}_t, \\mathbf{H}_{t-1}$，输出是$\\mathbf{H}_t$：\n",
    "\n",
    "$f(\\mathbf{X}_t, \\mathbf{H}_{t-1}) = \\mathbf{H}_t$\n",
    "\n",
    "假设输入为第$0$层，输出为第$L+1$层，在一共$L$个隐含层的循环神经网络中，上式中可以拓展成以下的函数:\n",
    "\n",
    "$f(\\mathbf{H}_t^{(l-1)}, \\mathbf{H}_{t-1}^{(l)}) = \\mathbf{H}_t^{(l)}$\n",
    "\n",
    "如下图所示。\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/multi-layer-rnn.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'rnn_tanh'\n",
    "\n",
    "embed_dim = 100\n",
    "hidden_dim = 100\n",
    "num_layers = 2\n",
    "clipping_norm = 0.2\n",
    "batch_size = 32\n",
    "num_steps = 5\n",
    "dropout_rate = 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量采样\n",
    "我们将数据进一步处理为便于相邻批量采样的格式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, batch_size):\n",
    "    \"\"\"数据形状 (num_batches, batch_size)\"\"\"\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.reshape((batch_size, num_batches)).T\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(num_steps, source.shape[0] - 1 - i)\n",
    "    data = source[i : i + seq_len]\n",
    "    target = source[i + 1 : i + 1 + seq_len]\n",
    "    return data, target\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, batch_size)\n",
    "test_data = batchify(corpus.test, batch_size)\n",
    "\n",
    "model = RNNModel(model_name, vocab_size, embed_dim, hidden_dim,\n",
    "                       num_layers, dropout_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "> <ipython-input-5-1d9b6898ce6c>(30)forward()\n",
      "-> emb = tf.nn.embedding_lookup(self.embedding_weights, inputs)\n",
      "(Pdb) c\n",
      "[<tf.Tensor 'rnn_1/strided_slice_5:0' shape=(32, 100) dtype=float32>, <tf.Tensor 'rnn_1/strided_slice_6:0' shape=(32, 100) dtype=float32>]\n",
      "Tensor(\"sparse_softmax_cross_entropy_loss/value:0\", shape=(), dtype=float32)\n",
      "rnn/embedding\n",
      "rnn/rnn/multi_rnn_cell/cell_0/basic_rnn_cell/kernel\n",
      "rnn/rnn/multi_rnn_cell/cell_0/basic_rnn_cell/bias\n",
      "rnn/fully_connected/weights\n",
      "rnn/fully_connected/biases\n",
      "[Epoch 1 Batch 100] loss 8.21, perplexity 3678.74\n",
      "[Epoch 1 Batch 200] loss 7.51, perplexity 1834.23\n",
      "[Epoch 1 Batch 300] loss 7.28, perplexity 1444.55\n",
      "[Epoch 1 Batch 400] loss 7.28, perplexity 1453.88\n",
      "[Epoch 1 Batch 500] loss 7.20, perplexity 1340.72\n",
      "[Epoch 1 Batch 600] loss 6.92, perplexity 1008.93\n",
      "[Epoch 1 Batch 700] loss 7.08, perplexity 1182.86\n",
      "[Epoch 1 Batch 800] loss 7.48, perplexity 1768.37\n",
      "[Epoch 1 Batch 900] loss 6.91, perplexity 1004.55\n",
      "[Epoch 1 Batch 1000] loss 6.75, perplexity 856.27\n",
      "[Epoch 1 Batch 1100] loss 6.71, perplexity 824.33\n",
      "[Epoch 1 Batch 1200] loss 6.80, perplexity 898.91\n",
      "[Epoch 1 Batch 1300] loss 7.03, perplexity 1134.77\n",
      "[Epoch 1 Batch 1400] loss 6.76, perplexity 861.10\n",
      "[Epoch 1 Batch 1500] loss 7.10, perplexity 1218.03\n",
      "[Epoch 1 Batch 1600] loss 6.67, perplexity 785.22\n",
      "[Epoch 1 Batch 1700] loss 6.79, perplexity 890.66\n",
      "[Epoch 1 Batch 1800] loss 6.89, perplexity 979.75\n",
      "[Epoch 1 Batch 1900] loss 6.70, perplexity 813.84\n",
      "[Epoch 1 Batch 2000] loss 6.86, perplexity 957.42\n",
      "[Epoch 1 Batch 2100] loss 6.78, perplexity 879.42\n",
      "[Epoch 1 Batch 2200] loss 6.80, perplexity 902.25\n",
      "[Epoch 1 Batch 2300] loss 7.01, perplexity 1105.74\n",
      "[Epoch 1 Batch 2400] loss 6.86, perplexity 956.04\n",
      "[Epoch 1 Batch 2500] loss 6.58, perplexity 720.03\n",
      "[Epoch 1 Batch 2600] loss 6.50, perplexity 664.38\n",
      "[Epoch 1 Batch 2700] loss 6.18, perplexity 485.38\n",
      "[Epoch 1 Batch 2800] loss 7.01, perplexity 1105.57\n",
      "[Epoch 1 Batch 2900] loss 6.80, perplexity 899.73\n",
      "[Epoch 1 Batch 3000] loss 6.97, perplexity 1063.09\n",
      "[Epoch 1 Batch 3100] loss 6.58, perplexity 720.60\n",
      "[Epoch 1 Batch 3200] loss 6.68, perplexity 792.93\n",
      "[Epoch 1 Batch 3300] loss 6.93, perplexity 1025.54\n",
      "[Epoch 1 Batch 3400] loss 6.62, perplexity 749.28\n",
      "[Epoch 1 Batch 3500] loss 6.48, perplexity 652.18\n",
      "[Epoch 1 Batch 3600] loss 6.75, perplexity 853.06\n",
      "[Epoch 1 Batch 3700] loss 6.71, perplexity 819.90\n",
      "[Epoch 1 Batch 3800] loss 6.61, perplexity 738.92\n",
      "[Epoch 1 Batch 3900] loss 6.50, perplexity 668.21\n",
      "[Epoch 1 Batch 4000] loss 6.70, perplexity 809.34\n",
      "[Epoch 1 Batch 4100] loss 6.35, perplexity 574.36\n",
      "[Epoch 1 Batch 4200] loss 6.51, perplexity 668.48\n",
      "[Epoch 1 Batch 4300] loss 6.40, perplexity 604.10\n",
      "[Epoch 1 Batch 4400] loss 6.74, perplexity 847.16\n",
      "[Epoch 1 Batch 4500] loss 6.46, perplexity 641.75\n",
      "[Epoch 1 Batch 4600] loss 6.79, perplexity 884.51\n",
      "[Epoch 1 Batch 4700] loss 6.89, perplexity 983.59\n",
      "[Epoch 1 Batch 4800] loss 6.58, perplexity 722.58\n",
      "[Epoch 1 Batch 4900] loss 6.64, perplexity 764.14\n",
      "[Epoch 1 Batch 5000] loss 6.45, perplexity 632.92\n",
      "[Epoch 1 Batch 5100] loss 6.96, perplexity 1058.74\n",
      "[Epoch 1 Batch 5200] loss 6.80, perplexity 893.74\n",
      "[Epoch 1 Batch 5300] loss 6.66, perplexity 781.74\n",
      "[Epoch 1 Batch 5400] loss 6.99, perplexity 1081.12\n",
      "[Epoch 1 Batch 5500] loss 6.77, perplexity 870.94\n",
      "[Epoch 1 Batch 5600] loss 6.69, perplexity 802.00\n",
      "[Epoch 1 Batch 5700] loss 6.36, perplexity 579.90\n",
      "[Epoch 1 Batch 5800] loss 6.52, perplexity 676.54\n",
      "[Epoch 2 Batch 100] loss 6.68, perplexity 799.63\n",
      "[Epoch 2 Batch 200] loss 6.57, perplexity 709.85\n",
      "[Epoch 2 Batch 300] loss 6.54, perplexity 690.72\n",
      "[Epoch 2 Batch 400] loss 6.58, perplexity 720.11\n",
      "[Epoch 2 Batch 500] loss 6.85, perplexity 941.52\n",
      "[Epoch 2 Batch 600] loss 6.53, perplexity 682.46\n",
      "[Epoch 2 Batch 700] loss 6.70, perplexity 813.22\n",
      "[Epoch 2 Batch 800] loss 6.85, perplexity 946.83\n",
      "[Epoch 2 Batch 900] loss 6.55, perplexity 697.09\n",
      "[Epoch 2 Batch 1000] loss 6.44, perplexity 626.47\n",
      "[Epoch 2 Batch 1100] loss 6.38, perplexity 590.79\n",
      "[Epoch 2 Batch 1200] loss 6.45, perplexity 633.30\n",
      "[Epoch 2 Batch 1300] loss 6.87, perplexity 959.81\n",
      "[Epoch 2 Batch 1400] loss 6.52, perplexity 677.80\n",
      "[Epoch 2 Batch 1500] loss 6.85, perplexity 944.93\n",
      "[Epoch 2 Batch 1600] loss 6.41, perplexity 609.50\n",
      "[Epoch 2 Batch 1700] loss 6.49, perplexity 658.59\n",
      "[Epoch 2 Batch 1800] loss 6.67, perplexity 790.85\n",
      "[Epoch 2 Batch 1900] loss 6.52, perplexity 678.67\n",
      "[Epoch 2 Batch 2000] loss 6.64, perplexity 764.95\n",
      "[Epoch 2 Batch 2100] loss 6.57, perplexity 714.52\n",
      "[Epoch 2 Batch 2200] loss 6.61, perplexity 742.17\n",
      "[Epoch 2 Batch 2300] loss 6.84, perplexity 931.87\n",
      "[Epoch 2 Batch 2400] loss 6.65, perplexity 770.81\n",
      "[Epoch 2 Batch 2500] loss 6.46, perplexity 639.22\n",
      "[Epoch 2 Batch 2600] loss 6.37, perplexity 582.88\n",
      "[Epoch 2 Batch 2700] loss 6.04, perplexity 417.80\n",
      "[Epoch 2 Batch 2800] loss 6.84, perplexity 935.90\n",
      "[Epoch 2 Batch 2900] loss 6.68, perplexity 797.41\n",
      "[Epoch 2 Batch 3000] loss 6.84, perplexity 934.00\n",
      "[Epoch 2 Batch 3100] loss 6.47, perplexity 644.03\n",
      "[Epoch 2 Batch 3200] loss 6.57, perplexity 710.32\n",
      "[Epoch 2 Batch 3300] loss 6.84, perplexity 936.40\n",
      "[Epoch 2 Batch 3400] loss 6.48, perplexity 649.13\n",
      "[Epoch 2 Batch 3500] loss 6.38, perplexity 588.39\n",
      "[Epoch 2 Batch 3600] loss 6.66, perplexity 779.45\n",
      "[Epoch 2 Batch 3700] loss 6.60, perplexity 733.75\n",
      "[Epoch 2 Batch 3800] loss 6.46, perplexity 640.13\n",
      "[Epoch 2 Batch 3900] loss 6.40, perplexity 604.67\n",
      "[Epoch 2 Batch 4000] loss 6.61, perplexity 740.08\n",
      "[Epoch 2 Batch 4100] loss 6.26, perplexity 525.70\n",
      "[Epoch 2 Batch 4200] loss 6.42, perplexity 614.22\n",
      "[Epoch 2 Batch 4300] loss 6.32, perplexity 555.93\n",
      "[Epoch 2 Batch 4400] loss 6.63, perplexity 756.71\n",
      "[Epoch 2 Batch 4500] loss 6.35, perplexity 572.69\n",
      "[Epoch 2 Batch 4600] loss 6.68, perplexity 794.21\n",
      "[Epoch 2 Batch 4700] loss 6.82, perplexity 913.46\n",
      "[Epoch 2 Batch 4800] loss 6.52, perplexity 677.42\n",
      "[Epoch 2 Batch 4900] loss 6.55, perplexity 700.95\n",
      "[Epoch 2 Batch 5000] loss 6.36, perplexity 578.22\n",
      "[Epoch 2 Batch 5100] loss 6.90, perplexity 991.64\n",
      "[Epoch 2 Batch 5200] loss 6.74, perplexity 847.59\n",
      "[Epoch 2 Batch 5300] loss 6.62, perplexity 749.74\n",
      "[Epoch 2 Batch 5400] loss 6.93, perplexity 1022.50\n",
      "[Epoch 2 Batch 5500] loss 6.70, perplexity 814.71\n",
      "[Epoch 2 Batch 5600] loss 6.64, perplexity 762.26\n",
      "[Epoch 2 Batch 5700] loss 6.30, perplexity 546.25\n",
      "[Epoch 2 Batch 5800] loss 6.47, perplexity 647.39\n",
      "[Epoch 3 Batch 100] loss 6.62, perplexity 751.40\n",
      "[Epoch 3 Batch 200] loss 6.51, perplexity 668.62\n",
      "[Epoch 3 Batch 300] loss 6.45, perplexity 635.47\n",
      "[Epoch 3 Batch 400] loss 6.51, perplexity 672.87\n",
      "[Epoch 3 Batch 500] loss 6.82, perplexity 916.59\n",
      "[Epoch 3 Batch 600] loss 6.48, perplexity 653.61\n",
      "[Epoch 3 Batch 700] loss 6.65, perplexity 771.79\n",
      "[Epoch 3 Batch 800] loss 6.77, perplexity 873.50\n",
      "[Epoch 3 Batch 900] loss 6.51, perplexity 672.29\n",
      "[Epoch 3 Batch 1000] loss 6.38, perplexity 590.22\n",
      "[Epoch 3 Batch 1100] loss 6.33, perplexity 559.55\n",
      "[Epoch 3 Batch 1200] loss 6.39, perplexity 595.31\n",
      "[Epoch 3 Batch 1300] loss 6.83, perplexity 925.10\n",
      "[Epoch 3 Batch 1400] loss 6.47, perplexity 647.30\n",
      "[Epoch 3 Batch 1500] loss 6.80, perplexity 900.31\n",
      "[Epoch 3 Batch 1600] loss 6.35, perplexity 573.23\n",
      "[Epoch 3 Batch 1700] loss 6.43, perplexity 617.88\n",
      "[Epoch 3 Batch 1800] loss 6.62, perplexity 746.63\n",
      "[Epoch 3 Batch 1900] loss 6.48, perplexity 652.92\n",
      "[Epoch 3 Batch 2000] loss 6.60, perplexity 737.04\n",
      "[Epoch 3 Batch 2100] loss 6.52, perplexity 679.52\n",
      "[Epoch 3 Batch 2200] loss 6.57, perplexity 712.72\n",
      "[Epoch 3 Batch 2300] loss 6.81, perplexity 903.77\n",
      "[Epoch 3 Batch 2400] loss 6.59, perplexity 728.23\n",
      "[Epoch 3 Batch 2500] loss 6.43, perplexity 619.74\n",
      "[Epoch 3 Batch 2600] loss 6.34, perplexity 564.94\n",
      "[Epoch 3 Batch 2700] loss 6.00, perplexity 401.97\n",
      "[Epoch 3 Batch 2800] loss 6.80, perplexity 897.20\n",
      "[Epoch 3 Batch 2900] loss 6.65, perplexity 770.03\n",
      "[Epoch 3 Batch 3000] loss 6.81, perplexity 903.26\n",
      "[Epoch 3 Batch 3100] loss 6.44, perplexity 624.78\n",
      "[Epoch 3 Batch 3200] loss 6.53, perplexity 683.43\n",
      "[Epoch 3 Batch 3300] loss 6.82, perplexity 915.94\n",
      "[Epoch 3 Batch 3400] loss 6.44, perplexity 625.23\n",
      "[Epoch 3 Batch 3500] loss 6.34, perplexity 568.83\n",
      "[Epoch 3 Batch 3600] loss 6.63, perplexity 757.73\n",
      "[Epoch 3 Batch 3700] loss 6.56, perplexity 706.35\n",
      "[Epoch 3 Batch 3800] loss 6.42, perplexity 614.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 Batch 3900] loss 6.38, perplexity 587.49\n",
      "[Epoch 3 Batch 4000] loss 6.58, perplexity 717.86\n",
      "[Epoch 3 Batch 4100] loss 6.24, perplexity 511.64\n",
      "[Epoch 3 Batch 4200] loss 6.40, perplexity 603.93\n",
      "[Epoch 3 Batch 4300] loss 6.28, perplexity 536.39\n",
      "[Epoch 3 Batch 4400] loss 6.59, perplexity 725.36\n",
      "[Epoch 3 Batch 4500] loss 6.32, perplexity 554.26\n",
      "[Epoch 3 Batch 4600] loss 6.64, perplexity 764.63\n",
      "[Epoch 3 Batch 4700] loss 6.80, perplexity 894.76\n",
      "[Epoch 3 Batch 4800] loss 6.49, perplexity 661.02\n",
      "[Epoch 3 Batch 4900] loss 6.53, perplexity 684.22\n",
      "[Epoch 3 Batch 5000] loss 6.32, perplexity 558.12\n",
      "[Epoch 3 Batch 5100] loss 6.88, perplexity 973.03\n",
      "[Epoch 3 Batch 5200] loss 6.72, perplexity 826.55\n",
      "[Epoch 3 Batch 5300] loss 6.60, perplexity 738.31\n",
      "[Epoch 3 Batch 5400] loss 6.91, perplexity 1000.91\n",
      "[Epoch 3 Batch 5500] loss 6.69, perplexity 806.26\n",
      "[Epoch 3 Batch 5600] loss 6.62, perplexity 750.38\n",
      "[Epoch 3 Batch 5700] loss 6.28, perplexity 535.38\n",
      "[Epoch 3 Batch 5800] loss 6.46, perplexity 637.35\n",
      "[Epoch 4 Batch 100] loss 6.60, perplexity 734.41\n",
      "[Epoch 4 Batch 200] loss 6.48, perplexity 653.70\n",
      "[Epoch 4 Batch 300] loss 6.43, perplexity 617.96\n",
      "[Epoch 4 Batch 400] loss 6.49, perplexity 661.56\n",
      "[Epoch 4 Batch 500] loss 6.81, perplexity 905.21\n",
      "[Epoch 4 Batch 600] loss 6.47, perplexity 643.27\n",
      "[Epoch 4 Batch 700] loss 6.63, perplexity 756.79\n",
      "[Epoch 4 Batch 800] loss 6.74, perplexity 847.04\n",
      "[Epoch 4 Batch 900] loss 6.50, perplexity 663.35\n",
      "[Epoch 4 Batch 1000] loss 6.36, perplexity 577.39\n",
      "[Epoch 4 Batch 1100] loss 6.31, perplexity 548.38\n",
      "[Epoch 4 Batch 1200] loss 6.36, perplexity 579.50\n",
      "[Epoch 4 Batch 1300] loss 6.81, perplexity 908.58\n",
      "[Epoch 4 Batch 1400] loss 6.45, perplexity 632.91\n",
      "[Epoch 4 Batch 1500] loss 6.78, perplexity 879.34\n",
      "[Epoch 4 Batch 1600] loss 6.33, perplexity 559.76\n",
      "[Epoch 4 Batch 1700] loss 6.41, perplexity 605.11\n",
      "[Epoch 4 Batch 1800] loss 6.59, perplexity 727.93\n",
      "[Epoch 4 Batch 1900] loss 6.46, perplexity 641.68\n",
      "[Epoch 4 Batch 2000] loss 6.59, perplexity 730.33\n",
      "[Epoch 4 Batch 2100] loss 6.50, perplexity 664.94\n",
      "[Epoch 4 Batch 2200] loss 6.55, perplexity 700.35\n",
      "[Epoch 4 Batch 2300] loss 6.79, perplexity 891.83\n",
      "[Epoch 4 Batch 2400] loss 6.57, perplexity 713.74\n",
      "[Epoch 4 Batch 2500] loss 6.42, perplexity 611.56\n",
      "[Epoch 4 Batch 2600] loss 6.32, perplexity 557.70\n",
      "[Epoch 4 Batch 2700] loss 5.98, perplexity 396.17\n",
      "[Epoch 4 Batch 2800] loss 6.78, perplexity 882.98\n",
      "[Epoch 4 Batch 2900] loss 6.63, perplexity 755.90\n",
      "[Epoch 4 Batch 3000] loss 6.79, perplexity 892.31\n",
      "[Epoch 4 Batch 3100] loss 6.42, perplexity 615.43\n",
      "[Epoch 4 Batch 3200] loss 6.51, perplexity 669.42\n",
      "[Epoch 4 Batch 3300] loss 6.81, perplexity 906.55\n",
      "[Epoch 4 Batch 3400] loss 6.42, perplexity 614.94\n",
      "[Epoch 4 Batch 3500] loss 6.33, perplexity 558.50\n",
      "[Epoch 4 Batch 3600] loss 6.62, perplexity 748.66\n",
      "[Epoch 4 Batch 3700] loss 6.54, perplexity 693.34\n",
      "[Epoch 4 Batch 3800] loss 6.40, perplexity 603.93\n",
      "[Epoch 4 Batch 3900] loss 6.36, perplexity 579.20\n",
      "[Epoch 4 Batch 4000] loss 6.56, perplexity 706.57\n",
      "[Epoch 4 Batch 4100] loss 6.22, perplexity 504.84\n",
      "[Epoch 4 Batch 4200] loss 6.40, perplexity 600.17\n",
      "[Epoch 4 Batch 4300] loss 6.27, perplexity 526.30\n",
      "[Epoch 4 Batch 4400] loss 6.56, perplexity 708.81\n",
      "[Epoch 4 Batch 4500] loss 6.30, perplexity 546.87\n",
      "[Epoch 4 Batch 4600] loss 6.62, perplexity 750.63\n",
      "[Epoch 4 Batch 4700] loss 6.79, perplexity 886.98\n",
      "[Epoch 4 Batch 4800] loss 6.48, perplexity 652.73\n",
      "[Epoch 4 Batch 4900] loss 6.52, perplexity 676.78\n",
      "[Epoch 4 Batch 5000] loss 6.31, perplexity 549.07\n",
      "[Epoch 4 Batch 5100] loss 6.87, perplexity 966.18\n",
      "[Epoch 4 Batch 5200] loss 6.70, perplexity 813.53\n",
      "[Epoch 4 Batch 5300] loss 6.60, perplexity 732.41\n",
      "[Epoch 4 Batch 5400] loss 6.90, perplexity 989.74\n",
      "[Epoch 4 Batch 5500] loss 6.69, perplexity 805.72\n",
      "[Epoch 4 Batch 5600] loss 6.61, perplexity 745.26\n",
      "[Epoch 4 Batch 5700] loss 6.27, perplexity 530.82\n",
      "[Epoch 4 Batch 5800] loss 6.45, perplexity 632.46\n",
      "[Epoch 5 Batch 100] loss 6.59, perplexity 726.33\n",
      "[Epoch 5 Batch 200] loss 6.47, perplexity 646.78\n",
      "[Epoch 5 Batch 300] loss 6.42, perplexity 611.72\n",
      "[Epoch 5 Batch 400] loss 6.49, perplexity 657.08\n",
      "[Epoch 5 Batch 500] loss 6.80, perplexity 898.85\n",
      "[Epoch 5 Batch 600] loss 6.46, perplexity 638.31\n",
      "[Epoch 5 Batch 700] loss 6.62, perplexity 749.09\n",
      "[Epoch 5 Batch 800] loss 6.73, perplexity 833.16\n",
      "[Epoch 5 Batch 900] loss 6.49, perplexity 658.42\n",
      "[Epoch 5 Batch 1000] loss 6.35, perplexity 571.81\n",
      "[Epoch 5 Batch 1100] loss 6.30, perplexity 543.48\n",
      "[Epoch 5 Batch 1200] loss 6.35, perplexity 570.81\n",
      "[Epoch 5 Batch 1300] loss 6.80, perplexity 897.93\n",
      "[Epoch 5 Batch 1400] loss 6.44, perplexity 625.33\n",
      "[Epoch 5 Batch 1500] loss 6.76, perplexity 866.78\n",
      "[Epoch 5 Batch 1600] loss 6.32, perplexity 553.16\n",
      "[Epoch 5 Batch 1700] loss 6.40, perplexity 600.07\n",
      "[Epoch 5 Batch 1800] loss 6.58, perplexity 717.77\n",
      "[Epoch 5 Batch 1900] loss 6.45, perplexity 635.81\n",
      "[Epoch 5 Batch 2000] loss 6.59, perplexity 728.11\n",
      "[Epoch 5 Batch 2100] loss 6.49, perplexity 657.29\n",
      "[Epoch 5 Batch 2200] loss 6.54, perplexity 693.32\n",
      "[Epoch 5 Batch 2300] loss 6.79, perplexity 885.85\n",
      "[Epoch 5 Batch 2400] loss 6.56, perplexity 707.18\n",
      "[Epoch 5 Batch 2500] loss 6.41, perplexity 606.89\n",
      "[Epoch 5 Batch 2600] loss 6.32, perplexity 553.96\n",
      "[Epoch 5 Batch 2700] loss 5.98, perplexity 393.61\n",
      "[Epoch 5 Batch 2800] loss 6.78, perplexity 876.70\n",
      "[Epoch 5 Batch 2900] loss 6.62, perplexity 747.47\n",
      "[Epoch 5 Batch 3000] loss 6.79, perplexity 887.54\n",
      "[Epoch 5 Batch 3100] loss 6.41, perplexity 609.47\n",
      "[Epoch 5 Batch 3200] loss 6.49, perplexity 661.20\n",
      "[Epoch 5 Batch 3300] loss 6.80, perplexity 901.76\n",
      "[Epoch 5 Batch 3400] loss 6.41, perplexity 609.29\n",
      "[Epoch 5 Batch 3500] loss 6.31, perplexity 552.40\n",
      "[Epoch 5 Batch 3600] loss 6.61, perplexity 744.21\n",
      "[Epoch 5 Batch 3700] loss 6.53, perplexity 686.16\n",
      "[Epoch 5 Batch 3800] loss 6.39, perplexity 598.07\n",
      "[Epoch 5 Batch 3900] loss 6.35, perplexity 574.68\n",
      "[Epoch 5 Batch 4000] loss 6.55, perplexity 699.53\n",
      "[Epoch 5 Batch 4100] loss 6.22, perplexity 500.76\n",
      "[Epoch 5 Batch 4200] loss 6.39, perplexity 598.50\n",
      "[Epoch 5 Batch 4300] loss 6.25, perplexity 520.45\n",
      "[Epoch 5 Batch 4400] loss 6.55, perplexity 699.09\n",
      "[Epoch 5 Batch 4500] loss 6.30, perplexity 543.06\n",
      "[Epoch 5 Batch 4600] loss 6.61, perplexity 742.83\n",
      "[Epoch 5 Batch 4700] loss 6.78, perplexity 882.88\n",
      "[Epoch 5 Batch 4800] loss 6.47, perplexity 647.95\n",
      "[Epoch 5 Batch 4900] loss 6.51, perplexity 672.79\n",
      "[Epoch 5 Batch 5000] loss 6.30, perplexity 544.39\n",
      "[Epoch 5 Batch 5100] loss 6.87, perplexity 963.25\n",
      "[Epoch 5 Batch 5200] loss 6.69, perplexity 804.56\n",
      "[Epoch 5 Batch 5300] loss 6.59, perplexity 729.16\n",
      "[Epoch 5 Batch 5400] loss 6.89, perplexity 982.92\n",
      "[Epoch 5 Batch 5500] loss 6.69, perplexity 806.71\n",
      "[Epoch 5 Batch 5600] loss 6.61, perplexity 742.73\n",
      "[Epoch 5 Batch 5700] loss 6.27, perplexity 528.71\n",
      "[Epoch 5 Batch 5800] loss 6.44, perplexity 629.46\n",
      "[Epoch 6 Batch 100] loss 6.58, perplexity 721.90\n",
      "[Epoch 6 Batch 200] loss 6.47, perplexity 643.00\n",
      "[Epoch 6 Batch 300] loss 6.41, perplexity 609.39\n",
      "[Epoch 6 Batch 400] loss 6.48, perplexity 654.70\n",
      "[Epoch 6 Batch 500] loss 6.80, perplexity 895.05\n",
      "[Epoch 6 Batch 600] loss 6.45, perplexity 635.71\n",
      "[Epoch 6 Batch 700] loss 6.61, perplexity 744.51\n",
      "[Epoch 6 Batch 800] loss 6.72, perplexity 824.96\n",
      "[Epoch 6 Batch 900] loss 6.49, perplexity 655.44\n",
      "[Epoch 6 Batch 1000] loss 6.34, perplexity 568.99\n",
      "[Epoch 6 Batch 1100] loss 6.29, perplexity 541.12\n",
      "[Epoch 6 Batch 1200] loss 6.34, perplexity 565.30\n",
      "[Epoch 6 Batch 1300] loss 6.79, perplexity 890.70\n",
      "[Epoch 6 Batch 1400] loss 6.43, perplexity 621.01\n",
      "[Epoch 6 Batch 1500] loss 6.76, perplexity 858.38\n",
      "[Epoch 6 Batch 1600] loss 6.31, perplexity 549.35\n",
      "[Epoch 6 Batch 1700] loss 6.39, perplexity 597.95\n",
      "[Epoch 6 Batch 1800] loss 6.57, perplexity 711.61\n",
      "[Epoch 6 Batch 1900] loss 6.45, perplexity 632.42\n",
      "[Epoch 6 Batch 2000] loss 6.59, perplexity 727.16\n",
      "[Epoch 6 Batch 2100] loss 6.48, perplexity 652.75\n",
      "[Epoch 6 Batch 2200] loss 6.53, perplexity 688.81\n",
      "[Epoch 6 Batch 2300] loss 6.78, perplexity 882.60\n",
      "[Epoch 6 Batch 2400] loss 6.56, perplexity 703.50\n",
      "[Epoch 6 Batch 2500] loss 6.40, perplexity 603.85\n",
      "[Epoch 6 Batch 2600] loss 6.31, perplexity 551.60\n",
      "[Epoch 6 Batch 2700] loss 5.97, perplexity 392.22\n",
      "[Epoch 6 Batch 2800] loss 6.77, perplexity 873.69\n",
      "[Epoch 6 Batch 2900] loss 6.61, perplexity 742.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6 Batch 3000] loss 6.79, perplexity 885.22\n",
      "[Epoch 6 Batch 3100] loss 6.41, perplexity 605.07\n",
      "[Epoch 6 Batch 3200] loss 6.49, perplexity 656.03\n",
      "[Epoch 6 Batch 3300] loss 6.80, perplexity 899.10\n",
      "[Epoch 6 Batch 3400] loss 6.41, perplexity 605.85\n",
      "[Epoch 6 Batch 3500] loss 6.31, perplexity 548.54\n",
      "[Epoch 6 Batch 3600] loss 6.61, perplexity 741.70\n",
      "[Epoch 6 Batch 3700] loss 6.52, perplexity 681.83\n",
      "[Epoch 6 Batch 3800] loss 6.39, perplexity 594.61\n",
      "[Epoch 6 Batch 3900] loss 6.35, perplexity 572.09\n",
      "[Epoch 6 Batch 4000] loss 6.54, perplexity 694.68\n",
      "[Epoch 6 Batch 4100] loss 6.21, perplexity 498.04\n",
      "[Epoch 6 Batch 4200] loss 6.39, perplexity 597.73\n",
      "[Epoch 6 Batch 4300] loss 6.25, perplexity 516.74\n",
      "[Epoch 6 Batch 4400] loss 6.54, perplexity 693.07\n",
      "[Epoch 6 Batch 4500] loss 6.29, perplexity 540.74\n",
      "[Epoch 6 Batch 4600] loss 6.60, perplexity 738.01\n",
      "[Epoch 6 Batch 4700] loss 6.78, perplexity 880.47\n",
      "[Epoch 6 Batch 4800] loss 6.47, perplexity 645.08\n",
      "[Epoch 6 Batch 4900] loss 6.51, perplexity 670.42\n",
      "[Epoch 6 Batch 5000] loss 6.29, perplexity 541.63\n",
      "[Epoch 6 Batch 5100] loss 6.87, perplexity 961.92\n",
      "[Epoch 6 Batch 5200] loss 6.68, perplexity 797.98\n",
      "[Epoch 6 Batch 5300] loss 6.59, perplexity 727.37\n",
      "[Epoch 6 Batch 5400] loss 6.89, perplexity 978.46\n",
      "[Epoch 6 Batch 5500] loss 6.69, perplexity 808.02\n",
      "[Epoch 6 Batch 5600] loss 6.61, perplexity 741.38\n",
      "[Epoch 6 Batch 5700] loss 6.27, perplexity 527.70\n",
      "[Epoch 6 Batch 5800] loss 6.44, perplexity 627.33\n",
      "[Epoch 7 Batch 100] loss 6.58, perplexity 719.31\n",
      "[Epoch 7 Batch 200] loss 6.46, perplexity 640.77\n",
      "[Epoch 7 Batch 300] loss 6.41, perplexity 608.51\n",
      "[Epoch 7 Batch 400] loss 6.48, perplexity 653.31\n",
      "[Epoch 7 Batch 500] loss 6.79, perplexity 892.70\n",
      "[Epoch 7 Batch 600] loss 6.45, perplexity 634.22\n",
      "[Epoch 7 Batch 700] loss 6.61, perplexity 741.57\n",
      "[Epoch 7 Batch 800] loss 6.71, perplexity 819.81\n",
      "[Epoch 7 Batch 900] loss 6.48, perplexity 653.57\n",
      "[Epoch 7 Batch 1000] loss 6.34, perplexity 567.42\n",
      "[Epoch 7 Batch 1100] loss 6.29, perplexity 539.90\n",
      "[Epoch 7 Batch 1200] loss 6.33, perplexity 561.54\n",
      "[Epoch 7 Batch 1300] loss 6.79, perplexity 885.72\n",
      "[Epoch 7 Batch 1400] loss 6.43, perplexity 618.36\n",
      "[Epoch 7 Batch 1500] loss 6.75, perplexity 852.40\n",
      "[Epoch 7 Batch 1600] loss 6.30, perplexity 546.93\n",
      "[Epoch 7 Batch 1700] loss 6.39, perplexity 597.09\n",
      "[Epoch 7 Batch 1800] loss 6.56, perplexity 707.70\n",
      "[Epoch 7 Batch 1900] loss 6.45, perplexity 630.33\n",
      "[Epoch 7 Batch 2000] loss 6.59, perplexity 726.62\n",
      "[Epoch 7 Batch 2100] loss 6.48, perplexity 649.86\n",
      "[Epoch 7 Batch 2200] loss 6.53, perplexity 685.76\n",
      "[Epoch 7 Batch 2300] loss 6.78, perplexity 880.63\n",
      "[Epoch 7 Batch 2400] loss 6.55, perplexity 701.10\n",
      "[Epoch 7 Batch 2500] loss 6.40, perplexity 601.73\n",
      "[Epoch 7 Batch 2600] loss 6.31, perplexity 549.90\n",
      "[Epoch 7 Batch 2700] loss 5.97, perplexity 391.34\n",
      "[Epoch 7 Batch 2800] loss 6.77, perplexity 872.20\n",
      "[Epoch 7 Batch 2900] loss 6.60, perplexity 738.34\n",
      "[Epoch 7 Batch 3000] loss 6.78, perplexity 884.05\n",
      "[Epoch 7 Batch 3100] loss 6.40, perplexity 601.60\n",
      "[Epoch 7 Batch 3200] loss 6.48, perplexity 652.58\n",
      "[Epoch 7 Batch 3300] loss 6.80, perplexity 897.43\n",
      "[Epoch 7 Batch 3400] loss 6.40, perplexity 603.63\n",
      "[Epoch 7 Batch 3500] loss 6.30, perplexity 545.97\n",
      "[Epoch 7 Batch 3600] loss 6.61, perplexity 740.12\n",
      "[Epoch 7 Batch 3700] loss 6.52, perplexity 679.04\n",
      "[Epoch 7 Batch 3800] loss 6.38, perplexity 592.47\n",
      "[Epoch 7 Batch 3900] loss 6.35, perplexity 570.55\n",
      "[Epoch 7 Batch 4000] loss 6.54, perplexity 691.13\n",
      "[Epoch 7 Batch 4100] loss 6.21, perplexity 496.09\n",
      "[Epoch 7 Batch 4200] loss 6.39, perplexity 597.37\n",
      "[Epoch 7 Batch 4300] loss 6.24, perplexity 514.22\n",
      "[Epoch 7 Batch 4400] loss 6.54, perplexity 689.16\n",
      "[Epoch 7 Batch 4500] loss 6.29, perplexity 539.17\n",
      "[Epoch 7 Batch 4600] loss 6.60, perplexity 734.78\n",
      "[Epoch 7 Batch 4700] loss 6.78, perplexity 879.00\n",
      "[Epoch 7 Batch 4800] loss 6.47, perplexity 643.28\n",
      "[Epoch 7 Batch 4900] loss 6.51, perplexity 668.89\n",
      "[Epoch 7 Batch 5000] loss 6.29, perplexity 539.85\n",
      "[Epoch 7 Batch 5100] loss 6.87, perplexity 961.32\n",
      "[Epoch 7 Batch 5200] loss 6.68, perplexity 792.98\n",
      "[Epoch 7 Batch 5300] loss 6.59, perplexity 726.37\n",
      "[Epoch 7 Batch 5400] loss 6.88, perplexity 975.48\n",
      "[Epoch 7 Batch 5500] loss 6.70, perplexity 809.36\n",
      "[Epoch 7 Batch 5600] loss 6.61, perplexity 740.66\n",
      "[Epoch 7 Batch 5700] loss 6.27, perplexity 527.23\n",
      "[Epoch 7 Batch 5800] loss 6.44, perplexity 625.71\n",
      "[Epoch 8 Batch 100] loss 6.58, perplexity 717.72\n",
      "[Epoch 8 Batch 200] loss 6.46, perplexity 639.37\n",
      "[Epoch 8 Batch 300] loss 6.41, perplexity 608.20\n",
      "[Epoch 8 Batch 400] loss 6.48, perplexity 652.46\n",
      "[Epoch 8 Batch 500] loss 6.79, perplexity 891.23\n",
      "[Epoch 8 Batch 600] loss 6.45, perplexity 633.26\n",
      "[Epoch 8 Batch 700] loss 6.61, perplexity 739.57\n",
      "[Epoch 8 Batch 800] loss 6.70, perplexity 816.42\n",
      "[Epoch 8 Batch 900] loss 6.48, perplexity 652.34\n",
      "[Epoch 8 Batch 1000] loss 6.34, perplexity 566.50\n",
      "[Epoch 8 Batch 1100] loss 6.29, perplexity 539.23\n",
      "[Epoch 8 Batch 1200] loss 6.33, perplexity 558.87\n",
      "[Epoch 8 Batch 1300] loss 6.78, perplexity 882.23\n",
      "[Epoch 8 Batch 1400] loss 6.42, perplexity 616.62\n",
      "[Epoch 8 Batch 1500] loss 6.74, perplexity 847.96\n",
      "[Epoch 8 Batch 1600] loss 6.30, perplexity 545.28\n",
      "[Epoch 8 Batch 1700] loss 6.39, perplexity 596.81\n",
      "[Epoch 8 Batch 1800] loss 6.56, perplexity 705.16\n",
      "[Epoch 8 Batch 1900] loss 6.44, perplexity 628.98\n",
      "[Epoch 8 Batch 2000] loss 6.59, perplexity 726.23\n",
      "[Epoch 8 Batch 2100] loss 6.47, perplexity 647.92\n",
      "[Epoch 8 Batch 2200] loss 6.53, perplexity 683.62\n",
      "[Epoch 8 Batch 2300] loss 6.78, perplexity 879.30\n",
      "[Epoch 8 Batch 2400] loss 6.55, perplexity 699.42\n",
      "[Epoch 8 Batch 2500] loss 6.40, perplexity 600.16\n",
      "[Epoch 8 Batch 2600] loss 6.31, perplexity 548.57\n",
      "[Epoch 8 Batch 2700] loss 5.97, perplexity 390.71\n",
      "[Epoch 8 Batch 2800] loss 6.77, perplexity 871.47\n",
      "[Epoch 8 Batch 2900] loss 6.60, perplexity 735.67\n",
      "[Epoch 8 Batch 3000] loss 6.78, perplexity 883.48\n",
      "[Epoch 8 Batch 3100] loss 6.39, perplexity 598.78\n",
      "[Epoch 8 Batch 3200] loss 6.48, perplexity 650.15\n",
      "[Epoch 8 Batch 3300] loss 6.80, perplexity 896.24\n",
      "[Epoch 8 Batch 3400] loss 6.40, perplexity 602.16\n",
      "[Epoch 8 Batch 3500] loss 6.30, perplexity 544.18\n",
      "[Epoch 8 Batch 3600] loss 6.61, perplexity 739.03\n",
      "[Epoch 8 Batch 3700] loss 6.52, perplexity 677.14\n",
      "[Epoch 8 Batch 3800] loss 6.38, perplexity 591.11\n",
      "[Epoch 8 Batch 3900] loss 6.34, perplexity 569.60\n",
      "[Epoch 8 Batch 4000] loss 6.53, perplexity 688.47\n",
      "[Epoch 8 Batch 4100] loss 6.20, perplexity 494.61\n",
      "[Epoch 8 Batch 4200] loss 6.39, perplexity 597.21\n",
      "[Epoch 8 Batch 4300] loss 6.24, perplexity 512.43\n",
      "[Epoch 8 Batch 4400] loss 6.53, perplexity 686.51\n",
      "[Epoch 8 Batch 4500] loss 6.29, perplexity 538.04\n",
      "[Epoch 8 Batch 4600] loss 6.60, perplexity 732.48\n",
      "[Epoch 8 Batch 4700] loss 6.78, perplexity 878.10\n",
      "[Epoch 8 Batch 4800] loss 6.46, perplexity 642.13\n",
      "[Epoch 8 Batch 4900] loss 6.50, perplexity 667.81\n",
      "[Epoch 8 Batch 5000] loss 6.29, perplexity 538.62\n",
      "[Epoch 8 Batch 5100] loss 6.87, perplexity 961.05\n",
      "[Epoch 8 Batch 5200] loss 6.67, perplexity 789.08\n",
      "[Epoch 8 Batch 5300] loss 6.59, perplexity 725.79\n",
      "[Epoch 8 Batch 5400] loss 6.88, perplexity 973.45\n",
      "[Epoch 8 Batch 5500] loss 6.70, perplexity 810.62\n",
      "[Epoch 8 Batch 5600] loss 6.61, perplexity 740.28\n",
      "[Epoch 8 Batch 5700] loss 6.27, perplexity 527.04\n",
      "[Epoch 8 Batch 5800] loss 6.44, perplexity 624.42\n",
      "[Epoch 9 Batch 100] loss 6.57, perplexity 716.70\n",
      "[Epoch 9 Batch 200] loss 6.46, perplexity 638.46\n",
      "[Epoch 9 Batch 300] loss 6.41, perplexity 608.10\n",
      "[Epoch 9 Batch 400] loss 6.48, perplexity 651.94\n",
      "[Epoch 9 Batch 500] loss 6.79, perplexity 890.30\n",
      "[Epoch 9 Batch 600] loss 6.45, perplexity 632.58\n",
      "[Epoch 9 Batch 700] loss 6.60, perplexity 738.12\n",
      "[Epoch 9 Batch 800] loss 6.70, perplexity 814.11\n",
      "[Epoch 9 Batch 900] loss 6.48, perplexity 651.51\n",
      "[Epoch 9 Batch 1000] loss 6.34, perplexity 565.96\n",
      "[Epoch 9 Batch 1100] loss 6.29, perplexity 538.83\n",
      "[Epoch 9 Batch 1200] loss 6.32, perplexity 556.92\n",
      "[Epoch 9 Batch 1300] loss 6.78, perplexity 879.73\n",
      "[Epoch 9 Batch 1400] loss 6.42, perplexity 615.40\n",
      "[Epoch 9 Batch 1500] loss 6.74, perplexity 844.58\n",
      "[Epoch 9 Batch 1600] loss 6.30, perplexity 544.10\n",
      "[Epoch 9 Batch 1700] loss 6.39, perplexity 596.80\n",
      "[Epoch 9 Batch 1800] loss 6.56, perplexity 703.47\n",
      "[Epoch 9 Batch 1900] loss 6.44, perplexity 628.08\n",
      "[Epoch 9 Batch 2000] loss 6.59, perplexity 725.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9 Batch 2100] loss 6.47, perplexity 646.58\n",
      "[Epoch 9 Batch 2200] loss 6.53, perplexity 682.09\n",
      "[Epoch 9 Batch 2300] loss 6.78, perplexity 878.30\n",
      "[Epoch 9 Batch 2400] loss 6.55, perplexity 698.17\n",
      "[Epoch 9 Batch 2500] loss 6.40, perplexity 598.94\n",
      "[Epoch 9 Batch 2600] loss 6.31, perplexity 547.49\n",
      "[Epoch 9 Batch 2700] loss 5.97, perplexity 390.22\n",
      "[Epoch 9 Batch 2800] loss 6.77, perplexity 871.12\n",
      "[Epoch 9 Batch 2900] loss 6.60, perplexity 733.69\n",
      "[Epoch 9 Batch 3000] loss 6.78, perplexity 883.26\n",
      "[Epoch 9 Batch 3100] loss 6.39, perplexity 596.44\n",
      "[Epoch 9 Batch 3200] loss 6.47, perplexity 648.34\n",
      "[Epoch 9 Batch 3300] loss 6.80, perplexity 895.35\n",
      "[Epoch 9 Batch 3400] loss 6.40, perplexity 601.15\n",
      "[Epoch 9 Batch 3500] loss 6.30, perplexity 542.88\n",
      "[Epoch 9 Batch 3600] loss 6.60, perplexity 738.24\n",
      "[Epoch 9 Batch 3700] loss 6.52, perplexity 675.79\n",
      "[Epoch 9 Batch 3800] loss 6.38, perplexity 590.22\n",
      "[Epoch 9 Batch 3900] loss 6.34, perplexity 569.02\n",
      "[Epoch 9 Batch 4000] loss 6.53, perplexity 686.43\n",
      "[Epoch 9 Batch 4100] loss 6.20, perplexity 493.45\n",
      "[Epoch 9 Batch 4200] loss 6.39, perplexity 597.13\n",
      "[Epoch 9 Batch 4300] loss 6.24, perplexity 511.11\n",
      "[Epoch 9 Batch 4400] loss 6.53, perplexity 684.63\n",
      "[Epoch 9 Batch 4500] loss 6.29, perplexity 537.18\n",
      "[Epoch 9 Batch 4600] loss 6.59, perplexity 730.75\n",
      "[Epoch 9 Batch 4700] loss 6.78, perplexity 877.53\n",
      "[Epoch 9 Batch 4800] loss 6.46, perplexity 641.35\n",
      "[Epoch 9 Batch 4900] loss 6.50, perplexity 667.00\n",
      "[Epoch 9 Batch 5000] loss 6.29, perplexity 537.74\n",
      "[Epoch 9 Batch 5100] loss 6.87, perplexity 960.94\n",
      "[Epoch 9 Batch 5200] loss 6.67, perplexity 785.99\n",
      "[Epoch 9 Batch 5300] loss 6.59, perplexity 725.42\n",
      "[Epoch 9 Batch 5400] loss 6.88, perplexity 972.04\n",
      "[Epoch 9 Batch 5500] loss 6.70, perplexity 811.74\n",
      "[Epoch 9 Batch 5600] loss 6.61, perplexity 740.10\n",
      "[Epoch 9 Batch 5700] loss 6.27, perplexity 526.98\n",
      "[Epoch 9 Batch 5800] loss 6.44, perplexity 623.37\n",
      "[Epoch 10 Batch 100] loss 6.57, perplexity 716.00\n",
      "[Epoch 10 Batch 200] loss 6.46, perplexity 637.85\n",
      "[Epoch 10 Batch 300] loss 6.41, perplexity 608.10\n",
      "[Epoch 10 Batch 400] loss 6.48, perplexity 651.62\n",
      "[Epoch 10 Batch 500] loss 6.79, perplexity 889.74\n",
      "[Epoch 10 Batch 600] loss 6.45, perplexity 632.08\n",
      "[Epoch 10 Batch 700] loss 6.60, perplexity 737.01\n",
      "[Epoch 10 Batch 800] loss 6.70, perplexity 812.48\n",
      "[Epoch 10 Batch 900] loss 6.48, perplexity 650.93\n",
      "[Epoch 10 Batch 1000] loss 6.34, perplexity 565.66\n",
      "[Epoch 10 Batch 1100] loss 6.29, perplexity 538.58\n",
      "[Epoch 10 Batch 1200] loss 6.32, perplexity 555.45\n",
      "[Epoch 10 Batch 1300] loss 6.78, perplexity 877.90\n",
      "[Epoch 10 Batch 1400] loss 6.42, perplexity 614.51\n",
      "[Epoch 10 Batch 1500] loss 6.74, perplexity 841.96\n",
      "[Epoch 10 Batch 1600] loss 6.30, perplexity 543.22\n",
      "[Epoch 10 Batch 1700] loss 6.39, perplexity 596.92\n",
      "[Epoch 10 Batch 1800] loss 6.55, perplexity 702.33\n",
      "[Epoch 10 Batch 1900] loss 6.44, perplexity 627.47\n",
      "[Epoch 10 Batch 2000] loss 6.59, perplexity 725.59\n",
      "[Epoch 10 Batch 2100] loss 6.47, perplexity 645.62\n",
      "[Epoch 10 Batch 2200] loss 6.52, perplexity 680.95\n",
      "[Epoch 10 Batch 2300] loss 6.78, perplexity 877.48\n",
      "[Epoch 10 Batch 2400] loss 6.55, perplexity 697.24\n",
      "[Epoch 10 Batch 2500] loss 6.39, perplexity 597.97\n",
      "[Epoch 10 Batch 2600] loss 6.30, perplexity 546.60\n",
      "[Epoch 10 Batch 2700] loss 5.97, perplexity 389.85\n",
      "[Epoch 10 Batch 2800] loss 6.77, perplexity 870.98\n",
      "[Epoch 10 Batch 2900] loss 6.60, perplexity 732.19\n",
      "[Epoch 10 Batch 3000] loss 6.78, perplexity 883.24\n",
      "[Epoch 10 Batch 3100] loss 6.39, perplexity 594.48\n",
      "[Epoch 10 Batch 3200] loss 6.47, perplexity 646.92\n",
      "[Epoch 10 Batch 3300] loss 6.80, perplexity 894.63\n",
      "[Epoch 10 Batch 3400] loss 6.40, perplexity 600.45\n",
      "[Epoch 10 Batch 3500] loss 6.30, perplexity 541.89\n",
      "[Epoch 10 Batch 3600] loss 6.60, perplexity 737.65\n",
      "[Epoch 10 Batch 3700] loss 6.51, perplexity 674.81\n",
      "[Epoch 10 Batch 3800] loss 6.38, perplexity 589.61\n",
      "[Epoch 10 Batch 3900] loss 6.34, perplexity 568.66\n",
      "[Epoch 10 Batch 4000] loss 6.53, perplexity 684.87\n",
      "[Epoch 10 Batch 4100] loss 6.20, perplexity 492.52\n",
      "[Epoch 10 Batch 4200] loss 6.39, perplexity 597.08\n",
      "[Epoch 10 Batch 4300] loss 6.23, perplexity 510.10\n",
      "[Epoch 10 Batch 4400] loss 6.53, perplexity 683.24\n",
      "[Epoch 10 Batch 4500] loss 6.29, perplexity 536.50\n",
      "[Epoch 10 Batch 4600] loss 6.59, perplexity 729.40\n",
      "[Epoch 10 Batch 4700] loss 6.78, perplexity 877.17\n",
      "[Epoch 10 Batch 4800] loss 6.46, perplexity 640.81\n",
      "[Epoch 10 Batch 4900] loss 6.50, perplexity 666.35\n",
      "[Epoch 10 Batch 5000] loss 6.29, perplexity 537.10\n",
      "[Epoch 10 Batch 5100] loss 6.87, perplexity 960.91\n",
      "[Epoch 10 Batch 5200] loss 6.66, perplexity 783.51\n",
      "[Epoch 10 Batch 5300] loss 6.59, perplexity 725.14\n",
      "[Epoch 10 Batch 5400] loss 6.88, perplexity 971.03\n",
      "[Epoch 10 Batch 5500] loss 6.70, perplexity 812.72\n",
      "[Epoch 10 Batch 5600] loss 6.61, perplexity 740.03\n",
      "[Epoch 10 Batch 5700] loss 6.27, perplexity 526.99\n",
      "[Epoch 10 Batch 5800] loss 6.43, perplexity 622.50\n",
      "[Epoch 11 Batch 100] loss 6.57, perplexity 715.51\n",
      "[Epoch 11 Batch 200] loss 6.46, perplexity 637.41\n",
      "[Epoch 11 Batch 300] loss 6.41, perplexity 608.13\n",
      "[Epoch 11 Batch 400] loss 6.48, perplexity 651.42\n",
      "[Epoch 11 Batch 500] loss 6.79, perplexity 889.40\n",
      "[Epoch 11 Batch 600] loss 6.45, perplexity 631.69\n",
      "[Epoch 11 Batch 700] loss 6.60, perplexity 736.12\n",
      "[Epoch 11 Batch 800] loss 6.70, perplexity 811.28\n",
      "[Epoch 11 Batch 900] loss 6.48, perplexity 650.52\n",
      "[Epoch 11 Batch 1000] loss 6.34, perplexity 565.51\n",
      "[Epoch 11 Batch 1100] loss 6.29, perplexity 538.42\n",
      "[Epoch 11 Batch 1200] loss 6.32, perplexity 554.31\n",
      "[Epoch 11 Batch 1300] loss 6.78, perplexity 876.54\n",
      "[Epoch 11 Batch 1400] loss 6.42, perplexity 613.82\n",
      "[Epoch 11 Batch 1500] loss 6.73, perplexity 839.90\n",
      "[Epoch 11 Batch 1600] loss 6.30, perplexity 542.55\n",
      "[Epoch 11 Batch 1700] loss 6.39, perplexity 597.10\n",
      "[Epoch 11 Batch 1800] loss 6.55, perplexity 701.55\n",
      "[Epoch 11 Batch 1900] loss 6.44, perplexity 627.07\n",
      "[Epoch 11 Batch 2000] loss 6.59, perplexity 725.29\n",
      "[Epoch 11 Batch 2100] loss 6.47, perplexity 644.91\n",
      "[Epoch 11 Batch 2200] loss 6.52, perplexity 680.08\n",
      "[Epoch 11 Batch 2300] loss 6.78, perplexity 876.79\n",
      "[Epoch 11 Batch 2400] loss 6.55, perplexity 696.52\n",
      "[Epoch 11 Batch 2500] loss 6.39, perplexity 597.18\n",
      "[Epoch 11 Batch 2600] loss 6.30, perplexity 545.84\n",
      "[Epoch 11 Batch 2700] loss 5.96, perplexity 389.55\n",
      "[Epoch 11 Batch 2800] loss 6.77, perplexity 870.95\n",
      "[Epoch 11 Batch 2900] loss 6.59, perplexity 731.02\n",
      "[Epoch 11 Batch 3000] loss 6.78, perplexity 883.34\n",
      "[Epoch 11 Batch 3100] loss 6.38, perplexity 592.83\n",
      "[Epoch 11 Batch 3200] loss 6.47, perplexity 645.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fa02c760308d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mstate_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mibatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_period\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mibatch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 904\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    905\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1136\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1137\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1354\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1339\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "max_steps = 10000\n",
    "batch_size = 32\n",
    "num_epochs= 40\n",
    "learning_rate = 1e0\n",
    "eval_period = 100\n",
    "\n",
    "#训练\n",
    "print hidden_dim\n",
    "num_inputs = num_outputs = vocab_size\n",
    "input_placeholder = tf.placeholder(tf.int64, [num_steps, batch_size])\n",
    "state_placeholder = tf.placeholder(tf.float32, [num_layers, batch_size, hidden_dim])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [num_steps, batch_size, 1])\n",
    "\n",
    "state_\n",
    "outputs, state = model.forward(input_placeholder, state_placeholder, is_training=True)\n",
    "\n",
    "outputs = tf.concat(outputs, axis=0)\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(logits=tf.reshape(outputs, [batch_size * num_steps, vocab_size]),  labels=tf.reshape(gt_placeholder, (num_steps*batch_size, 1)))\n",
    "\n",
    "print loss\n",
    "\n",
    "params = []\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "params.append(var)\n",
    "\n",
    "\n",
    "op = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "gradients = tf.gradients(loss, params)\n",
    "\n",
    "#process gradients\n",
    "clipped_gradients, norm = tf.clip_by_global_norm(gradients, 5)\n",
    "\n",
    "train_op = op.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    state_init = np.zeros(shape=(num_layers, batch_size, hidden_dim))\n",
    "    for ibatch, i in enumerate(range(0, train_data.shape[0] - 1 - num_steps, num_steps)):\n",
    "\n",
    "        data, target = get_batch(train_data, i)\n",
    "        feed_dict = {input_placeholder: data, state_placeholder: state_init, gt_placeholder: np.expand_dims(target, axis=-1)}\n",
    "        loss_, state_, _ = sess.run([loss, state, train_op], feed_dict=feed_dict)\n",
    "        state_init = state_\n",
    "        if ibatch % eval_period == 0 and ibatch > 0:\n",
    "            print('[Epoch %d Batch %d] loss %.2f, perplexity %.2f' % (\n",
    "                epoch + 1, ibatch, loss_, math.exp(loss_)))\n",
    "    \n",
    "    '''\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    state_init = np.zeros(shape=(num_layers, batch_size, hidden_dim))\n",
    "    for i in range(0, val_data.shape[0] - 1 - num_steps, num_steps):\n",
    "        data, target = get_batch(val_data, i)\n",
    "        feed_dict = {input_placeholder: data, state_placeholder: state_init, gt_placeholder: np.expand_dims(target, axis=-1)}\n",
    "        loss_, state_ = sess.run([loss, state], feed_dict=feed_dict)\n",
    "        state_init = state_\n",
    "        total_L += loss_\n",
    "        ntotal += 1\n",
    "    \n",
    "    total_L /= ntotal\n",
    "    print('[Epoch %d] , validation loss %.2f, validation perplexity %.2f' % (epoch + 1, total_L, math.exp(total_L)))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
