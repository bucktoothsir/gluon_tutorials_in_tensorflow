{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 长短期记忆（LSTM）— 从0开始\n",
    "\n",
    "上一节中，我们介绍了循环神经网络中的梯度计算方法。我们发现，循环神经网络的隐含层变量梯度可能会出现衰减或爆炸。虽然梯度裁剪可以应对梯度爆炸，但无法解决梯度衰减的问题。因此，给定一个时间序列，例如文本序列，循环神经网络在实际中其实较难捕捉两个时刻距离较大的文本元素（字或词）之间的依赖关系。\n",
    "\n",
    "为了更好地捕捉时序数据中间隔较大的依赖关系，我们介绍了一种常用的门控循环神经网络，叫做门控循环单元。本节将介绍另一种常用的门控循环神经网络，长短期记忆（long short-term memory，简称LSTM）。它由Hochreiter和Schmidhuber在1997年被提出。事实上，它比门控循环单元的结构稍微更复杂一点。\n",
    "\n",
    "### 长短期记忆\n",
    "我们先介绍长短期记忆的构造。长短期记忆的隐含状态包括隐含层变量H和细胞C（也称记忆细胞）。它们形状相同。\n",
    "\n",
    "### 输入门、遗忘门和输出门\n",
    "\n",
    "假定隐含状态长度为h，给定时刻$t$的一个样本数为$n$特征向量维度为x的批量数据$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times x}$和上一时刻隐含状态$\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$，输入门（input gate）$\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}$、遗忘门（forget gate）$\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}$和输出门（output gate）$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}$的定义如下：\n",
    "\n",
    "$\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\mathbf{b}_i)$\n",
    "\n",
    "$\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\mathbf{b}_f)$\n",
    "\n",
    "$\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\mathbf{b}_o)$\n",
    "\n",
    "其中的$\\mathbf{W}_{xi}, \\mathbf{W}_{xf}, \\mathbf{W}_{xo} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hi}, \\mathbf{W}_{hf}, \\mathbf{W}_{ho} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。函数$\\sigma$自变量中的三项相加使用了广播。\n",
    "\n",
    "和门控循环单元中的重置门和更新门一样，这里的输入门、遗忘门和输出门中每个元素的值域都是$[0,1]$。\n",
    "\n",
    "### 候选细胞\n",
    "和门控循环单元中的候选隐含状态一样，长短期记忆中的候选细胞$\\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h}$也使用了值域在[−1,1]的双曲正切函数$\\tanh$做激活函数：\n",
    "\n",
    "$\\tilde{\\mathbf{C}}_t = \\text{tanh}(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c)$\n",
    "\n",
    "其中的$\\mathbf{W}_{xc} \\in \\mathbb{R}^{x \\times h}$和$\\mathbf{W}_{hc} \\in \\mathbb{R}^{h \\times h}$是可学习的权重参数，$\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}$是可学习的偏移参数。\n",
    "\n",
    "### 细胞\n",
    "我们可以通过元素值域在$[0,1]$的输入门、遗忘门和输出门来控制隐含状态中信息的流动：这通常可以应用按元素乘法符$\\odot$。当前时刻细胞$\\mathbf{C}_t \\in \\mathbb{R}^{n \\times h}$的计算组合了上一时刻细胞和当前时刻候选细胞的信息，并通过遗忘门和输入门来控制信息的流动：\n",
    "\n",
    "$\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t$\n",
    "\n",
    "需要注意的是，如果遗忘门一直近似1且输入门一直近似0，过去的细胞将一直通过时间保存并传递至当前时刻。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时序数据中间隔较大的依赖关系。\n",
    "\n",
    "### 隐含状态\n",
    "有了细胞以后，接下来我们还可以通过输出门来控制从细胞到隐含层变量$\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$的信息的流动：\n",
    "\n",
    "$\\mathbf{H}_t = \\mathbf{O}_t \\odot \\text{tanh}(\\mathbf{C}_t)$\n",
    "\n",
    "需要注意的是，当输出门近似$1$，细胞信息将传递到隐含层变量；当输出门近似$0$，细胞信息只自己保留。\n",
    "\n",
    "输出层的设计可参照循环神经网络中的描述。\n",
    "\n",
    "### 实验\n",
    "为了实现并展示门控循环单元，我们依然使用周杰伦歌词数据集来训练模型作词。这里除长短期记忆以外的实现已在循环神经网络中介绍。\n",
    "\n",
    "### 数据处理\n",
    "我们先读取并对数据集做简单处理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vocab size:', 1465)\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('../../data/jaychou_lyrics.txt.zip', 'r') as zin:\n",
    "    zin.extractall('../../data/')\n",
    "\n",
    "with open('../../data/jaychou_lyrics.txt') as f:\n",
    "    corpus_chars = f.read().decode('utf-8')\n",
    "\n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
    "corpus_chars = corpus_chars[0:20000]\n",
    "\n",
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print('vocab size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用onehot来将字符索引表示成向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot(positions, vocal_size):\n",
    "    if len(positions.shape) == 0:\n",
    "        positions = np.expand_dims(positions, 0)\n",
    "        positions = np.expand_dims(positions, 1)\n",
    "    if len(positions.shape) == 1:\n",
    "        positions = np.expand_dims(positions, 1)\n",
    "    all_zeros = np.zeros((positions.shape[0], vocab_size))\n",
    "    for i in xrange(positions.shape[0]):\n",
    "        for j in xrange(positions.shape[1]):\n",
    "            all_zeros[i, int(positions[i, j])] = 1\n",
    "        \n",
    "    return all_zeros\n",
    "        \n",
    "one_hot(np.array([0, 2]), vocab_size)\n",
    "\n",
    "def get_inputs(data):\n",
    "    return [one_hot(X, vocab_size) for X in data.T]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "以下部分对模型参数进行初始化。参数hidden_dim定义了隐含状态的长度。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def data_iter_random(corpus_indices, batch_size, num_steps):\n",
    "    # 减一是因为label的索引是相应data的索引加一\n",
    "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
    "    epoch_size = num_examples // batch_size\n",
    "    # 随机化样本\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "\n",
    "    # 返回num_steps个数据\n",
    "    def _data(pos):\n",
    "        return corpus_indices[pos: pos + num_steps]\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # 每次读取batch_size个随机样本\n",
    "        i = i * batch_size\n",
    "        batch_indices = example_indices[i: i + batch_size]\n",
    "        data = np.array(\n",
    "            [_data(j * num_steps) for j in batch_indices])\n",
    "        label = np.array(\n",
    "            [_data(j * num_steps + 1) for j in batch_indices])\n",
    "        yield data.astype(np.float32), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_dim = vocab_size\n",
    "# 隐含状态长度\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "weight_scale = .01\n",
    "\n",
    "\n",
    "with tf.variable_scope('rnn', reuse=tf.AUTO_REUSE):\n",
    "    # input gate\n",
    "    W_xi = tf.get_variable(name='weights_xi', shape=[input_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    W_hi = tf.get_variable(name='weights_hi', shape=[hidden_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_i = tf.get_variable(name='bias_i', shape=[hidden_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "\n",
    "    # forget gate\n",
    "    W_xf = tf.get_variable(name='weights_xf', shape=[input_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    W_hf = tf.get_variable(name='weights_hf', shape=[hidden_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_f = tf.get_variable(name='bias_f', shape=[hidden_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    \n",
    "    # output gate\n",
    "    W_xo = tf.get_variable(name='weights_xo', shape=[input_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    W_ho = tf.get_variable(name='weights_ho', shape=[hidden_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_o = tf.get_variable(name='bias_o', shape=[hidden_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "                          \n",
    "    # candidate\n",
    "    W_xc = tf.get_variable(name='weights_xc', shape=[input_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    W_hc = tf.get_variable(name='weights_hc', shape=[hidden_dim, hidden_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_c = tf.get_variable(name='bias_c', shape=[hidden_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "    \n",
    "    W_hy = tf.get_variable(name='weights_y', shape=[hidden_dim, output_dim], initializer=tf.random_normal_initializer(mean=0.0, stddev=weight_scale), dtype=tf.float32)\n",
    "    b_y = tf.get_variable(name='bias_y', shape=[output_dim], initializer=tf.constant_initializer(0.0), dtype=tf.float32)\n",
    "\n",
    "\n",
    "params = [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hy, b_y]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "我们将前面的模型公式翻译成代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_rnn(inputs, state_h, state_c, *params):\n",
    "    # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    # H: 尺寸为 batch_size * hidden_dim 矩阵\n",
    "    # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
    "     W_hy, b_y] = params[0]\n",
    "\n",
    "    H = state_h\n",
    "    C = state_c\n",
    "    num_steps = inputs.get_shape().as_list()[0]\n",
    "    outputs = []\n",
    "    for i in range(num_steps):\n",
    "        X = inputs[i]\n",
    "        I = tf.nn.sigmoid(tf.matmul(X, W_xi) + tf.matmul(H, W_hi) + b_i)\n",
    "        F = tf.nn.sigmoid(tf.matmul(X, W_xf) + tf.matmul(H, W_hf) + b_f)\n",
    "        O = tf.nn.sigmoid(tf.matmul(X, W_xo) + tf.matmul(H, W_ho) + b_o)\n",
    "        C_tilda = tf.nn.tanh(tf.matmul(X, W_xc) + tf.matmul(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * tf.nn.tanh(C)\n",
    "        Y = tf.matmul(H, W_hy) + b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs, H, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "下面我们开始训练模型。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”。这里采用的是相邻批量采样实验门控循环单元谱写歌词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "here\n",
      "rnn/weights_xi\n",
      "rnn/weights_hi\n",
      "rnn/bias_i\n",
      "rnn/weights_xf\n",
      "rnn/weights_hf\n",
      "rnn/bias_f\n",
      "rnn/weights_xo\n",
      "rnn/weights_ho\n",
      "rnn/bias_o\n",
      "rnn/weights_xc\n",
      "rnn/weights_hc\n",
      "rnn/bias_c\n",
      "rnn/weights_y\n",
      "rnn/bias_y\n",
      "1465.0054\n",
      "1446.6422\n",
      "1336.6493\n",
      "579.2244\n",
      "629.6254\n",
      "568.9277\n",
      "491.1674\n",
      "485.17874\n",
      "501.9527\n",
      "456.99716\n",
      "521.6606\n",
      "492.75812\n",
      "446.82953\n",
      "537.7353\n",
      "418.52774\n",
      "394.7151\n",
      "386.33472\n",
      "314.07187\n",
      "353.33072\n",
      "399.35757\n",
      "306.4753\n",
      "395.93472\n",
      "316.72934\n",
      "395.2645\n",
      "343.06134\n",
      "310.43784\n",
      "360.92493\n",
      "358.66595\n",
      "382.51663\n",
      "362.1812\n",
      "349.73572\n",
      "317.208\n",
      "366.74347\n",
      "301.0899\n",
      "296.95175\n",
      "335.08267\n",
      "302.10104\n",
      "285.61328\n",
      "318.74973\n",
      "337.79404\n",
      "312.68756\n",
      "332.42575\n",
      "345.10153\n",
      "267.1354\n",
      "294.80148\n",
      "326.2878\n",
      "294.47723\n",
      "306.24362\n",
      "324.6005\n",
      "255.1995\n",
      "299.3433\n",
      "271.00662\n",
      "276.6562\n",
      "299.6069\n",
      "252.93404\n",
      "275.5353\n",
      "277.9437\n",
      "277.24637\n",
      "298.81192\n",
      "281.11435\n",
      "263.84335\n",
      "259.11795\n",
      "260.08115\n",
      "269.55942\n",
      "264.33408\n",
      "230.79706\n",
      "233.93735\n",
      "235.9854\n",
      "242.55965\n",
      "239.12659\n",
      "202.14499\n",
      "205.45868\n",
      "223.99442\n",
      "216.43933\n",
      "196.0131\n",
      "205.93192\n",
      "227.01497\n",
      "220.35992\n",
      "180.69107\n",
      "205.53893\n",
      "231.59842\n",
      "230.21628\n",
      "192.9043\n",
      "168.32649\n",
      "203.82167\n",
      "187.75922\n",
      "157.27798\n",
      "157.66026\n",
      "142.32602\n",
      "174.02148\n",
      "143.9967\n",
      "146.73796\n",
      "129.66472\n",
      "108.03884\n",
      "142.97528\n",
      "142.23225\n",
      "139.95552\n",
      "149.83495\n",
      "141.09122\n",
      "135.95312\n",
      "166.8487\n",
      "158.51064\n",
      "87.787476\n",
      "94.4444\n",
      "105.12659\n",
      "113.03779\n",
      "96.53759\n",
      "84.917274\n",
      "90.23283\n",
      "90.29967\n",
      "95.59541\n",
      "86.12072\n",
      "72.76491\n",
      "78.89315\n",
      "87.44109\n",
      "92.934654\n",
      "89.53967\n",
      "68.83956\n",
      "86.59753\n",
      "46.274834\n",
      "64.31909\n",
      "46.88882\n",
      "62.305016\n",
      "58.512314\n",
      "60.27308\n",
      "49.49728\n",
      "43.24092\n",
      "45.78775\n",
      "51.918247\n",
      "44.727577\n",
      "43.459164\n",
      "44.107925\n",
      "43.68434\n",
      "49.031467\n",
      "46.01323\n",
      "48.96718\n",
      "35.25745\n",
      "23.504559\n",
      "25.911066\n",
      "32.906494\n",
      "22.551764\n",
      "27.638384\n",
      "41.153687\n",
      "24.819939\n",
      "23.30499\n",
      "27.909779\n",
      "28.831528\n",
      "23.316202\n",
      "25.687485\n",
      "26.495829\n",
      "23.880713\n",
      "21.76666\n",
      "21.003967\n",
      "13.433068\n",
      "11.410735\n",
      "17.351906\n",
      "17.136679\n",
      "19.457422\n",
      "17.943882\n",
      "14.508266\n",
      "19.578472\n",
      "18.1676\n",
      "14.828042\n",
      "16.561268\n",
      "13.379077\n",
      "10.503421\n",
      "13.850666\n",
      "16.438622\n",
      "11.995598\n",
      "12.214847\n",
      "9.063096\n",
      "9.5938015\n",
      "9.114837\n",
      "9.808903\n",
      "9.128668\n",
      "10.452676\n",
      "9.884555\n",
      "9.832853\n",
      "6.4188766\n",
      "11.314855\n",
      "7.943675\n",
      "8.382989\n",
      "6.190232\n",
      "8.754478\n",
      "8.601667\n",
      "7.5467067\n",
      "7.6910286\n",
      "6.9910064\n",
      "6.452027\n",
      "6.643524\n",
      "5.511187\n",
      "5.7770677\n",
      "5.1615734\n",
      "5.9877944\n",
      "6.75871\n",
      "5.872888\n",
      "5.8577867\n",
      "6.324845\n",
      "4.9865317\n",
      "5.348063\n",
      "4.8126493\n",
      "7.6685367\n",
      "5.0562477\n",
      "4.5465126\n",
      "3.928092\n",
      "4.9603825\n",
      "3.2693987\n",
      "4.066045\n",
      "5.006056\n",
      "3.086\n",
      "3.7642083\n",
      "3.6446447\n",
      "4.663902\n",
      "3.4405587\n",
      "3.367144\n",
      "4.237152\n",
      "4.4359155\n",
      "3.1090262\n",
      "4.445451\n",
      "4.82963\n",
      "3.7614133\n",
      "3.0392826\n",
      "2.793961\n",
      "2.942056\n",
      "2.8255322\n",
      "2.483704\n",
      "2.7576919\n",
      "2.9288578\n",
      "3.057053\n",
      "3.2843819\n",
      "3.1596823\n",
      "2.86488\n",
      "3.0111947\n",
      "3.1258798\n",
      "2.7502162\n",
      "2.893927\n",
      "2.5523782\n",
      "3.3791833\n",
      "2.368118\n",
      "2.3902075\n",
      "2.2408063\n",
      "2.4059904\n",
      "2.5965207\n",
      "2.8209424\n",
      "2.4833584\n",
      "1.9841162\n",
      "2.4423633\n",
      "2.249157\n",
      "2.3933485\n",
      "2.1174128\n",
      "2.388078\n",
      "2.3921824\n",
      "2.285608\n",
      "2.6195066\n",
      "2.4934328\n",
      "2.1449938\n",
      "1.9505304\n",
      "1.9906712\n",
      "2.3245728\n",
      "2.2642438\n",
      "1.9140521\n",
      "2.2259965\n",
      "1.899689\n",
      "1.6765785\n",
      "2.0642145\n",
      "2.0866039\n",
      "1.8420377\n",
      "1.8671616\n",
      "1.9796398\n",
      "1.7271202\n",
      "2.012938\n",
      "1.8709507\n",
      "1.6946014\n",
      "1.7695154\n",
      "1.5786008\n",
      "1.707787\n",
      "1.7793683\n",
      "1.8308746\n",
      "1.6301293\n",
      "1.6285061\n",
      "1.6077685\n",
      "1.7954773\n",
      "1.7414593\n",
      "1.8830466\n",
      "1.8215942\n",
      "1.6474097\n",
      "1.8178103\n",
      "1.6531723\n",
      "1.6078457\n",
      "1.533152\n",
      "1.4431605\n",
      "1.3936571\n",
      "1.6027076\n",
      "1.5623479\n",
      "1.3977171\n",
      "1.5442013\n",
      "1.4926518\n",
      "1.4829082\n",
      "1.5222878\n",
      "1.5513766\n",
      "1.9940386\n",
      "1.4374199\n",
      "1.5627297\n",
      "1.6846807\n",
      "1.4812593\n",
      "1.6491367\n",
      "1.4059317\n",
      "1.4867424\n",
      "1.4339252\n",
      "1.4063785\n",
      "1.4576662\n",
      "1.4472965\n",
      "1.4355454\n",
      "1.4036825\n",
      "1.4389116\n",
      "1.4240596\n",
      "1.4759005\n",
      "1.4657209\n",
      "1.4002556\n",
      "1.3625946\n",
      "1.4384611\n",
      "1.4611424\n",
      "1.4612944\n",
      "1.4341592\n",
      "1.3492578\n",
      "1.2728732\n",
      "1.2776108\n",
      "1.365686\n",
      "1.3714277\n",
      "1.31483\n",
      "1.3071606\n",
      "1.3871136\n",
      "1.3852595\n",
      "1.2356403\n",
      "1.3960924\n",
      "1.2379074\n",
      "1.371787\n",
      "1.3744615\n",
      "1.3541331\n",
      "1.3680736\n",
      "1.2362635\n",
      "1.2260128\n",
      "1.1976408\n",
      "1.3212143\n",
      "1.2821901\n",
      "1.2722343\n",
      "1.2982374\n",
      "1.2862365\n",
      "1.2349012\n",
      "1.2271829\n",
      "1.2405192\n",
      "1.3359013\n",
      "1.3063915\n",
      "1.2972443\n",
      "1.3165865\n",
      "1.3095568\n",
      "1.2665238\n",
      "1.2414808\n",
      "1.1547453\n",
      "1.203268\n",
      "1.2325509\n",
      "1.27521\n",
      "1.21454\n",
      "1.2492466\n",
      "1.160821\n",
      "1.1894639\n",
      "1.2190365\n",
      "1.2189665\n",
      "1.2776042\n",
      "1.245935\n",
      "1.1746659\n",
      "1.2263161\n",
      "1.2047011\n",
      "1.2764865\n",
      "1.1873587\n",
      "1.1842642\n",
      "1.1917311\n",
      "1.1569788\n",
      "1.1823242\n",
      "1.1892868\n",
      "1.1519809\n",
      "1.1834803\n",
      "1.1763592\n",
      "1.1810328\n",
      "1.2426434\n",
      "1.152304\n",
      "1.1676017\n",
      "1.2414792\n",
      "1.1998856\n",
      "1.1659346\n",
      "1.2291178\n",
      "1.1744522\n",
      "1.1514021\n",
      "1.1434772\n",
      "1.1362177\n",
      "1.1562121\n",
      "1.1818547\n",
      "1.1327704\n",
      "1.1459972\n",
      "1.1591327\n",
      "1.1940905\n",
      "1.1744401\n",
      "1.1453875\n",
      "1.1687914\n",
      "1.1556557\n",
      "1.1518503\n",
      "1.1404828\n",
      "1.166005\n",
      "1.1372694\n",
      "1.1534667\n",
      "1.1394409\n",
      "1.1345259\n",
      "1.132621\n",
      "1.1274322\n",
      "1.1441463\n",
      "1.1372981\n",
      "1.1252621\n",
      "1.136915\n",
      "1.151596\n",
      "1.1312437\n",
      "1.1226861\n",
      "1.118737\n",
      "1.1449946\n",
      "1.1187726\n",
      "1.1615965\n",
      "1.0992342\n",
      "1.1058953\n",
      "1.1077344\n",
      "1.1070904\n",
      "1.1005266\n",
      "1.1010633\n",
      "1.1341923\n",
      "1.1143851\n",
      "1.1131892\n",
      "1.1341423\n",
      "1.1391724\n",
      "1.1311574\n",
      "1.1193658\n",
      "1.1531214\n",
      "1.1094679\n",
      "1.1360735\n",
      "1.1271663\n",
      "1.1048237\n",
      "1.085544\n",
      "1.1030442\n",
      "1.0942043\n",
      "1.1105307\n",
      "1.1086233\n",
      "1.1019349\n",
      "1.104209\n",
      "1.0876235\n",
      "1.0862285\n",
      "1.1056416\n",
      "1.090487\n",
      "1.1087228\n",
      "1.1230536\n",
      "1.0931687\n",
      "1.1240544\n",
      "1.1353729\n",
      "1.0909058\n",
      "1.0866076\n",
      "1.0837423\n",
      "1.0860997\n",
      "1.0834358\n",
      "1.100425\n",
      "1.0994668\n",
      "1.0943033\n",
      "1.1008663\n",
      "1.0866237\n",
      "1.088556\n",
      "1.0914624\n",
      "1.0891913\n",
      "1.1082522\n",
      "1.1061785\n",
      "1.0943129\n",
      "1.097369\n",
      "1.0918391\n",
      "1.0756319\n",
      "1.076379\n",
      "1.0851624\n",
      "1.0771855\n",
      "1.0848999\n",
      "1.0755407\n",
      "1.090001\n",
      "1.0897084\n",
      "1.0850713\n",
      "1.0720466\n",
      "1.075897\n",
      "1.1086702\n",
      "1.0962516\n",
      "1.1047996\n",
      "1.0984868\n",
      "1.0999103\n",
      "1.0745281\n",
      "1.0708708\n",
      "1.0575701\n",
      "1.0822208\n",
      "1.0842556\n",
      "1.0888286\n",
      "1.0873033\n",
      "1.0974399\n",
      "1.0736917\n",
      "1.0855818\n",
      "1.0805508\n",
      "1.0854539\n",
      "1.0832508\n",
      "1.0812298\n",
      "1.08639\n",
      "1.0680249\n",
      "1.0991206\n",
      "1.0517259\n",
      "1.0806508\n",
      "1.0577798\n",
      "1.0768676\n",
      "1.0717757\n",
      "1.0836622\n",
      "1.0671434\n",
      "1.0794041\n",
      "1.0769252\n",
      "1.078437\n",
      "1.1100944\n",
      "1.0781269\n",
      "1.097337\n",
      "1.076843\n",
      "1.0731064\n",
      "1.0844129\n",
      "1.096718\n",
      "1.0734193\n",
      "1.0684129\n",
      "1.0720775\n",
      "1.0602993\n",
      "1.0692211\n",
      "1.0819007\n",
      "1.0908685\n",
      "1.0574635\n",
      "1.0797282\n",
      "1.067096\n",
      "1.0668257\n",
      "1.083439\n",
      "1.0754184\n",
      "1.0866394\n",
      "1.0833448\n",
      "1.0863883\n",
      "1.0813898\n",
      "1.0662316\n",
      "1.0610279\n",
      "1.0673758\n",
      "1.0786847\n",
      "1.069069\n",
      "1.0645409\n",
      "1.0815823\n",
      "1.0685731\n",
      "1.0600771\n",
      "1.0989519\n",
      "1.0882022\n",
      "1.083777\n",
      "1.0954984\n",
      "1.0695987\n",
      "1.0636829\n",
      "1.1177393\n",
      "1.0815405\n",
      "1.065083\n",
      "1.0599866\n",
      "1.0638878\n",
      "1.0845666\n",
      "1.0760422\n",
      "1.0775177\n",
      "1.0624353\n",
      "1.067135\n",
      "1.0839769\n",
      "1.0730541\n",
      "1.0798199\n",
      "1.0876867\n",
      "1.0947603\n",
      "1.0679135\n",
      "1.065814\n",
      "1.0931544\n",
      "1.0841055\n",
      "1.0629848\n",
      "1.0644133\n",
      "1.0680517\n",
      "1.0777733\n",
      "1.0676316\n",
      "1.0521846\n",
      "1.0748976\n",
      "1.059984\n",
      "1.0803711\n",
      "1.0680482\n",
      "1.0864794\n",
      "1.0685949\n",
      "1.0689706\n",
      "1.0844736\n",
      "1.0823784\n",
      "1.0719837\n",
      "1.0966611\n",
      "1.0796926\n",
      "1.063983\n",
      "1.0525064\n",
      "1.0602274\n",
      "1.0643971\n",
      "1.0738872\n",
      "1.0716503\n",
      "1.0645374\n",
      "1.0675588\n",
      "1.0795742\n",
      "1.0859615\n",
      "1.076835\n",
      "1.0658846\n",
      "1.0877169\n",
      "1.0575911\n",
      "1.0867394\n",
      "1.0897118\n",
      "1.0781407\n",
      "1.0654235\n",
      "1.0553133\n",
      "1.0670956\n",
      "1.0627717\n",
      "1.0660051\n",
      "1.0590823\n",
      "1.080101\n",
      "1.0713077\n",
      "1.0610515\n",
      "1.0607839\n",
      "1.0735435\n",
      "1.0449262\n",
      "1.0909458\n",
      "1.0872743\n",
      "1.081541\n",
      "1.0812888\n",
      "1.0719543\n",
      "1.0575068\n",
      "1.0561668\n",
      "1.0832844\n",
      "1.0502121\n",
      "1.046587\n",
      "1.0608478\n",
      "1.07254\n",
      "1.0536352\n",
      "1.0842116\n",
      "1.0832431\n",
      "1.0662558\n",
      "1.0766602\n",
      "1.055274\n",
      "1.0801105\n",
      "1.0749556\n",
      "1.073657\n",
      "1.0470577\n",
      "1.0754641\n",
      "1.0755005\n",
      "1.0497878\n",
      "1.0808425\n",
      "1.0746088\n",
      "1.05643\n",
      "1.062832\n",
      "1.0728194\n",
      "1.0507601\n",
      "1.0454446\n",
      "1.0807233\n",
      "1.0883657\n",
      "1.0845628\n",
      "1.074765\n",
      "1.0703099\n",
      "1.0649121\n",
      "1.0549223\n",
      "1.0580406\n",
      "1.0644228\n",
      "1.0647165\n",
      "1.051971\n",
      "1.0528601\n",
      "1.0764838\n",
      "1.0593306\n",
      "1.0814717\n",
      "1.044226\n",
      "1.0649282\n",
      "1.044488\n",
      "1.0581063\n",
      "1.0679966\n",
      "1.0844315\n",
      "1.0818098\n",
      "1.0879036\n",
      "1.0470843\n",
      "1.0564837\n",
      "1.0616667\n",
      "1.0626316\n",
      "1.0672754\n",
      "1.059557\n",
      "1.0520158\n",
      "1.0580794\n",
      "1.0672622\n",
      "1.0640633\n",
      "1.0772597\n",
      "1.0502881\n",
      "1.0537597\n",
      "1.0674629\n",
      "1.0890743\n",
      "1.0597854\n",
      "1.0646862\n",
      "1.0623717\n",
      "1.0608935\n",
      "1.0543085\n",
      "1.0636643\n",
      "1.0524596\n",
      "1.044367\n",
      "1.0488713\n",
      "1.0466304\n",
      "1.0649118\n",
      "1.0892189\n",
      "1.0653764\n",
      "1.0565273\n",
      "1.0484297\n",
      "1.0619953\n",
      "1.0768445\n",
      "1.0567666\n",
      "1.0634174\n",
      "1.0578679\n",
      "1.0400892\n",
      "1.0512167\n",
      "1.0437425\n",
      "1.0547105\n",
      "1.052927\n",
      "1.0508349\n",
      "1.0541923\n",
      "1.0575272\n",
      "1.0491362\n",
      "1.0540509\n",
      "1.0605928\n",
      "1.0573986\n",
      "1.0664943\n",
      "1.0546831\n",
      "1.0752901\n",
      "1.0700653\n",
      "1.043757\n",
      "1.0390972\n",
      "1.0602728\n",
      "1.058816\n",
      "1.0587561\n",
      "1.0444406\n",
      "1.0494457\n",
      "1.0539258\n",
      "1.0591254\n",
      "1.0475452\n",
      "1.0568242\n",
      "1.0542673\n",
      "1.0546614\n",
      "1.0566251\n",
      "1.0583147\n",
      "1.0545081\n",
      "1.0801622\n",
      "1.0455749\n",
      "1.04424\n",
      "1.0464704\n",
      "1.066615\n",
      "1.0395223\n",
      "1.040214\n",
      "1.0533757\n",
      "1.049428\n",
      "1.0606166\n",
      "1.0552751\n",
      "1.0494899\n",
      "1.0665643\n",
      "1.0482723\n",
      "1.0654016\n",
      "1.0567752\n",
      "1.0454643\n",
      "1.0630695\n",
      "1.0468162\n",
      "1.0524846\n",
      "1.0372633\n",
      "1.0400581\n",
      "1.0439098\n",
      "1.0374118\n",
      "1.0543083\n",
      "1.0564119\n",
      "1.0399078\n",
      "1.0547619\n",
      "1.0728922\n",
      "1.056737\n",
      "1.0712569\n",
      "1.0651841\n",
      "1.0564557\n",
      "1.0673146\n",
      "1.0719743\n",
      "1.0475909\n",
      "1.0536138\n",
      "1.0472054\n",
      "1.0397851\n",
      "1.0518581\n",
      "1.0357572\n",
      "1.0469246\n",
      "1.0677595\n",
      "1.0781275\n",
      "1.0617534\n",
      "1.0631691\n",
      "1.0619334\n",
      "1.0558177\n",
      "1.0726155\n",
      "1.048775\n",
      "1.0799875\n",
      "1.0612587\n",
      "1.0484756\n",
      "1.0689657\n",
      "1.0567224\n",
      "1.0550843\n",
      "1.049549\n",
      "1.050904\n",
      "1.06589\n",
      "1.061853\n",
      "1.052825\n",
      "1.0641288\n",
      "1.0409456\n",
      "1.0461856\n",
      "1.0549681\n",
      "1.0557605\n",
      "1.0628293\n",
      "1.0592977\n",
      "1.043077\n",
      "1.0333118\n",
      "1.0389512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.044435\n",
      "1.0503662\n",
      "1.0454395\n",
      "1.052401\n",
      "1.0617268\n",
      "1.0606318\n",
      "1.0443627\n",
      "1.0615921\n",
      "1.055911\n",
      "1.0452732\n",
      "1.0589021\n",
      "1.0666507\n",
      "1.0601906\n",
      "1.0733757\n",
      "1.0527297\n",
      "1.0531968\n",
      "1.0344459\n",
      "1.0609926\n",
      "1.039907\n",
      "1.0649062\n",
      "1.0403237\n",
      "1.0701368\n",
      "1.0602999\n",
      "1.0431423\n",
      "1.0535117\n",
      "1.06745\n",
      "1.0511435\n",
      "1.0586345\n",
      "1.0675024\n",
      "1.0696015\n",
      "1.0564175\n",
      "1.0650334\n"
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "learning_rate = 1e-1\n",
    "max_steps = 10000\n",
    "batch_size = 32\n",
    "train_loss = 0.0\n",
    "train_acc = 0.0\n",
    "is_random_iter= True\n",
    "epochs= 50\n",
    "num_steps= 35 \n",
    "learning_rate= 1e-2\n",
    "batch_size= 32\n",
    "is_lstm = True\n",
    "\n",
    "#训练\n",
    "print hidden_dim\n",
    "num_inputs = num_outputs = vocab_size\n",
    "input_placeholder = tf.placeholder(tf.float32, [num_steps, None, num_inputs])\n",
    "state_h_placeholder = tf.placeholder(tf.float32, [None, hidden_dim])\n",
    "print 'here'\n",
    "\n",
    "state_c_placeholder = tf.placeholder(tf.float32, [None, hidden_dim])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [num_steps, None, 1])\n",
    "\n",
    "if is_lstm:\n",
    "    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "    outputs, state_h, state_c = lstm_rnn(input_placeholder, state_h_placeholder, state_c_placeholder, params)\n",
    "else:\n",
    "    outputs, state_h = rnn(input_placeholder, state_h_placeholder, params)\n",
    "\n",
    "outputs = tf.concat(outputs, axis=0)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(logits=outputs,  labels=tf.reshape(gt_placeholder, (num_steps*batch_size, 1)))\n",
    "\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "op = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "gradients = tf.gradients(loss, params)\n",
    "\n",
    "#process gradients\n",
    "clipped_gradients, norm = tf.clip_by_global_norm(gradients, 1)\n",
    "\n",
    "train_op = op.apply_gradients(zip(clipped_gradients, params))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "if is_random_iter:\n",
    "    data_iter = data_iter_random\n",
    "else:\n",
    "    data_iter = data_iter_consecutive\n",
    "for e in range(0, epochs):\n",
    "    # 如使用相邻批量采样，在同一个epoch中，隐含变量只需要在该epoch开始的时候初始化。\n",
    "    if not is_random_iter:\n",
    "        state_h_init = np.zeros(shape=(batch_size, hidden_dim))\n",
    "        if is_lstm:\n",
    "            # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "            state_c_init = np.zeros((batch_size, hidden_dim))\n",
    "    train_loss, num_examples = 0, 0\n",
    "\n",
    "    for data, label in data_iter(corpus_indices, batch_size, num_steps):\n",
    "        # 如使用随机批量采样，处理每个随机小批量前都需要初始化隐含变量。\n",
    "        if is_random_iter:\n",
    "            state_h_init = np.zeros(shape=(batch_size, hidden_dim))\n",
    "            if is_lstm:\n",
    "                # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "                state_c_init = np.zeros((batch_size, hidden_dim))\n",
    "                feed_dict = {input_placeholder: get_inputs(data), state_h_placeholder: state_h_init, state_c_placeholder: state_c_init, gt_placeholder: np.expand_dims(label.T, axis=-1)}\n",
    "                loss_, state_h_, state_c_, _ = sess.run([loss, state_h, state_c, train_op], feed_dict=feed_dict)\n",
    "                state_h_init = state_h_\n",
    "                state_c_init = state_c_\n",
    "            else:\n",
    "                feed_dict = {input_placeholder: get_inputs(data), state_h_placeholder: state_h_init, gt_placeholder: np.expand_dims(label.T, axis=-1)}\n",
    "                loss_, state_h_, _ = sess.run([loss, state_h, train_op], feed_dict=feed_dict)\n",
    "                state_h_init = state_h_\n",
    "\n",
    "        print np.exp(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分开\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "num_steps = 35\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "seq1 = '分开'.decode('utf-8')\n",
    "seq2 = '不分开'.decode('utf-8')\n",
    "seq3 = '战争中部队'.decode('utf-8')\n",
    "seqs = [seq1, seq2, seq3]\n",
    "print seq1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-66bfb44cf4d5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-66bfb44cf4d5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    我们先采用随机批量采样实验循环神经网络谱写歌词。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "我们先采用随机批量采样实验循环神经网络谱写歌词。我们假定谱写歌词的前缀分别为“分开”、“不分开”和“战争中部队”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn/weights_xi\n",
      "rnn/weights_hi\n",
      "rnn/bias_i\n",
      "rnn/weights_xf\n",
      "rnn/weights_hf\n",
      "rnn/bias_f\n",
      "rnn/weights_xo\n",
      "rnn/weights_ho\n",
      "rnn/bias_o\n",
      "rnn/weights_xc\n",
      "rnn/weights_hc\n",
      "rnn/bias_c\n",
      "rnn/weights_y\n",
      "rnn/bias_y\n",
      "分开始可爱女人 谁肯安慰 我们 半兽人 为什么我只剩下枪生 眼泪水在我有一条热 单已 我都没有一切 没人在人在人在人在人在人在战壕里没人帮着你会没人在屋檐走在弹唱 小铁步一步往 爬满的飞 一枚铜币的军河挡 \n",
      "()\n",
      "不分开的天堂景象 没有你甘会听 连咪都ㄟ 但是因 我给你的路 那些事 你慢慢慢去没人在美 我不好慢 我们将会分开 没人 离开始轮有 白墙黑口被象 你拿 告诉我们一起毕业的学校 反射出世事看你 连咪都提 我们一\n",
      "()\n",
      "战争中部队  谁肯定 没法挑剔它 漫天黄沙凉过 塞北的怀念 他们 半兽人 你叫我也许就快实现在我以让生命就这样的过去 试着黑色幽默 那首来没人在人在人在人在人在你在我有你ㄟ鱼 单影 有我害怕你心疼的我都没有错亏我\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "pred_len = 100\n",
    "test_num_steps = 1\n",
    "# 预测\n",
    "num_inputs = num_outputs = vocab_size    \n",
    "test_input_placeholder = tf.placeholder(tf.float32, [test_num_steps, None, num_inputs])\n",
    "test_state_h_placeholder = tf.placeholder(tf.float32, [test_num_steps, hidden_dim])\n",
    "test_state_c_placeholder = tf.placeholder(tf.float32, [test_num_steps, hidden_dim])\n",
    "\n",
    "if is_lstm:\n",
    "    # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "    outputs, state_h, state_c = lstm_rnn(test_input_placeholder, test_state_h_placeholder, test_state_c_placeholder, params)\n",
    "else:\n",
    "    outputs, state_h = rnn(test_input_placeholder, test_state_h_placeholder, params)\n",
    "\n",
    "outputs = tf.concat(outputs, axis=0)\n",
    "\n",
    "\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "\n",
    "\n",
    "for seq in seqs:\n",
    "    prefix = seq\n",
    "\n",
    "    prefix = prefix.lower()\n",
    "    test_state_h_init = np.zeros((1, hidden_dim))\n",
    "    if is_lstm:\n",
    "        # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "        test_state_c_init = np.zeros((1, hidden_dim))\n",
    "    output_seq = [char_to_idx[prefix[0]]]\n",
    "    for i in range(pred_len + len(prefix)):\n",
    "        X = np.array([output_seq[-1]])\n",
    "        # 在序列中循环迭代隐含变量。\n",
    "        if is_lstm:\n",
    "            # 当RNN使用LSTM时才会用到，这里可以忽略。\n",
    "            Y, state_h_, state_c_ = sess.run([outputs, state_h, state_c], feed_dict={test_input_placeholder: get_inputs(X), test_state_c_placeholder: test_state_c_init, test_state_h_placeholder: test_state_h_init})\n",
    "            test_state_h_init = state_c_\n",
    "        else:\n",
    "            Y, state_h_ = sess.run([outputs, state_h], feed_dict={test_input_placeholder: get_inputs(X), test_state_h_placeholder: test_state_h_init})\n",
    "        test_state_h_init = state_h_\n",
    "        if i < len(prefix)-1:\n",
    "            next_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            next_input = np.argmax(Y[0])\n",
    "        output_seq.append(next_input)\n",
    "    print ''.join([idx_to_char[i] for i in output_seq])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
