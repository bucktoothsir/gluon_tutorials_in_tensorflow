{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DenseNet：稠密连接的卷积神经网络\n",
    "ResNet的跨层连接思想影响了接下来的众多工作。这里我们介绍其中的一个：DenseNet。下图展示了这两个的主要区别：\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/densenet.svg)\n",
    "\n",
    "可以看到DenseNet里来自跳层的输出不是通过加法（+）而是拼接（concat）来跟目前层的输出合并。因为是拼接，所以底层的输出会保留的进入上面所有层。这是为什么叫“稠密连接”的原因\n",
    "\n",
    "### 稠密块（Dense Block）\n",
    "我们先来定义一个稠密连接块。DenseNet的卷积块使用ResNet改进版本的``BN->Relu->Conv``。每个卷积的输出通道数被称之为``growth_rate``，这是因为假设输出为``in_channels``，而且有``layers``层，那么输出的通道数就是``in_channels+growth_rate*layers``。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense/concat_1:0\", shape=(4, 8, 8, 23), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def conv_block(input, channels, is_training=False, scope='conv'):\n",
    "    with tf.variable_scope(scope):\n",
    "        bn = slim.batch_norm(input, is_training=is_training)\n",
    "        relu = tf.nn.relu(bn)\n",
    "        conv = slim.conv2d(relu, channels, [3, 3], padding='SAME')\n",
    "        return conv\n",
    "\n",
    "def DenseBlock(input, num_layers, growth_rate, is_training=False, scope='dense'):\n",
    "    with tf.variable_scope(scope):\n",
    "        for i in range(num_layers):\n",
    "            conv = conv_block(input, growth_rate, is_training=is_training, scope='block'+str(i))\n",
    "            input = tf.concat([input, conv], -1)\n",
    "        return input \n",
    "\n",
    "\n",
    "x = np.random.uniform(size=(4,8,8,3)).astype(np.float32)\n",
    "print DenseBlock(x, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过渡块（Transition Block）\n",
    "因为使用拼接的缘故，每经过一次拼接输出通道数可能会激增。为了控制模型复杂度，这里引入一个过渡块，它不仅把输入的长宽减半，同时也使用$1×1$卷积来改变通道数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transition_block/AvgPool2D/AvgPool:0\", shape=(4, 4, 4, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def transition_block(input, channels, is_training=False, scope='transition_block'):\n",
    "    with tf.variable_scope(scope):\n",
    "        bn = slim.batch_norm(input, is_training=is_training)\n",
    "        relu = tf.nn.relu(bn)\n",
    "        conv = slim.conv2d(relu, channels, [1, 1])\n",
    "        return slim.avg_pool2d(conv, [2, 2], 2)\n",
    "\n",
    "print transition_block(x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet\n",
    "DenseNet的主体就是交替串联稠密块和过渡块。它使用全局的`growth_rate`使得配置更加简单。过渡层每次都将通道数减半。下面定义一个121层的DenseNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_channels = 64\n",
    "growth_rate = 32\n",
    "block_layers = [6, 12, 24, 16]\n",
    "num_classes = 10\n",
    "\n",
    "def dense_net(input, is_training=False, scope='densenet'):\n",
    "    with tf.variable_scope(scope):\n",
    "        # first block\n",
    "        input = tf.pad(input, [[0,0],[3,3],[3,3],[0,0]])\n",
    "        conv1 = slim.conv2d(input, init_channels, [7, 7], stride=2, scope='conv1')\n",
    "        bn1 = slim.batch_norm(conv1, is_training=is_training)\n",
    "        relu1 = tf.nn.relu(bn1)\n",
    "        relu1 = tf.pad(relu1, [[0,0],[1,1],[1,1],[0,0]])\n",
    "        pool1 = slim.max_pool2d(relu1, [3,3], 2)\n",
    "        \n",
    "        denseinput = pool1\n",
    "        \n",
    "        # dense blocks\n",
    "        channels = init_channels\n",
    "        for i, layers in enumerate(block_layers):\n",
    "            denseoutput = DenseBlock(denseinput, layers, channels, is_training=is_training, scope='dense'+str(i))\n",
    "            channels += layers * growth_rate\n",
    "            print 'num_channels: ' + str(channels)\n",
    "            if i != len(block_layers)-1:\n",
    "                denseoutput =  transition_block(denseoutput, channels/2, is_training=is_training, scope='transition_block'+str(i))\n",
    "            denseinput = denseoutput\n",
    "        \n",
    "        # last block\n",
    "        bn_last = slim.batch_norm(denseoutput, is_training=is_training)\n",
    "        relu_last = tf.nn.relu(bn_last)\n",
    "        pool_last = slim.avg_pool2d(relu_last, [1,1])\n",
    "        return slim.fully_connected(slim.flatten(pool_last), num_classes, activation_fn=None, scope='fc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据并训练\n",
    "因为这里我们使用了比较深的网络，所以我们进一步把输入减少到$32×32$来训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print test_images.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)\n",
    "test_dataset = DataSet(test_images, test_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_channels: 256\n",
      "num_channels: 640\n",
      "num_channels: 1408\n",
      "num_channels: 1920\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "max_steps = 1000\n",
    "batch_size = 8\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "resize_input = tf.image.resize_images(input_placeholder, [32, 32])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "logits = dense_net(resize_input, is_training)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width, 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label, is_training: True}\n",
    "    loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)\n",
    "    print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "        \n",
    "for i in range(100):\n",
    "    test_data, test_label = test_dataset.next_batch(100)\n",
    "    test_data = np.reshape(test_data, (100, height, width, num_channels))\n",
    "    test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_data, gt_placeholder: test_label, is_training: False})\n",
    "    test_acc.append(test_acc_)\n",
    "print (\"Test Loss: %f, Test acc %f \" % (np.mean(test_loss_), np.mean(test_acc_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
