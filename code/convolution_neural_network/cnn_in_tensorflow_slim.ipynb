{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络 — 使用Gluon\n",
    "现在我们使用tensorflow/slim来实现上一章的卷积神经网络。\n",
    "\n",
    "### 定义模型\n",
    "下面是LeNet在tensorflow/slim里的实现，注意到我们不再需要实现去计算每层的输入大小，尤其是接在卷积后面的那个全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据和训练\n",
    "剩下的跟上一章没什么不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def net(input):\n",
    "    with tf.name_scope('lenet'):\n",
    "        conv1 = slim.conv2d(input, 20, [5, 5], scope='conv1_1', weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool_1')\n",
    "        conv2 = slim.conv2d(pool1, 50, [3, 3], scope='conv2_2', weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool_2')\n",
    "        pool2 = slim.flatten(pool2)\n",
    "        fc1 = slim.fully_connected(pool2, 128, scope='fc1', weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        fc2 = slim.fully_connected(fc1, 10, scope='fc2', activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        return conv1, conv2, fc1, fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope(var.op.name):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-42b1490a5316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#tf.reset_default_graph()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0minput_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;31m#input_placeholder = tf.placeholder(tf.float32, [None, height*width*num_channels])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print test_images.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)\n",
    "test_dataset = DataSet(test_images, test_labels, one_hot=True)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "max_steps = 1000\n",
    "batch_size = 256\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "#input_placeholder = tf.placeholder(tf.float32, [None, height*width*num_channels])\n",
    "\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "conv1, conv2, fc1, logits = net(input_placeholder)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits,  onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width))\n",
    "    \n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "with tf.name_scope('output'):\n",
    "    variable_summaries(conv1)\n",
    "    variable_summaries(conv2)\n",
    "    variable_summaries(fc1)\n",
    "    variable_summaries(logits)\n",
    "    \n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    variable_summaries(var)\n",
    "    \n",
    "with tf.name_scope('gradients'):\n",
    "\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "    grad_var_list = optimizer.compute_gradients(loss=loss, var_list=var_list)\n",
    "    grad_list = [grad for (grad, var) in grad_var_list]\n",
    "    for  grad, var in grad_var_list:\n",
    "        print var.op.name\n",
    "        with tf.name_scope(var.op.name):\n",
    "            variable_summaries(grad)\n",
    "        \n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('log/', sess.graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.431581, Train acc 0.140625 \n",
      "Test Loss: 1.541882, Test acc 0.540000 \n",
      "Batch 100, Loss: 0.707734, Train acc 0.875000 \n",
      "Test Loss: 0.680130, Test acc 0.870000 \n",
      "Batch 200, Loss: 0.598317, Train acc 0.878906 \n",
      "Test Loss: 0.675763, Test acc 0.880000 \n",
      "Batch 300, Loss: 0.645145, Train acc 0.882812 \n",
      "Test Loss: 0.550532, Test acc 0.920000 \n",
      "Batch 400, Loss: 0.623004, Train acc 0.882812 \n",
      "Test Loss: 0.578785, Test acc 0.880000 \n",
      "Batch 500, Loss: 0.602844, Train acc 0.875000 \n",
      "Test Loss: 0.665458, Test acc 0.860000 \n",
      "Batch 600, Loss: 0.541238, Train acc 0.906250 \n",
      "Test Loss: 0.559770, Test acc 0.900000 \n",
      "Batch 700, Loss: 0.569063, Train acc 0.925781 \n",
      "Test Loss: 0.556408, Test acc 0.930000 \n",
      "Batch 800, Loss: 0.513513, Train acc 0.914062 \n",
      "Test Loss: 0.623122, Test acc 0.920000 \n",
      "Batch 900, Loss: 0.549423, Train acc 0.914062 \n",
      "Test Loss: 0.613668, Test acc 0.870000 \n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label}\n",
    "    conv1_, summary_, loss_, acc_, _ = sess.run([conv1, merged, loss, acc, train_op], feed_dict=feed_dict)\n",
    "    train_writer.add_summary(summary_, step)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "        for i in range(100):\n",
    "            test_data, test_label = test_dataset.next_batch(100)\n",
    "            test_data = np.reshape(test_data, (100, height, width, num_channels))\n",
    "            test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_data, gt_placeholder: test_label})\n",
    "            test_acc.append(test_acc_)\n",
    "        print (\"Test Loss: %f, Test acc %f \" % (np.mean(test_loss_), np.mean(test_acc_)))\n",
    "\n",
    "train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
