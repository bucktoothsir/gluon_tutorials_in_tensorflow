{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG：使用重复元素的非常深的网络\n",
    "我们从Alexnet看到网络的层数的激增。这个意味着即使是用Gluon手动写代码一层一层的堆每一层也很麻烦，更不用说从0开始了。幸运的是编程语言提供了很好的方法来解决这个问题：函数和循环。如果网络结构里面有大量重复结构，那么我们可以很紧凑来构造这些网络。第一个使用这种结构的深度网络是VGG。\n",
    "\n",
    "### VGG架构\n",
    "VGG的一个关键是使用很多有着相对小的kernel（$3×3$）的卷积层然后接上一个池化层，之后再将这个模块重复多次。下面我们先定义一个这样的块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def vgg_block(input, num_convs, num_channels, scope='vgg_block'):\n",
    "    with tf.variable_scope(scope) as sc:\n",
    "        for i in xrange(num_convs):\n",
    "            input = tf.pad(input, [[0,0],[1,1],[1,1],[0,0]])\n",
    "            conv = slim.conv2d(input, num_channels, [3, 3], scope='conv' + str(i), padding='VALID')\n",
    "            input = conv\n",
    "        pool = slim.max_pool2d(conv, [2, 2], 2, scope='max_pool')\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实例化一个这样的块，里面有两个卷积层，每个卷积层输出通道是128：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们定义如何将这些块堆起来：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_stack(input, architecture):\n",
    "    for i, (num_convs, channels) in enumerate(architecture):\n",
    "        out = vgg_block(input, num_convs, channels, 'vgg_block_'+str(i))\n",
    "        input = out\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "这里我们定义一个最简单的一个VGG结构，它有8个卷积层，和跟Alexnet一样的3个全连接层。这个网络又称VGG 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 10\n",
    "architecture = ((1,64), (1,128), (2,256), (2,512), (2,512))\n",
    "def vggnet(input, is_training):\n",
    "    with tf.variable_scope('vgg') as sc:\n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected]):\n",
    "            output = vgg_stack(input, architecture)\n",
    "            output = slim.flatten(output)\n",
    "            fc1 = slim.fully_connected(output, 4096, scope='fc1')\n",
    "            fc1 = slim.dropout(fc1, keep_prob=0.5, is_training=is_training)\n",
    "            \n",
    "            fc2 = slim.fully_connected(fc1, 4096, scope='fc2')\n",
    "            fc2 = slim.dropout(fc2, keep_prob=0.5, is_training=is_training)     \n",
    "            \n",
    "            return slim.fully_connected(fc2, num_outputs, scope='fc3', activation_fn=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "这里跟Alexnet的训练代码一样除了我们只将图片扩大到$96×96$来节省些计算，和默认使用稍微大点的学习率。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print test_images.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)\n",
    "test_dataset = DataSet(test_images, test_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.py:560: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(tensor.tensor_content, dtype=dtype).reshape(shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-3-6930ad184bd9>(7)vggnet()\n",
      "-> output = vgg_stack(input, architecture)\n",
      "(Pdb) c\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Batch 0, Loss: 2.302390, Train acc 0.109375 \n",
      "Batch 1, Loss: 2.301980, Train acc 0.093750 \n",
      "Batch 2, Loss: 2.303169, Train acc 0.156250 \n",
      "Batch 3, Loss: 2.302202, Train acc 0.109375 \n",
      "Batch 4, Loss: 2.301865, Train acc 0.109375 \n",
      "Batch 5, Loss: 2.302718, Train acc 0.109375 \n",
      "Batch 6, Loss: 2.300086, Train acc 0.125000 \n",
      "Batch 7, Loss: 2.297577, Train acc 0.140625 \n",
      "Batch 8, Loss: 2.303068, Train acc 0.109375 \n",
      "Batch 9, Loss: 2.304276, Train acc 0.046875 \n",
      "Batch 10, Loss: 2.306055, Train acc 0.062500 \n",
      "Batch 11, Loss: 2.295269, Train acc 0.140625 \n",
      "Batch 12, Loss: 2.298706, Train acc 0.109375 \n",
      "Batch 13, Loss: 2.301336, Train acc 0.062500 \n",
      "Batch 14, Loss: 2.298002, Train acc 0.125000 \n",
      "Batch 15, Loss: 2.297425, Train acc 0.140625 \n",
      "Batch 16, Loss: 2.298120, Train acc 0.109375 \n",
      "Batch 17, Loss: 2.298824, Train acc 0.109375 \n",
      "Batch 18, Loss: 2.300314, Train acc 0.109375 \n",
      "Batch 19, Loss: 2.296943, Train acc 0.156250 \n",
      "Batch 20, Loss: 2.298516, Train acc 0.109375 \n",
      "Batch 21, Loss: 2.294805, Train acc 0.078125 \n",
      "Batch 22, Loss: 2.292089, Train acc 0.187500 \n",
      "Batch 23, Loss: 2.293770, Train acc 0.125000 \n",
      "Batch 24, Loss: 2.299814, Train acc 0.031250 \n",
      "Batch 25, Loss: 2.292371, Train acc 0.109375 \n",
      "Batch 26, Loss: 2.292305, Train acc 0.125000 \n",
      "Batch 27, Loss: 2.289232, Train acc 0.125000 \n",
      "Batch 28, Loss: 2.296508, Train acc 0.109375 \n",
      "Batch 29, Loss: 2.281915, Train acc 0.093750 \n",
      "Batch 30, Loss: 2.296657, Train acc 0.093750 \n",
      "Batch 31, Loss: 2.289458, Train acc 0.109375 \n",
      "Batch 32, Loss: 2.280043, Train acc 0.125000 \n",
      "Batch 33, Loss: 2.273917, Train acc 0.203125 \n",
      "Batch 34, Loss: 2.283221, Train acc 0.093750 \n",
      "Batch 35, Loss: 2.280112, Train acc 0.093750 \n",
      "Batch 36, Loss: 2.270617, Train acc 0.093750 \n",
      "Batch 37, Loss: 2.257901, Train acc 0.156250 \n",
      "Batch 38, Loss: 2.269341, Train acc 0.109375 \n",
      "Batch 39, Loss: 2.252603, Train acc 0.265625 \n",
      "Batch 40, Loss: 2.252915, Train acc 0.171875 \n",
      "Batch 41, Loss: 2.241446, Train acc 0.171875 \n",
      "Batch 42, Loss: 2.212504, Train acc 0.187500 \n",
      "Batch 43, Loss: 2.198920, Train acc 0.109375 \n",
      "Batch 44, Loss: 2.201719, Train acc 0.218750 \n",
      "Batch 45, Loss: 2.156088, Train acc 0.187500 \n",
      "Batch 46, Loss: 2.276280, Train acc 0.046875 \n",
      "Batch 47, Loss: 2.248315, Train acc 0.343750 \n",
      "Batch 48, Loss: 2.187596, Train acc 0.218750 \n",
      "Batch 49, Loss: 2.205338, Train acc 0.281250 \n",
      "Batch 50, Loss: 2.095579, Train acc 0.296875 \n",
      "Batch 51, Loss: 1.956721, Train acc 0.312500 \n",
      "Batch 52, Loss: 2.480269, Train acc 0.203125 \n",
      "Batch 53, Loss: 2.268116, Train acc 0.093750 \n",
      "Batch 54, Loss: 2.300022, Train acc 0.140625 \n",
      "Batch 55, Loss: 2.272226, Train acc 0.125000 \n",
      "Batch 56, Loss: 2.231108, Train acc 0.203125 \n",
      "Batch 57, Loss: 2.240775, Train acc 0.218750 \n",
      "Batch 58, Loss: 2.175913, Train acc 0.281250 \n",
      "Batch 59, Loss: 2.217926, Train acc 0.218750 \n",
      "Batch 60, Loss: 2.113502, Train acc 0.265625 \n",
      "Batch 61, Loss: 2.118222, Train acc 0.218750 \n",
      "Batch 62, Loss: 1.948982, Train acc 0.406250 \n",
      "Batch 63, Loss: 1.913801, Train acc 0.375000 \n",
      "Batch 64, Loss: 2.794878, Train acc 0.078125 \n",
      "Batch 65, Loss: 2.299113, Train acc 0.093750 \n",
      "Batch 66, Loss: 2.295014, Train acc 0.109375 \n",
      "Batch 67, Loss: 2.311337, Train acc 0.062500 \n",
      "Batch 68, Loss: 2.302308, Train acc 0.109375 \n",
      "Batch 69, Loss: 2.310441, Train acc 0.062500 \n",
      "Batch 70, Loss: 2.308660, Train acc 0.078125 \n",
      "Batch 71, Loss: 2.286053, Train acc 0.171875 \n",
      "Batch 72, Loss: 2.291921, Train acc 0.125000 \n",
      "Batch 73, Loss: 2.287495, Train acc 0.171875 \n",
      "Batch 74, Loss: 2.294531, Train acc 0.125000 \n",
      "Batch 75, Loss: 2.290596, Train acc 0.156250 \n",
      "Batch 76, Loss: 2.281767, Train acc 0.203125 \n",
      "Batch 77, Loss: 2.283745, Train acc 0.156250 \n",
      "Batch 78, Loss: 2.270931, Train acc 0.171875 \n",
      "Batch 79, Loss: 2.248668, Train acc 0.281250 \n",
      "Batch 80, Loss: 2.270096, Train acc 0.218750 \n",
      "Batch 81, Loss: 2.227571, Train acc 0.265625 \n",
      "Batch 82, Loss: 2.208348, Train acc 0.328125 \n",
      "Batch 83, Loss: 2.199208, Train acc 0.375000 \n",
      "Batch 84, Loss: 2.138640, Train acc 0.484375 \n",
      "Batch 85, Loss: 2.116323, Train acc 0.296875 \n",
      "Batch 86, Loss: 2.041967, Train acc 0.281250 \n",
      "Batch 87, Loss: 1.841306, Train acc 0.390625 \n",
      "Batch 88, Loss: 1.561219, Train acc 0.343750 \n",
      "Batch 89, Loss: 3.029881, Train acc 0.234375 \n",
      "Batch 90, Loss: 2.314323, Train acc 0.156250 \n",
      "Batch 91, Loss: 2.317969, Train acc 0.109375 \n",
      "Batch 92, Loss: 2.310565, Train acc 0.046875 \n",
      "Batch 93, Loss: 2.283499, Train acc 0.093750 \n",
      "Batch 94, Loss: 2.301454, Train acc 0.078125 \n",
      "Batch 95, Loss: 2.309928, Train acc 0.062500 \n",
      "Batch 96, Loss: 2.334446, Train acc 0.093750 \n",
      "Batch 97, Loss: 2.306603, Train acc 0.062500 \n",
      "Batch 98, Loss: 2.329856, Train acc 0.093750 \n",
      "Batch 99, Loss: 2.296755, Train acc 0.078125 \n",
      "Batch 100, Loss: 2.270747, Train acc 0.140625 \n",
      "Batch 101, Loss: 2.320475, Train acc 0.109375 \n",
      "Batch 102, Loss: 2.310979, Train acc 0.078125 \n",
      "Batch 103, Loss: 2.291575, Train acc 0.031250 \n",
      "Batch 104, Loss: 2.299027, Train acc 0.062500 \n",
      "Batch 105, Loss: 2.279396, Train acc 0.203125 \n",
      "Batch 106, Loss: 2.280262, Train acc 0.125000 \n",
      "Batch 107, Loss: 2.290327, Train acc 0.203125 \n",
      "Batch 108, Loss: 2.276606, Train acc 0.234375 \n",
      "Batch 109, Loss: 2.300143, Train acc 0.250000 \n",
      "Batch 110, Loss: 2.249299, Train acc 0.250000 \n",
      "Batch 111, Loss: 2.283485, Train acc 0.234375 \n",
      "Batch 112, Loss: 2.241992, Train acc 0.203125 \n",
      "Batch 113, Loss: 2.265924, Train acc 0.187500 \n",
      "Batch 114, Loss: 2.246393, Train acc 0.218750 \n",
      "Batch 115, Loss: 2.209105, Train acc 0.250000 \n",
      "Batch 116, Loss: 2.217265, Train acc 0.203125 \n",
      "Batch 117, Loss: 2.164920, Train acc 0.281250 \n",
      "Batch 118, Loss: 2.173814, Train acc 0.265625 \n",
      "Batch 119, Loss: 2.117345, Train acc 0.281250 \n",
      "Batch 120, Loss: 2.161398, Train acc 0.218750 \n",
      "Batch 121, Loss: 2.117718, Train acc 0.171875 \n",
      "Batch 122, Loss: 1.962870, Train acc 0.281250 \n",
      "Batch 123, Loss: 1.878430, Train acc 0.203125 \n",
      "Batch 124, Loss: 1.905459, Train acc 0.312500 \n",
      "Batch 125, Loss: 1.938291, Train acc 0.359375 \n",
      "Batch 126, Loss: 2.071480, Train acc 0.312500 \n",
      "Batch 127, Loss: 1.857772, Train acc 0.343750 \n",
      "Batch 128, Loss: 1.710109, Train acc 0.265625 \n",
      "Batch 129, Loss: 1.654552, Train acc 0.437500 \n",
      "Batch 130, Loss: 1.746534, Train acc 0.375000 \n",
      "Batch 131, Loss: 1.799269, Train acc 0.343750 \n",
      "Batch 132, Loss: 1.648841, Train acc 0.296875 \n",
      "Batch 133, Loss: 1.739853, Train acc 0.375000 \n",
      "Batch 134, Loss: 1.824728, Train acc 0.203125 \n",
      "Batch 135, Loss: 2.054293, Train acc 0.171875 \n",
      "Batch 136, Loss: 1.810570, Train acc 0.343750 \n",
      "Batch 137, Loss: 1.410629, Train acc 0.421875 \n",
      "Batch 138, Loss: 1.304878, Train acc 0.453125 \n",
      "Batch 139, Loss: 1.631159, Train acc 0.328125 \n",
      "Batch 140, Loss: 1.687172, Train acc 0.453125 \n",
      "Batch 141, Loss: 1.531854, Train acc 0.437500 \n",
      "Batch 142, Loss: 1.256908, Train acc 0.468750 \n",
      "Batch 143, Loss: 1.617658, Train acc 0.375000 \n",
      "Batch 144, Loss: 1.720954, Train acc 0.359375 \n",
      "Batch 145, Loss: 1.548409, Train acc 0.437500 \n",
      "Batch 146, Loss: 1.314263, Train acc 0.531250 \n",
      "Batch 147, Loss: 1.029821, Train acc 0.609375 \n",
      "Batch 148, Loss: 1.522607, Train acc 0.531250 \n",
      "Batch 149, Loss: 1.923570, Train acc 0.296875 \n",
      "Batch 150, Loss: 1.957726, Train acc 0.343750 \n",
      "Batch 151, Loss: 1.666115, Train acc 0.421875 \n",
      "Batch 152, Loss: 1.336649, Train acc 0.500000 \n",
      "Batch 153, Loss: 1.265318, Train acc 0.468750 \n",
      "Batch 154, Loss: 1.182074, Train acc 0.531250 \n",
      "Batch 155, Loss: 1.077249, Train acc 0.593750 \n",
      "Batch 156, Loss: 1.124740, Train acc 0.578125 \n",
      "Batch 157, Loss: 1.814870, Train acc 0.281250 \n",
      "Batch 158, Loss: 1.775279, Train acc 0.265625 \n",
      "Batch 159, Loss: 1.554509, Train acc 0.406250 \n",
      "Batch 160, Loss: 1.299201, Train acc 0.562500 \n",
      "Batch 161, Loss: 1.143973, Train acc 0.500000 \n",
      "Batch 162, Loss: 1.311538, Train acc 0.453125 \n",
      "Batch 163, Loss: 1.186040, Train acc 0.500000 \n",
      "Batch 164, Loss: 1.104707, Train acc 0.531250 \n",
      "Batch 165, Loss: 1.161969, Train acc 0.593750 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 166, Loss: 1.251129, Train acc 0.640625 \n",
      "Batch 167, Loss: 1.287834, Train acc 0.531250 \n",
      "Batch 168, Loss: 0.928725, Train acc 0.609375 \n",
      "Batch 169, Loss: 0.983610, Train acc 0.609375 \n",
      "Batch 170, Loss: 1.074163, Train acc 0.562500 \n",
      "Batch 171, Loss: 1.043530, Train acc 0.656250 \n",
      "Batch 172, Loss: 1.174133, Train acc 0.515625 \n",
      "Batch 173, Loss: 0.794117, Train acc 0.671875 \n",
      "Batch 174, Loss: 1.143815, Train acc 0.484375 \n",
      "Batch 175, Loss: 1.761341, Train acc 0.328125 \n",
      "Batch 176, Loss: 1.727128, Train acc 0.562500 \n",
      "Batch 177, Loss: 1.318952, Train acc 0.562500 \n",
      "Batch 178, Loss: 1.116539, Train acc 0.546875 \n",
      "Batch 179, Loss: 1.156007, Train acc 0.515625 \n",
      "Batch 180, Loss: 1.084418, Train acc 0.546875 \n",
      "Batch 181, Loss: 1.024498, Train acc 0.484375 \n",
      "Batch 182, Loss: 1.299186, Train acc 0.484375 \n",
      "Batch 183, Loss: 1.313792, Train acc 0.437500 \n",
      "Batch 184, Loss: 1.024073, Train acc 0.625000 \n",
      "Batch 185, Loss: 1.170929, Train acc 0.578125 \n",
      "Batch 186, Loss: 1.101246, Train acc 0.609375 \n",
      "Batch 187, Loss: 1.089614, Train acc 0.562500 \n",
      "Batch 188, Loss: 0.959552, Train acc 0.593750 \n",
      "Batch 189, Loss: 0.789333, Train acc 0.640625 \n",
      "Batch 190, Loss: 1.318136, Train acc 0.515625 \n",
      "Batch 191, Loss: 1.293303, Train acc 0.562500 \n",
      "Batch 192, Loss: 1.175099, Train acc 0.515625 \n",
      "Batch 193, Loss: 1.014440, Train acc 0.703125 \n",
      "Batch 194, Loss: 0.741315, Train acc 0.671875 \n",
      "Batch 195, Loss: 1.309288, Train acc 0.593750 \n",
      "Batch 196, Loss: 1.449580, Train acc 0.453125 \n",
      "Batch 197, Loss: 1.031770, Train acc 0.609375 \n",
      "Batch 198, Loss: 1.092335, Train acc 0.484375 \n",
      "Batch 199, Loss: 1.039027, Train acc 0.531250 \n",
      "Batch 200, Loss: 0.907710, Train acc 0.703125 \n",
      "Batch 201, Loss: 0.934829, Train acc 0.562500 \n",
      "Batch 202, Loss: 0.953516, Train acc 0.609375 \n",
      "Batch 203, Loss: 1.139242, Train acc 0.562500 \n",
      "Batch 204, Loss: 1.198127, Train acc 0.437500 \n",
      "Batch 205, Loss: 1.063442, Train acc 0.578125 \n",
      "Batch 206, Loss: 0.840994, Train acc 0.625000 \n",
      "Batch 207, Loss: 1.088844, Train acc 0.625000 \n",
      "Batch 208, Loss: 1.143059, Train acc 0.562500 \n",
      "Batch 209, Loss: 0.920797, Train acc 0.609375 \n",
      "Batch 210, Loss: 0.946370, Train acc 0.671875 \n",
      "Batch 211, Loss: 1.192690, Train acc 0.578125 \n",
      "Batch 212, Loss: 0.949690, Train acc 0.718750 \n",
      "Batch 213, Loss: 0.791568, Train acc 0.687500 \n",
      "Batch 214, Loss: 0.740992, Train acc 0.718750 \n",
      "Batch 215, Loss: 0.827486, Train acc 0.609375 \n",
      "Batch 216, Loss: 1.186950, Train acc 0.578125 \n",
      "Batch 217, Loss: 1.058331, Train acc 0.609375 \n",
      "Batch 218, Loss: 0.923337, Train acc 0.640625 \n",
      "Batch 219, Loss: 0.958637, Train acc 0.625000 \n",
      "Batch 220, Loss: 0.949293, Train acc 0.609375 \n",
      "Batch 221, Loss: 0.762049, Train acc 0.609375 \n",
      "Batch 222, Loss: 1.110319, Train acc 0.546875 \n",
      "Batch 223, Loss: 1.006230, Train acc 0.656250 \n",
      "Batch 224, Loss: 1.004061, Train acc 0.531250 \n",
      "Batch 225, Loss: 0.969790, Train acc 0.593750 \n",
      "Batch 226, Loss: 0.807379, Train acc 0.718750 \n",
      "Batch 227, Loss: 0.897904, Train acc 0.671875 \n",
      "Batch 228, Loss: 1.067771, Train acc 0.578125 \n",
      "Batch 229, Loss: 0.895404, Train acc 0.703125 \n",
      "Batch 230, Loss: 0.964261, Train acc 0.656250 \n",
      "Batch 231, Loss: 1.145509, Train acc 0.437500 \n",
      "Batch 232, Loss: 0.932184, Train acc 0.609375 \n",
      "Batch 233, Loss: 0.890432, Train acc 0.640625 \n",
      "Batch 234, Loss: 0.812370, Train acc 0.671875 \n",
      "Batch 235, Loss: 0.794641, Train acc 0.671875 \n",
      "Batch 236, Loss: 0.698092, Train acc 0.750000 \n",
      "Batch 237, Loss: 0.828562, Train acc 0.593750 \n",
      "Batch 238, Loss: 0.852793, Train acc 0.640625 \n",
      "Batch 239, Loss: 0.591252, Train acc 0.765625 \n",
      "Batch 240, Loss: 0.939478, Train acc 0.593750 \n",
      "Batch 241, Loss: 1.076418, Train acc 0.625000 \n",
      "Batch 242, Loss: 0.881670, Train acc 0.687500 \n",
      "Batch 243, Loss: 0.702688, Train acc 0.718750 \n",
      "Batch 244, Loss: 1.022916, Train acc 0.640625 \n",
      "Batch 245, Loss: 0.843215, Train acc 0.609375 \n",
      "Batch 246, Loss: 0.851006, Train acc 0.671875 \n",
      "Batch 247, Loss: 0.803265, Train acc 0.718750 \n",
      "Batch 248, Loss: 0.724770, Train acc 0.750000 \n",
      "Batch 249, Loss: 0.892593, Train acc 0.656250 \n",
      "Batch 250, Loss: 1.108027, Train acc 0.578125 \n",
      "Batch 251, Loss: 1.040460, Train acc 0.656250 \n",
      "Batch 252, Loss: 0.851538, Train acc 0.656250 \n",
      "Batch 253, Loss: 0.964654, Train acc 0.609375 \n",
      "Batch 254, Loss: 0.871900, Train acc 0.625000 \n",
      "Batch 255, Loss: 0.696724, Train acc 0.796875 \n",
      "Batch 256, Loss: 0.782134, Train acc 0.750000 \n",
      "Batch 257, Loss: 0.840398, Train acc 0.687500 \n",
      "Batch 258, Loss: 0.871355, Train acc 0.625000 \n",
      "Batch 259, Loss: 0.772077, Train acc 0.625000 \n",
      "Batch 260, Loss: 0.823964, Train acc 0.640625 \n",
      "Batch 261, Loss: 0.866230, Train acc 0.625000 \n",
      "Batch 262, Loss: 0.832465, Train acc 0.687500 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-06d0878703ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch %d, Loss: %f, Train acc %f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 904\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    905\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1136\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1137\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1354\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1339\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "max_steps = 1000\n",
    "batch_size = 64\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "resize_input = tf.image.resize_images(input_placeholder, [96, 96])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "logits = vggnet(resize_input, is_training)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width, 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label, is_training: True}\n",
    "    loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)\n",
    "    print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "        \n",
    "for i in range(100):\n",
    "    test_data, test_label = test_dataset.next_batch(100)\n",
    "    test_data = np.reshape(test_data, (100, height, width, num_channels))\n",
    "    test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_data, gt_placeholder: test_label, is_training: False})\n",
    "    test_acc.append(test_acc_)\n",
    "print (\"Test Loss: %f, Test acc %f \" % (np.mean(test_loss_), np.mean(test_acc_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
