{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络 — 从0开始\n",
    "之前的教程里，在输入神经网络前我们将输入图片直接转成了向量。这样做有两个不好的地方：\n",
    "\n",
    "在图片里相近的像素在向量表示里可能很远，从而模型很难捕获他们的空间关系。\n",
    "对于大图片输入，模型可能会很大。例如输入是$256×256×3$的照片（仍然远比手机拍的小），输出层是1000，那么这一层的模型大小是将近1GB.\n",
    "这一节我们介绍卷积神经网络，其有效了解决了上述两个问题。\n",
    "\n",
    "### 卷积神经网络\n",
    "卷积神经网络是指主要由卷积层构成的神经网络。\n",
    "\n",
    "### 卷积层\n",
    "卷积层跟前面的全连接层类似，但输入和权重不是做简单的矩阵乘法，而是使用每次作用在一个窗口上的卷积。下图演示了输入是一个$4×4$矩阵，使用一个$3×3$的权重，计算得到$2×2$结果的过程。每次我们采样一个跟权重一样大小的窗口，让它跟权重做按元素的乘法然后相加。通常我们也是用卷积的术语把这个权重叫kernel或者filter。\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/no_padding_no_strides.gif)\n",
    "\n",
    "（图片版权属于vdumoulin@github）\n",
    "\n",
    "我们使用`tf.nn.conv2d`来演示这个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input:', array([[[[0.],\n",
      "         [1.],\n",
      "         [2.]],\n",
      "\n",
      "        [[3.],\n",
      "         [4.],\n",
      "         [5.]],\n",
      "\n",
      "        [[6.],\n",
      "         [7.],\n",
      "         [8.]]]], dtype=float32), '\\n\\nweight:', array([[[[0.]],\n",
      "\n",
      "        [[1.]]],\n",
      "\n",
      "\n",
      "       [[[2.]],\n",
      "\n",
      "        [[3.]]]], dtype=float32), '\\n\\nbias:', 1.0, '\\n\\noutput:', array([[[[20.],\n",
      "         [26.]],\n",
      "\n",
      "        [[38.],\n",
      "         [44.]]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#height*width*input_channels*output_channels\n",
    "w = tf.constant(np.arange(4).reshape((2,2,1,1)), dtype=tf.float32)\n",
    "b = tf.constant(np.array(1), dtype=tf.float32)\n",
    "#batch_size*height*width*input_channels\n",
    "data = tf.constant(np.arange(9).reshape(1,3,3,1), dtype=tf.float32)\n",
    " \n",
    "output = tf.nn.conv2d(data, w, strides = [1,1,1,1], padding ='VALID')\n",
    "output += b\n",
    "sess = tf.InteractiveSession()\n",
    "out = sess.run(output)\n",
    "\n",
    "print('input:', data.eval(), '\\n\\nweight:', w.eval(), '\\n\\nbias:', b.eval(), '\\n\\noutput:', out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以控制如何移动窗口，和在边缘的时候如何填充窗口。下图演示了stride=2和pad=1。\n",
    "![image.png](http://zh.gluon.ai/_images/padding_strides.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input:', array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [2.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [3.],\n",
      "         [4.],\n",
      "         [5.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [6.],\n",
      "         [7.],\n",
      "         [8.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32), '\\n\\nweight:', array([[[[0.]],\n",
      "\n",
      "        [[1.]]],\n",
      "\n",
      "\n",
      "       [[[2.]],\n",
      "\n",
      "        [[3.]]]], dtype=float32), '\\n\\nbias:', 1.0, '\\n\\noutput:', array([[[[ 1.],\n",
      "         [ 9.]],\n",
      "\n",
      "        [[22.],\n",
      "         [44.]]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "data_pad = tf.pad(data, [[0,0],[1,1],[1,1],[0,0]])\n",
    "output = tf.nn.conv2d(data_pad, w, strides=(1,2,2,1), padding='VALID')\n",
    "output += b\n",
    "\n",
    "print('input:', data_pad.eval(), '\\n\\nweight:', w.eval(), '\\n\\nbias:', b.eval(), '\\n\\noutput:', output.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当输入数据有多个通道的时候，每个通道会有对应的权重，然后会对每个通道做卷积之后在通道之间求和\n",
    "\n",
    "$conv(data, w, b) = \\sum_i conv(data[:,i,:,:], w[:,i,:,:], b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input:', array([[[[ 0.,  1.],\n",
      "         [ 2.,  3.],\n",
      "         [ 4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.],\n",
      "         [ 8.,  9.],\n",
      "         [10., 11.]],\n",
      "\n",
      "        [[12., 13.],\n",
      "         [14., 15.],\n",
      "         [16., 17.]]]], dtype=float32), '\\n\\nweight:', array([[[[0.],\n",
      "         [1.]],\n",
      "\n",
      "        [[2.],\n",
      "         [3.]]],\n",
      "\n",
      "\n",
      "       [[[4.],\n",
      "         [5.]],\n",
      "\n",
      "        [[6.],\n",
      "         [7.]]]], dtype=float32), '\\n\\nbias:', 1.0, '\\n\\noutput:', array([[[[185.],\n",
      "         [241.]],\n",
      "\n",
      "        [[353.],\n",
      "         [409.]]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(np.arange(8).reshape((2,2,2,1)), dtype=tf.float32)\n",
    "data = tf.constant(np.arange(18).reshape(1,3,3,2), dtype=tf.float32)\n",
    "\n",
    "output = tf.nn.conv2d(data, w, strides = [1,1,1,1], padding ='VALID')\n",
    "output += b\n",
    "print('input:', data.eval(), '\\n\\nweight:', w.eval(), '\\n\\nbias:', b.eval(), '\\n\\noutput:', output.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当输出需要多通道时，每个输出通道有对应权重，然后每个通道上做卷积。\n",
    "\n",
    "$conv(data, w, b)[:,i,:,:] = conv(data, w[i,:,:,:], b[i])$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('input:', array([[[[ 0.,  1.],\n",
      "         [ 2.,  3.],\n",
      "         [ 4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.],\n",
      "         [ 8.,  9.],\n",
      "         [10., 11.]],\n",
      "\n",
      "        [[12., 13.],\n",
      "         [14., 15.],\n",
      "         [16., 17.]]]], dtype=float32), '\\n\\nweight:', array([[[[ 0.,  1.],\n",
      "         [ 2.,  3.]],\n",
      "\n",
      "        [[ 4.,  5.],\n",
      "         [ 6.,  7.]]],\n",
      "\n",
      "\n",
      "       [[[ 8.,  9.],\n",
      "         [10., 11.]],\n",
      "\n",
      "        [[12., 13.],\n",
      "         [14., 15.]]]], dtype=float32), '\\n\\nbias:', array([1., 2.], dtype=float32), '\\n\\noutput:', array([[[[369., 406.],\n",
      "         [481., 534.]],\n",
      "\n",
      "        [[705., 790.],\n",
      "         [817., 918.]]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(np.arange(16).reshape((2,2,2,2)), dtype=tf.float32)\n",
    "data = tf.constant(np.arange(18).reshape(1,3,3,2), dtype=tf.float32)\n",
    "b = tf.constant(np.array([1,2]), dtype=tf.float32)\n",
    "\n",
    "output = tf.nn.conv2d(data, w, strides = [1,1,1,1], padding ='VALID')\n",
    "output += b\n",
    "print('input:', data.eval(), '\\n\\nweight:', w.eval(), '\\n\\nbias:', b.eval(), '\\n\\noutput:', output.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层（pooling）\n",
    "因为卷积层每次作用在一个窗口，它对位置很敏感。池化层能够很好的缓解这个问题。它跟卷积类似每次看一个小窗口，然后选出窗口里面最大的元素，或者平均元素作为输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('data:', array([[[[ 0.,  1.],\n",
      "         [ 2.,  3.],\n",
      "         [ 4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.],\n",
      "         [ 8.,  9.],\n",
      "         [10., 11.]],\n",
      "\n",
      "        [[12., 13.],\n",
      "         [14., 15.],\n",
      "         [16., 17.]]]], dtype=float32), '\\n\\nmax pooling:', array([[[[ 8.,  9.],\n",
      "         [10., 11.]],\n",
      "\n",
      "        [[14., 15.],\n",
      "         [16., 17.]]]], dtype=float32), '\\n\\navg pooling:', array([[[[ 4.,  5.],\n",
      "         [ 6.,  7.]],\n",
      "\n",
      "        [[10., 11.],\n",
      "         [12., 13.]]]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "data = tf.constant(np.arange(18).reshape(1,3,3,2), dtype=tf.float32)\n",
    "max_pool = tf.nn.max_pool(data, [1,2,2,1], [1,1,1,1], padding='VALID')\n",
    "avg_pool = tf.nn.avg_pool(data, [1,2,2,1], [1,1,1,1], padding='VALID')\n",
    "\n",
    "print('data:', data.eval(), '\\n\\nmax pooling:', max_pool.eval(), '\\n\\navg pooling:', avg_pool.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们可以开始使用这些层构建模型了。\n",
    "\n",
    "### 获取数据\n",
    "我们继续使用FashionMNIST（希望你还没有彻底厌烦这个数据）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print train_labels.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用MNIST常用的LeNet，它有两个卷积层，之后是两个全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.name_scope('cnn'):\n",
    "    # output channels = 20, kernel = (5,5)\n",
    "    W1 = tf.Variable(tf.truncated_normal([5,5,1,20], mean=0.0, stddev=0.01, seed=None, dtype=tf.float32))\n",
    "    b1 = tf.Variable(tf.constant(0.0, shape=[20]))\n",
    "    # output channels = 50, kernel = (3,3)\n",
    "    W2 = tf.Variable(tf.truncated_normal([3,3,20,50], mean=0.0, stddev=0.01, seed=None, dtype=tf.float32))\n",
    "    b2 = tf.Variable(tf.constant(0.0, shape=[50]))\n",
    "    # output dim = 128\n",
    "    W3 = tf.Variable(tf.truncated_normal([1250, 128], mean=0.0, stddev=0.01, seed=None, dtype=tf.float32))\n",
    "    #W3 = tf.Variable(tf.random_normal([784, 128], mean=0.0, stddev=0.01, seed=None, dtype=tf.float32))\n",
    "    b3 = tf.Variable(tf.constant(0.0, shape=[128]))\n",
    "    # output dim = 10\n",
    "    W4 = tf.Variable(tf.truncated_normal([128, 10], mean=0.0, stddev=0.01, seed=None, dtype=tf.float32))\n",
    "    b4 = tf.Variable(tf.constant(0.0, shape=[10]))    \n",
    "\n",
    "    params = [W1, b1, W2, b2, W3, b3, W4, b4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积模块通常是“卷积层-激活层-池化层”。然后转成2D矩阵输出给后面的全连接层。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, verbose=False):\n",
    "    # 第一层卷积\n",
    "    #'''\n",
    "    h1_conv = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'VALID') \n",
    "    h1_activation = tf.nn.relu(h1_conv)\n",
    "    h1 = tf.nn.max_pool(h1_activation, [1,2,2,1], [1,2,2,1], padding = 'VALID')\n",
    "    # 第二层卷积\n",
    "    h2_conv = tf.nn.conv2d(h1, W2, strides = [1,1,1,1], padding = 'VALID') \n",
    "    h2_activation = tf.nn.relu(h2_conv)\n",
    "    h2 = tf.nn.max_pool(h2_activation, [1,2,2,1], [1,2,2,1], padding='VALID')\n",
    "    h2 = tf.layers.flatten(h2)\n",
    "    # 第一层全连接\n",
    "    #'''\n",
    "    h3_linear = tf.matmul(h2, W3) + b3\n",
    "\n",
    "    #h3_linear = tf.matmul(X, W3) + b3\n",
    "    h3 = tf.nn.relu(h3_linear)\n",
    "    # 第二层全连接\n",
    "    h4_linear = tf.matmul(h3, W4) + b4\n",
    "    if verbose:\n",
    "        print('1st conv block:', h1.get_shape().as_list())\n",
    "        print('2nd conv block:', h2.get_shape().as_list())\n",
    "        print('1st dense:', h3.get_shape().as_list())\n",
    "        print('2nd dense:', h4_linear.get_shape().as_list())\n",
    "        print('output:', h4_linear)\n",
    "    return h4_linear, h2_activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下，输出中间结果形状（当然可以直接打印结果)和最终结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1st conv block:', [64, 12, 12, 20])\n",
      "('2nd conv block:', [64, 1250])\n",
      "('1st dense:', [64, 128])\n",
      "('2nd dense:', [64, 10])\n",
      "('output:', <tf.Tensor 'add_5:0' shape=(64, 10) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "    data, label = train_dataset.next_batch(64)\n",
    "    data = tf.reshape(data, [64, 28, 28, 1])\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    out = net(data, verbose=True)\n",
    "    #print out.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "跟前面没有什么不同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Batch 0, Loss: 2.302582, Train acc 0.164062 \n",
      "Batch 10, Loss: 2.276734, Train acc 0.312500 \n",
      "Batch 20, Loss: 1.795223, Train acc 0.414062 \n",
      "Batch 30, Loss: 1.199801, Train acc 0.492188 \n",
      "Batch 40, Loss: 1.144416, Train acc 0.574219 \n",
      "Batch 50, Loss: 1.097209, Train acc 0.550781 \n",
      "Batch 60, Loss: 0.937172, Train acc 0.617188 \n",
      "Batch 70, Loss: 0.934170, Train acc 0.601562 \n",
      "Batch 80, Loss: 0.938392, Train acc 0.683594 \n",
      "Batch 90, Loss: 0.908565, Train acc 0.675781 \n",
      "Batch 100, Loss: 0.718502, Train acc 0.730469 \n",
      "Batch 110, Loss: 0.777633, Train acc 0.691406 \n",
      "Batch 120, Loss: 0.788591, Train acc 0.699219 \n",
      "Batch 130, Loss: 0.820860, Train acc 0.718750 \n",
      "Batch 140, Loss: 0.622196, Train acc 0.765625 \n",
      "Batch 150, Loss: 0.796487, Train acc 0.687500 \n",
      "Batch 160, Loss: 0.850881, Train acc 0.691406 \n",
      "Batch 170, Loss: 0.716029, Train acc 0.765625 \n",
      "Batch 180, Loss: 0.665293, Train acc 0.726562 \n",
      "Batch 190, Loss: 0.608100, Train acc 0.773438 \n",
      "Batch 200, Loss: 0.747986, Train acc 0.730469 \n",
      "Batch 210, Loss: 0.590251, Train acc 0.765625 \n",
      "Batch 220, Loss: 0.617286, Train acc 0.746094 \n",
      "Batch 230, Loss: 0.570664, Train acc 0.765625 \n",
      "Batch 240, Loss: 0.518371, Train acc 0.828125 \n",
      "Batch 250, Loss: 0.686827, Train acc 0.753906 \n",
      "Batch 260, Loss: 0.523954, Train acc 0.808594 \n",
      "Batch 270, Loss: 0.549975, Train acc 0.800781 \n",
      "Batch 280, Loss: 0.590643, Train acc 0.753906 \n",
      "Batch 290, Loss: 0.583851, Train acc 0.789062 \n",
      "Batch 300, Loss: 0.607774, Train acc 0.761719 \n",
      "Batch 310, Loss: 0.504818, Train acc 0.820312 \n",
      "Batch 320, Loss: 0.520808, Train acc 0.820312 \n",
      "Batch 330, Loss: 0.488830, Train acc 0.816406 \n",
      "Batch 340, Loss: 0.625809, Train acc 0.726562 \n",
      "Batch 350, Loss: 0.430033, Train acc 0.847656 \n",
      "Batch 360, Loss: 0.515834, Train acc 0.792969 \n",
      "Batch 370, Loss: 0.609937, Train acc 0.750000 \n",
      "Batch 380, Loss: 0.513880, Train acc 0.789062 \n",
      "Batch 390, Loss: 0.484887, Train acc 0.808594 \n",
      "Batch 400, Loss: 0.497155, Train acc 0.789062 \n",
      "Batch 410, Loss: 0.616026, Train acc 0.753906 \n",
      "Batch 420, Loss: 0.568539, Train acc 0.804688 \n",
      "Batch 430, Loss: 0.497067, Train acc 0.824219 \n",
      "Batch 440, Loss: 0.513959, Train acc 0.812500 \n",
      "Batch 450, Loss: 0.484089, Train acc 0.804688 \n",
      "Batch 460, Loss: 0.464473, Train acc 0.812500 \n",
      "Batch 470, Loss: 0.479721, Train acc 0.835938 \n",
      "Batch 480, Loss: 0.558564, Train acc 0.781250 \n",
      "Batch 490, Loss: 0.405688, Train acc 0.843750 \n",
      "Batch 500, Loss: 0.492553, Train acc 0.835938 \n",
      "Batch 510, Loss: 0.478713, Train acc 0.835938 \n",
      "Batch 520, Loss: 0.440307, Train acc 0.843750 \n",
      "Batch 530, Loss: 0.620375, Train acc 0.769531 \n",
      "Batch 540, Loss: 0.494251, Train acc 0.828125 \n",
      "Batch 550, Loss: 0.473099, Train acc 0.820312 \n",
      "Batch 560, Loss: 0.503075, Train acc 0.816406 \n",
      "Batch 570, Loss: 0.450425, Train acc 0.839844 \n",
      "Batch 580, Loss: 0.443068, Train acc 0.832031 \n",
      "Batch 590, Loss: 0.388380, Train acc 0.855469 \n",
      "Batch 600, Loss: 0.443075, Train acc 0.855469 \n",
      "Batch 610, Loss: 0.550852, Train acc 0.808594 \n",
      "Batch 620, Loss: 0.472792, Train acc 0.832031 \n",
      "Batch 630, Loss: 0.379422, Train acc 0.871094 \n",
      "Batch 640, Loss: 0.448842, Train acc 0.839844 \n",
      "Batch 650, Loss: 0.373997, Train acc 0.890625 \n",
      "Batch 660, Loss: 0.419854, Train acc 0.855469 \n",
      "Batch 670, Loss: 0.403623, Train acc 0.843750 \n",
      "Batch 680, Loss: 0.359013, Train acc 0.878906 \n",
      "Batch 690, Loss: 0.529395, Train acc 0.855469 \n",
      "Batch 700, Loss: 0.517542, Train acc 0.835938 \n",
      "Batch 710, Loss: 0.447042, Train acc 0.859375 \n",
      "Batch 720, Loss: 0.450175, Train acc 0.839844 \n",
      "Batch 730, Loss: 0.393548, Train acc 0.859375 \n",
      "Batch 740, Loss: 0.375811, Train acc 0.859375 \n",
      "Batch 750, Loss: 0.373066, Train acc 0.863281 \n",
      "Batch 760, Loss: 0.557368, Train acc 0.785156 \n",
      "Batch 770, Loss: 0.378461, Train acc 0.859375 \n",
      "Batch 780, Loss: 0.408231, Train acc 0.878906 \n",
      "Batch 790, Loss: 0.466222, Train acc 0.820312 \n",
      "Batch 800, Loss: 0.439091, Train acc 0.828125 \n",
      "Batch 810, Loss: 0.442285, Train acc 0.847656 \n",
      "Batch 820, Loss: 0.428090, Train acc 0.859375 \n",
      "Batch 830, Loss: 0.349124, Train acc 0.871094 \n",
      "Batch 840, Loss: 0.324598, Train acc 0.871094 \n",
      "Batch 850, Loss: 0.399257, Train acc 0.859375 \n",
      "Batch 860, Loss: 0.374583, Train acc 0.859375 \n",
      "Batch 870, Loss: 0.349841, Train acc 0.875000 \n",
      "Batch 880, Loss: 0.409488, Train acc 0.851562 \n",
      "Batch 890, Loss: 0.321711, Train acc 0.886719 \n",
      "Batch 900, Loss: 0.414713, Train acc 0.839844 \n",
      "Batch 910, Loss: 0.317305, Train acc 0.886719 \n",
      "Batch 920, Loss: 0.379483, Train acc 0.851562 \n",
      "Batch 930, Loss: 0.373412, Train acc 0.855469 \n",
      "Batch 940, Loss: 0.376034, Train acc 0.851562 \n",
      "Batch 950, Loss: 0.399379, Train acc 0.847656 \n",
      "Batch 960, Loss: 0.352676, Train acc 0.875000 \n",
      "Batch 970, Loss: 0.448762, Train acc 0.851562 \n",
      "Batch 980, Loss: 0.333302, Train acc 0.910156 \n",
      "Batch 990, Loss: 0.372449, Train acc 0.878906 \n",
      "Test Loss: 0.399823, Test acc 0.860100 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "learning_rate = 1e-3\n",
    "max_steps = 1000\n",
    "batch_size = 256\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "#input_placeholder = tf.placeholder(tf.float32, [None, height*width*num_channels])\n",
    "\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "logits, h2 = net(input_placeholder)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits,  onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width, num_channels))\n",
    "    \n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label}\n",
    "    h2_, loss_, acc_, _ = sess.run([h2, loss, acc, train_op], feed_dict=feed_dict)\n",
    "    if step % 10 == 0:\n",
    "        print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "\n",
    "test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_images_reshape / 255.0, gt_placeholder: test_labels})\n",
    "print (\"Test Loss: %f, Test acc %f \" % (test_loss_, test_acc_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
