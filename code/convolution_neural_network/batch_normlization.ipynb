{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量归一化 — 从0开始\n",
    "在Kaggle实战我们输入数据做了归一化。在实际应用中，我们通常将输入数据的每个样本或者每个特征进行归一化，就是将均值变为0方差变为1，来使得数值更稳定。\n",
    "\n",
    "这个对 我们在之前的课程里学过了线性回归和逻辑回归很有效。因为输入层的输入值的大小变化不剧烈，那么输入也不会。但是，对于一个可能有很多层的深度学习模型来说，情况可能会比较复杂。\n",
    "\n",
    "举个例子，随着第一层和第二层的参数在训练时不断变化，第三层所使用的激活函数的输入值可能由于乘法效应而变得极大或极小，例如和第一层所使用的激活函数的输入值不在一个数量级上。这种在训练时可能出现的情况会造成模型训练的不稳定性。例如，给定一个学习率，某次参数迭代后，目标函数值会剧烈变化或甚至升高。数学的解释是，如果把目标函数 $f$ 根据参数 $w$ 迭代（如 $f(\\mathbf{w} - \\eta \\nabla f(\\mathbf{w}))$）进行泰勒展开，有关学习率 $\\eta$ 的高阶项的系数可能由于数量级的原因（通常由于层数多）而不容忽略。然而常用的低阶优化算法（如梯度下降）对于不断降低目标函 数的有效性通常基于一个基本假设：在以上泰勒展开中把有关学习率的高阶项通通忽略不计。\n",
    "\n",
    "为了应对上述这种情况，Sergey Ioffe和Christian Szegedy在2015年提出了批量归一化的方法。简而言之，在训练时给定一个批量输入，批量归一化试图对深度学习模型的某一层所使用的激活函数的输入进行归一化：使批量呈标准正态分布（均值为0，标准差为1）。\n",
    "\n",
    "批量归一化通常应用于输入层或任意中间层。\n",
    "\n",
    "简化的批量归一化层\n",
    "给定一个批量 $B = \\{x_{1, ..., m}\\}$, 我们需要学习拉升参数 $\\gamma$ 和偏移参数 $\\beta$。\n",
    "\n",
    "我们定义：\n",
    "\n",
    "$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m}x_i$\n",
    "\n",
    "$\\mu_B \\leftarrow \\frac{1}{m}\\sum_{i = 1}^{m}x_i$\n",
    "\n",
    "$\\hat{x_i} \\leftarrow \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "\n",
    "$y_i \\leftarrow \\gamma \\hat{x_i} + \\beta \\equiv \\mbox{BN}_{\\gamma,\\beta}(x_i)$\n",
    "批量归一化层的输出是$\\{y_i = BN_{\\gamma, \\beta}(x_i)\\}$。\n",
    "\n",
    "我们现在来动手实现一个简化的批量归一化层。实现时对全连接层和二维卷积层两种情况做了区分。对于全连接层，很明显我们要对每个批量进行归一化。然而这里需要注意的是，对 于二维卷积，我们要对每个通道进行归一化，并需要保持四维形状使得可以正确地广播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pure_batch_norm(X, gamma, beta, eps=1e-5):\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    # 全连接: batch_size x feature\n",
    "    if len(X.shape) == 2:\n",
    "        # 每个输入维度在样本上的平均和方差\n",
    "        mean = X.mean(axis=0)\n",
    "        variance = ((X - mean)**2).mean(axis=0)\n",
    "    # 2D卷积: batch_size x channel x height x width\n",
    "    else:\n",
    "        # 对每个channel算均值和方差，需要保持4D形状使得可以正确地广播\n",
    "        mean = X.mean(axis=(0,2,3), keepdims=True)\n",
    "        variance = ((X - mean)**2).mean(axis=(0,2,3), keepdims=True)\n",
    "\n",
    "    # 均一化\n",
    "    X_hat = (X - mean) / np.sqrt(variance + eps)\n",
    "    # 拉升和偏移\n",
    "    return gamma.reshape(mean.shape) * X_hat + beta.reshape(mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们检查一下。我们先定义全连接层的输入是这样的。每一行是批量中的一个实例。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.arange(6).reshape((3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们希望批量中的每一列都被归一化。结果符合预期。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.22474258, -1.22474258],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 1.22474258,  1.22474258]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(A, gamma=np.array([1,1]), beta=np.array([0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]\n",
      "\n",
      "  [[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]]]\n"
     ]
    }
   ],
   "source": [
    "B = np.arange(18).reshape((1,2,3,3))\n",
    "print B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果也如预期那样，我们对每个通道做了归一化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-1.54919218, -1.16189413, -0.77459609],\n",
       "         [-0.38729804,  0.        ,  0.38729804],\n",
       "         [ 0.77459609,  1.16189413,  1.54919218]],\n",
       "\n",
       "        [[-1.54919218, -1.16189413, -0.77459609],\n",
       "         [-0.38729804,  0.        ,  0.38729804],\n",
       "         [ 0.77459609,  1.16189413,  1.54919218]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_batch_norm(B, gamma=np.array([1,1]), beta=np.array([0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量归一化层\n",
    "你可能会想，既然训练时用了批量归一化，那么测试时也该用批量归一化吗？其实这个问题乍一想不是很好回答，因为：\n",
    "\n",
    "不用的话，训练出的模型参数很可能在测试时就不准确了；\n",
    "用的话，万一测试的数据就只有一个数据实例就不好办了。\n",
    "事实上，在测试时我们还是需要继续使用批量归一化的，只是需要做些改动。在测试时，我们需要把原先训练时用到的批量均值和方差替换成整个训练数据的均值和方差。但 是当训练数据极大时，这个计算开销很大。因此，我们用移动平均的方法来近似计算（参见实现中的`moving_mean`和`moving_variance`）。\n",
    "\n",
    "为了方便讨论批量归一化层的实现，我们先看下面这段代码来理解`Python`变量可以如何修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, is_training, moving_mean, moving_variance,\n",
    "               eps = 1e-5, moving_momentum = 0.9):\n",
    "    assert len(X.shape) in (2, 4)\n",
    "    # 全连接: batch_size x feature\n",
    "    if len(X.shape) == 2:\n",
    "        # 每个输入维度在样本上的平均和方差\n",
    "        #mean = X.mean(axis=0)\n",
    "        #variance = ((X - mean)**2).mean(axis=0)\n",
    "        mean = tf.reduce_mean(X, axis=0)\n",
    "        variance = tf.reduce_mean((X - mean)**2, axis=0)\n",
    "    # 2D卷积: batch_size x channel x height x width\n",
    "    else:\n",
    "        # 对每个通道算均值和方差，需要保持4D形状使得可以正确的广播\n",
    "        #mean = X.mean(axis=(0,2,3), keepdims=True)\n",
    "        #variance = ((X - mean)**2).mean(axis=(0,2,3), keepdims=True)\n",
    "        mean = tf.reduce_mean(X, axis=(0,1,2), keep_dims=True)\n",
    "        variance = tf.reduce_mean((X - mean)**2, axis=(0,1,2), keep_dims=True)\n",
    "        # 变形使得可以正确的广播\n",
    "\n",
    "\n",
    "        moving_mean = tf.reshape(moving_mean, mean.shape)\n",
    "        moving_variance = tf.reshape(moving_variance, mean.shape)\n",
    "\n",
    "    # 均一化\n",
    "    def train_update():\n",
    "        X_hat = (X - mean) / tf.sqrt(variance + eps)\n",
    "        #!!! 更新全局的均值和方差\n",
    "        moving_mean_new = moving_momentum * moving_mean + (1.0 - moving_momentum) * mean\n",
    "        moving_variance_new = moving_momentum * moving_variance + (1.0 - moving_momentum) * variance\n",
    "        return X_hat, moving_mean_new, moving_variance_new\n",
    "    def test_update():\n",
    "        #!!! 测试阶段使用全局的均值和方差\n",
    "        X_hat = (X - moving_mean) / tf.sqrt(moving_variance + eps)\n",
    "        return X_hat, moving_mean, moving_variance\n",
    "    \n",
    "    X_hat, moving_mean, moving_variance = tf.cond(is_training, train_update, test_update)\n",
    "    # 拉升和偏移\n",
    "    return gamma* X_hat + beta, moving_mean, moving_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "weight_scale = .01\n",
    "\n",
    "# 输出通道 = 20, 卷积核 = (5,5)\n",
    "# height*width*input_channels*output_channels\n",
    "\n",
    "c1 = 20\n",
    "W1 = tf.Variable(tf.random_normal([5,5,1,c1], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "b1 = tf.Variable(tf.constant(0.0, shape=[c1]))\n",
    "\n",
    "# 第1层批量归一化\n",
    "gamma1 = tf.Variable(tf.random_normal([c1], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "beta1 = tf.Variable(tf.random_normal([c1], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "moving_mean1 = tf.Variable(tf.constant(0.0, shape=[c1]), trainable=False)\n",
    "moving_variance1 = tf.Variable(tf.constant(0.0, shape=[c1]), trainable=False)\n",
    "\n",
    "# 输出通道 = 50, 卷积核 = (3,3)\n",
    "c2 = 50\n",
    "W2 = tf.Variable(tf.random_normal([3,3,c1,c2], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "b2 = tf.Variable(tf.constant(0.0, shape=[c2]))\n",
    "\n",
    "# 第2层批量归一化\n",
    "gamma2 = tf.Variable(tf.random_normal([c2], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "beta2 = tf.Variable(tf.random_normal([c2], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "moving_mean2 = tf.Variable(tf.constant(0.0, shape=[c2]), trainable=False)\n",
    "moving_variance2 = tf.Variable(tf.constant(0.0, shape=[c2]), trainable=False)\n",
    "\n",
    "# 输出维度 = 128\n",
    "o3 = 128\n",
    "W3 = tf.Variable(tf.random_normal([1250, o3], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "b3 = tf.Variable(tf.constant(0.0, shape=[o3]), trainable=False)\n",
    "\n",
    "# 输出维度 = 10\n",
    "W4 = tf.Variable(tf.random_normal([o3, 10], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32))\n",
    "b4 = tf.Variable(tf.constant(0.0, shape=[10]), trainable=False)\n",
    "\n",
    "# 注意这里moving_*是不需要更新的\n",
    "params = [W1, b1, gamma1, beta1,\n",
    "          W2, b2, gamma2, beta2,\n",
    "          W3, b3, W4, b4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面定义模型。我们添加了批量归一化层。特别要注意我们添加的位置：在卷积层后，在激活函数前。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, is_training, verbose=False):\n",
    "    global moving_mean1, moving_variance1, moving_mean2, moving_variance2\n",
    "    # 第一层卷积\n",
    "    #'''\n",
    "    h1_conv = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'VALID') \n",
    "    h1_bn, moving_mean1, moving_variance1 = batch_norm(h1_conv, gamma1, beta1, is_training,\n",
    "                       moving_mean1, moving_variance1)\n",
    "    h1_activation = tf.nn.relu(h1_bn)\n",
    "    h1 = tf.nn.max_pool(h1_activation, [1,2,2,1], [1,2,2,1], padding = 'VALID')\n",
    "    # 第二层卷积\n",
    "    h2_conv = tf.nn.conv2d(h1, W2, strides = [1,1,1,1], padding = 'VALID') \n",
    "    h2_bn, moving_mean2, moving_variance2 = batch_norm(h2_conv, gamma2, beta2, is_training,\n",
    "                       moving_mean2, moving_variance2)\n",
    "    h2_activation = tf.nn.relu(h2_bn)\n",
    "    h2 = tf.nn.max_pool(h2_activation, [1,2,2,1], [1,2,2,1], padding='VALID')\n",
    "    h2 = tf.layers.flatten(h2)\n",
    "    # 第一层全连接\n",
    "    #'''\n",
    "    h3_linear = tf.matmul(h2, W3) + b3\n",
    "\n",
    "    #h3_linear = tf.matmul(X, W3) + b3\n",
    "    h3 = tf.nn.relu(h3_linear)\n",
    "    # 第二层全连接\n",
    "    h4_linear = tf.matmul(h3, W4) + b4\n",
    "    if verbose:\n",
    "        print('1st conv block:', h1.get_shape().as_list())\n",
    "        print('2nd conv block:', h2.get_shape().as_list())\n",
    "        print('1st dense:', h3.get_shape().as_list())\n",
    "        print('2nd dense:', h4_linear.get_shape().as_list())\n",
    "        print('output:', h4_linear)\n",
    "    return h4_linear, h2_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "WARNING:tensorflow:From <ipython-input-6-436b2df27ba9>:16: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Batch 0, Loss: 2.302599, Train acc 0.113281 \n",
      "Batch 10, Loss: 2.302525, Train acc 0.246094 \n",
      "Batch 20, Loss: 2.302248, Train acc 0.308594 \n",
      "Batch 30, Loss: 2.300201, Train acc 0.453125 \n",
      "Batch 40, Loss: 2.253733, Train acc 0.148438 \n",
      "Batch 50, Loss: 2.114796, Train acc 0.156250 \n",
      "Batch 60, Loss: 1.646562, Train acc 0.386719 \n",
      "Batch 70, Loss: 1.364214, Train acc 0.445312 \n",
      "Batch 80, Loss: 1.178418, Train acc 0.550781 \n",
      "Batch 90, Loss: 0.971520, Train acc 0.664062 \n",
      "Batch 100, Loss: 0.822911, Train acc 0.632812 \n",
      "Batch 110, Loss: 0.675300, Train acc 0.734375 \n",
      "Batch 120, Loss: 0.647378, Train acc 0.742188 \n",
      "Batch 130, Loss: 0.643133, Train acc 0.769531 \n",
      "Batch 140, Loss: 0.667272, Train acc 0.746094 \n",
      "Batch 150, Loss: 0.628600, Train acc 0.730469 \n",
      "Batch 160, Loss: 0.538986, Train acc 0.777344 \n",
      "Batch 170, Loss: 0.565672, Train acc 0.761719 \n",
      "Batch 180, Loss: 0.683208, Train acc 0.730469 \n",
      "Batch 190, Loss: 0.677867, Train acc 0.710938 \n",
      "Batch 200, Loss: 0.557403, Train acc 0.816406 \n",
      "Batch 210, Loss: 0.568887, Train acc 0.777344 \n",
      "Batch 220, Loss: 0.418457, Train acc 0.843750 \n",
      "Batch 230, Loss: 0.484224, Train acc 0.835938 \n",
      "Batch 240, Loss: 0.416737, Train acc 0.835938 \n",
      "Batch 250, Loss: 0.533000, Train acc 0.781250 \n",
      "Batch 260, Loss: 0.402527, Train acc 0.832031 \n",
      "Batch 270, Loss: 0.455515, Train acc 0.800781 \n",
      "Batch 280, Loss: 0.512768, Train acc 0.796875 \n",
      "Batch 290, Loss: 0.508844, Train acc 0.824219 \n",
      "Batch 300, Loss: 0.497134, Train acc 0.792969 \n",
      "Batch 310, Loss: 0.466485, Train acc 0.804688 \n",
      "Batch 320, Loss: 0.389234, Train acc 0.867188 \n",
      "Batch 330, Loss: 0.456529, Train acc 0.824219 \n",
      "Batch 340, Loss: 0.383521, Train acc 0.847656 \n",
      "Batch 350, Loss: 0.521513, Train acc 0.796875 \n",
      "Batch 360, Loss: 0.542607, Train acc 0.792969 \n",
      "Batch 370, Loss: 0.473064, Train acc 0.808594 \n",
      "Batch 380, Loss: 0.367399, Train acc 0.843750 \n",
      "Batch 390, Loss: 0.368278, Train acc 0.882812 \n",
      "Batch 400, Loss: 0.370618, Train acc 0.847656 \n",
      "Batch 410, Loss: 0.505837, Train acc 0.796875 \n",
      "Batch 420, Loss: 0.417852, Train acc 0.808594 \n",
      "Batch 430, Loss: 0.403864, Train acc 0.859375 \n",
      "Batch 440, Loss: 0.544129, Train acc 0.796875 \n",
      "Batch 450, Loss: 0.309121, Train acc 0.890625 \n",
      "Batch 460, Loss: 0.342906, Train acc 0.882812 \n",
      "Batch 470, Loss: 0.435968, Train acc 0.832031 \n",
      "Batch 480, Loss: 0.385511, Train acc 0.839844 \n",
      "Batch 490, Loss: 0.360504, Train acc 0.878906 \n",
      "Batch 500, Loss: 0.382516, Train acc 0.851562 \n",
      "Batch 510, Loss: 0.351001, Train acc 0.871094 \n",
      "Batch 520, Loss: 0.389375, Train acc 0.855469 \n",
      "Batch 530, Loss: 0.307393, Train acc 0.878906 \n",
      "Batch 540, Loss: 0.313030, Train acc 0.875000 \n",
      "Batch 550, Loss: 0.398134, Train acc 0.832031 \n",
      "Batch 560, Loss: 0.291426, Train acc 0.890625 \n",
      "Batch 570, Loss: 0.421733, Train acc 0.824219 \n",
      "Batch 580, Loss: 0.297726, Train acc 0.890625 \n",
      "Batch 590, Loss: 0.347323, Train acc 0.871094 \n",
      "Batch 600, Loss: 0.340738, Train acc 0.875000 \n",
      "Batch 610, Loss: 0.304203, Train acc 0.882812 \n",
      "Batch 620, Loss: 0.372055, Train acc 0.859375 \n",
      "Batch 630, Loss: 0.324906, Train acc 0.875000 \n",
      "Batch 640, Loss: 0.362392, Train acc 0.890625 \n",
      "Batch 650, Loss: 0.344079, Train acc 0.875000 \n",
      "Batch 660, Loss: 0.481533, Train acc 0.835938 \n",
      "Batch 670, Loss: 0.434933, Train acc 0.855469 \n",
      "Batch 680, Loss: 0.368044, Train acc 0.855469 \n",
      "Batch 690, Loss: 0.339949, Train acc 0.875000 \n",
      "Batch 700, Loss: 0.389520, Train acc 0.847656 \n",
      "Batch 710, Loss: 0.259129, Train acc 0.914062 \n",
      "Batch 720, Loss: 0.346255, Train acc 0.882812 \n",
      "Batch 730, Loss: 0.341552, Train acc 0.855469 \n",
      "Batch 740, Loss: 0.363824, Train acc 0.855469 \n",
      "Batch 750, Loss: 0.247457, Train acc 0.910156 \n",
      "Batch 760, Loss: 0.317726, Train acc 0.871094 \n",
      "Batch 770, Loss: 0.342984, Train acc 0.867188 \n",
      "Batch 780, Loss: 0.344976, Train acc 0.882812 \n",
      "Batch 790, Loss: 0.288847, Train acc 0.886719 \n",
      "Batch 800, Loss: 0.272944, Train acc 0.890625 \n",
      "Batch 810, Loss: 0.247329, Train acc 0.921875 \n",
      "Batch 820, Loss: 0.323071, Train acc 0.875000 \n",
      "Batch 830, Loss: 0.300718, Train acc 0.894531 \n",
      "Batch 840, Loss: 0.319898, Train acc 0.875000 \n",
      "Batch 850, Loss: 0.227780, Train acc 0.894531 \n",
      "Batch 860, Loss: 0.285902, Train acc 0.886719 \n",
      "Batch 870, Loss: 0.336454, Train acc 0.886719 \n",
      "Batch 880, Loss: 0.306936, Train acc 0.890625 \n",
      "Batch 890, Loss: 0.241244, Train acc 0.914062 \n",
      "Batch 900, Loss: 0.296978, Train acc 0.863281 \n",
      "Batch 910, Loss: 0.325112, Train acc 0.867188 \n",
      "Batch 920, Loss: 0.269851, Train acc 0.890625 \n",
      "Batch 930, Loss: 0.389692, Train acc 0.843750 \n",
      "Batch 940, Loss: 0.243604, Train acc 0.910156 \n",
      "Batch 950, Loss: 0.331995, Train acc 0.886719 \n",
      "Batch 960, Loss: 0.310525, Train acc 0.894531 \n",
      "Batch 970, Loss: 0.263122, Train acc 0.902344 \n",
      "Batch 980, Loss: 0.323514, Train acc 0.882812 \n",
      "Batch 990, Loss: 0.314118, Train acc 0.863281 \n",
      "Test Loss: 14613.356445, Test acc 0.733600 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print train_labels.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 1e-0\n",
    "max_steps = 1000\n",
    "batch_size = 256\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "#input_placeholder = tf.placeholder(tf.float32, [None, height*width*num_channels])\n",
    "\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "\n",
    "logits, h2 = net(input_placeholder, is_training)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits,  onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width, num_channels))\n",
    "    \n",
    "#train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label, is_training: True}\n",
    "    h2_, loss_, acc_, _ = sess.run([h2, loss, acc, train_op], feed_dict=feed_dict)\n",
    "    if step % 10 == 0:\n",
    "        print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "\n",
    "test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_images_reshape / 255.0, gt_placeholder: test_labels, is_training: False})\n",
    "print (\"Test Loss: %f, Test acc %f \" % (test_loss_, test_acc_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2b66fd261ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
