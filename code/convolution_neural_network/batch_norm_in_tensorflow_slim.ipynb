{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 批量归一化 — 使用tensorflow\n",
    "本章介绍如何使用tensorflow在训练和测试深度学习模型中使用批量归一化。\n",
    "\n",
    "### 定义模型并添加批量归一化层\n",
    "有了tensorflow，我们模型的定义工作变得简单了许多。我们只需要添加slim.batch_norm层并指定对二维卷积的通道(axis=1)进行批量归一化。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def net(input, is_training):\n",
    "    with tf.variable_scope('lenet'):\n",
    "        conv1 = slim.conv2d(input, 20, [5, 5], scope='conv1_1', activation_fn=None,weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        conv1 = slim.batch_norm(conv1, decay=0.9, is_training=is_training, updates_collections=None)\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        \n",
    "        pool1 = slim.max_pool2d(conv1, [2, 2], scope='pool_1')\n",
    "        \n",
    "        conv2 = slim.conv2d(pool1, 50, [3, 3], scope='conv2_2', activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        conv1 = slim.batch_norm(conv1, decay=0.9, is_training=is_training, updates_collections=None)\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "        \n",
    "        pool2 = slim.max_pool2d(conv2, [2, 2], scope='pool_2')\n",
    "        pool2 = slim.flatten(pool2)\n",
    "        fc1 = slim.fully_connected(pool2, 128, scope='fc1', activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        fc2 = slim.fully_connected(fc1, 10, scope='fc2', activation_fn=None, weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        return conv1, conv2, fc1, fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "> <ipython-input-1-0737824316c7>(8)net()\n",
      "-> conv1 = slim.conv2d(input, 20, [5, 5], scope='conv1_1', activation_fn=None,weights_initializer=tf.random_normal_initializer(stddev=0.01))\n",
      "(Pdb) c\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print test_images.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)\n",
    "test_dataset = DataSet(test_images, test_labels, one_hot=True)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "max_steps = 1000\n",
    "batch_size = 256\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "#input_placeholder = tf.placeholder(tf.float32, [None, height*width*num_channels])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "conv1, conv2, fc1, logits = net(input_placeholder, is_training)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits,  onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "test_images_reshape = np.expand_dims(np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width)), axis=-1)\n",
    "    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "test_acc = []\n",
    "test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 2.302401, Train acc 0.066406 \n",
      "Test Loss: 2.302563, Test acc 0.100000 \n",
      "Batch 10, Loss: 2.300360, Train acc 0.109375 \n",
      "Test Loss: 2.302372, Test acc 0.100000 \n",
      "Batch 20, Loss: 2.294809, Train acc 0.273438 \n",
      "Test Loss: 2.301208, Test acc 0.100200 \n",
      "Batch 30, Loss: 2.264817, Train acc 0.378906 \n",
      "Test Loss: 2.286034, Test acc 0.332900 \n",
      "Batch 40, Loss: 1.804419, Train acc 0.457031 \n",
      "Test Loss: 1.751478, Test acc 0.391500 \n",
      "Batch 50, Loss: 1.460266, Train acc 0.429688 \n",
      "Test Loss: 1.345761, Test acc 0.482500 \n",
      "Batch 60, Loss: 1.199955, Train acc 0.574219 \n",
      "Test Loss: 1.448195, Test acc 0.447100 \n",
      "Batch 70, Loss: 1.264191, Train acc 0.449219 \n",
      "Test Loss: 1.165174, Test acc 0.503500 \n",
      "Batch 80, Loss: 1.101233, Train acc 0.542969 \n",
      "Test Loss: 1.161045, Test acc 0.582100 \n",
      "Batch 90, Loss: 0.876922, Train acc 0.632812 \n",
      "Test Loss: 0.898799, Test acc 0.643500 \n",
      "Batch 100, Loss: 0.941172, Train acc 0.601562 \n",
      "Test Loss: 1.034490, Test acc 0.595900 \n",
      "Batch 110, Loss: 0.735028, Train acc 0.730469 \n",
      "Test Loss: 0.749271, Test acc 0.704700 \n",
      "Batch 120, Loss: 0.767148, Train acc 0.742188 \n",
      "Test Loss: 0.713140, Test acc 0.717000 \n",
      "Batch 130, Loss: 0.829067, Train acc 0.691406 \n",
      "Test Loss: 0.808483, Test acc 0.679700 \n",
      "Batch 140, Loss: 0.609609, Train acc 0.746094 \n",
      "Test Loss: 0.652637, Test acc 0.743000 \n",
      "Batch 150, Loss: 0.625463, Train acc 0.761719 \n",
      "Test Loss: 0.683117, Test acc 0.720900 \n",
      "Batch 160, Loss: 0.706493, Train acc 0.750000 \n",
      "Test Loss: 0.623663, Test acc 0.759600 \n",
      "Batch 170, Loss: 0.620340, Train acc 0.753906 \n",
      "Test Loss: 0.628628, Test acc 0.740700 \n",
      "Batch 180, Loss: 0.642271, Train acc 0.738281 \n",
      "Test Loss: 0.675658, Test acc 0.730000 \n",
      "Batch 190, Loss: 0.595547, Train acc 0.753906 \n",
      "Test Loss: 0.635001, Test acc 0.744300 \n",
      "Batch 200, Loss: 0.583224, Train acc 0.742188 \n",
      "Test Loss: 0.643095, Test acc 0.748400 \n",
      "Batch 210, Loss: 0.499265, Train acc 0.789062 \n",
      "Test Loss: 0.541001, Test acc 0.796900 \n",
      "Batch 220, Loss: 0.472221, Train acc 0.812500 \n",
      "Test Loss: 0.530006, Test acc 0.797700 \n",
      "Batch 230, Loss: 0.547845, Train acc 0.792969 \n",
      "Test Loss: 0.547373, Test acc 0.786600 \n",
      "Batch 240, Loss: 0.517485, Train acc 0.816406 \n",
      "Test Loss: 0.542190, Test acc 0.792300 \n",
      "Batch 250, Loss: 0.494800, Train acc 0.808594 \n",
      "Test Loss: 0.508599, Test acc 0.806600 \n",
      "Batch 260, Loss: 0.437555, Train acc 0.847656 \n",
      "Test Loss: 0.503264, Test acc 0.803400 \n",
      "Batch 270, Loss: 0.431582, Train acc 0.847656 \n",
      "Test Loss: 0.481247, Test acc 0.821100 \n",
      "Batch 280, Loss: 0.522481, Train acc 0.820312 \n",
      "Test Loss: 0.479763, Test acc 0.822100 \n",
      "Batch 290, Loss: 0.412117, Train acc 0.847656 \n",
      "Test Loss: 0.468861, Test acc 0.825900 \n",
      "Batch 300, Loss: 0.523884, Train acc 0.828125 \n",
      "Test Loss: 0.475054, Test acc 0.824800 \n",
      "Batch 310, Loss: 0.486570, Train acc 0.804688 \n",
      "Test Loss: 0.570302, Test acc 0.778400 \n",
      "Batch 320, Loss: 0.448778, Train acc 0.820312 \n",
      "Test Loss: 0.464789, Test acc 0.824000 \n",
      "Batch 330, Loss: 0.404047, Train acc 0.871094 \n",
      "Test Loss: 0.460298, Test acc 0.830700 \n",
      "Batch 340, Loss: 0.433124, Train acc 0.835938 \n",
      "Test Loss: 0.444182, Test acc 0.833800 \n",
      "Batch 350, Loss: 0.512605, Train acc 0.800781 \n",
      "Test Loss: 0.560041, Test acc 0.785300 \n",
      "Batch 360, Loss: 0.409115, Train acc 0.851562 \n",
      "Test Loss: 0.450893, Test acc 0.830800 \n",
      "Batch 370, Loss: 0.466819, Train acc 0.804688 \n",
      "Test Loss: 0.475832, Test acc 0.818300 \n",
      "Batch 380, Loss: 0.355648, Train acc 0.890625 \n",
      "Test Loss: 0.431769, Test acc 0.838000 \n",
      "Batch 390, Loss: 0.482232, Train acc 0.808594 \n",
      "Test Loss: 0.448594, Test acc 0.828800 \n",
      "Batch 400, Loss: 0.437010, Train acc 0.839844 \n",
      "Test Loss: 0.485172, Test acc 0.816200 \n",
      "Batch 410, Loss: 0.483084, Train acc 0.824219 \n",
      "Test Loss: 0.427566, Test acc 0.839500 \n",
      "Batch 420, Loss: 0.416697, Train acc 0.863281 \n",
      "Test Loss: 0.459407, Test acc 0.823700 \n",
      "Batch 430, Loss: 0.381552, Train acc 0.859375 \n",
      "Test Loss: 0.442349, Test acc 0.834600 \n",
      "Batch 440, Loss: 0.416464, Train acc 0.828125 \n",
      "Test Loss: 0.414103, Test acc 0.846200 \n",
      "Batch 450, Loss: 0.422024, Train acc 0.843750 \n",
      "Test Loss: 0.402767, Test acc 0.855000 \n",
      "Batch 460, Loss: 0.403521, Train acc 0.863281 \n",
      "Test Loss: 0.394637, Test acc 0.853800 \n",
      "Batch 470, Loss: 0.328741, Train acc 0.890625 \n",
      "Test Loss: 0.397687, Test acc 0.853400 \n",
      "Batch 480, Loss: 0.358006, Train acc 0.851562 \n",
      "Test Loss: 0.405899, Test acc 0.849100 \n",
      "Batch 490, Loss: 0.363973, Train acc 0.871094 \n",
      "Test Loss: 0.401036, Test acc 0.852400 \n",
      "Batch 500, Loss: 0.389044, Train acc 0.863281 \n",
      "Test Loss: 0.395233, Test acc 0.854900 \n",
      "Batch 510, Loss: 0.331560, Train acc 0.886719 \n",
      "Test Loss: 0.392146, Test acc 0.855900 \n",
      "Batch 520, Loss: 0.439984, Train acc 0.855469 \n",
      "Test Loss: 0.393546, Test acc 0.854300 \n",
      "Batch 530, Loss: 0.407080, Train acc 0.847656 \n",
      "Test Loss: 0.391749, Test acc 0.854000 \n",
      "Batch 540, Loss: 0.395485, Train acc 0.855469 \n",
      "Test Loss: 0.426713, Test acc 0.836100 \n",
      "Batch 550, Loss: 0.404102, Train acc 0.847656 \n",
      "Test Loss: 0.402595, Test acc 0.848700 \n",
      "Batch 560, Loss: 0.461540, Train acc 0.828125 \n",
      "Test Loss: 0.425919, Test acc 0.841200 \n",
      "Batch 570, Loss: 0.419933, Train acc 0.855469 \n",
      "Test Loss: 0.396369, Test acc 0.857300 \n",
      "Batch 580, Loss: 0.333946, Train acc 0.875000 \n",
      "Test Loss: 0.387158, Test acc 0.854100 \n",
      "Batch 590, Loss: 0.331157, Train acc 0.894531 \n",
      "Test Loss: 0.363073, Test acc 0.867700 \n",
      "Batch 600, Loss: 0.367268, Train acc 0.882812 \n",
      "Test Loss: 0.369146, Test acc 0.864100 \n",
      "Batch 610, Loss: 0.386330, Train acc 0.902344 \n",
      "Test Loss: 0.379472, Test acc 0.861400 \n",
      "Batch 620, Loss: 0.295906, Train acc 0.894531 \n",
      "Test Loss: 0.360582, Test acc 0.870500 \n",
      "Batch 630, Loss: 0.346908, Train acc 0.875000 \n",
      "Test Loss: 0.377234, Test acc 0.860600 \n",
      "Batch 640, Loss: 0.391660, Train acc 0.855469 \n",
      "Test Loss: 0.358770, Test acc 0.868200 \n",
      "Batch 650, Loss: 0.410758, Train acc 0.859375 \n",
      "Test Loss: 0.394896, Test acc 0.851700 \n",
      "Batch 660, Loss: 0.282004, Train acc 0.878906 \n",
      "Test Loss: 0.357841, Test acc 0.867700 \n",
      "Batch 670, Loss: 0.436381, Train acc 0.851562 \n",
      "Test Loss: 0.372225, Test acc 0.861600 \n",
      "Batch 680, Loss: 0.265227, Train acc 0.925781 \n",
      "Test Loss: 0.350202, Test acc 0.869400 \n",
      "Batch 690, Loss: 0.329802, Train acc 0.894531 \n",
      "Test Loss: 0.360189, Test acc 0.866800 \n",
      "Batch 700, Loss: 0.320785, Train acc 0.863281 \n",
      "Test Loss: 0.414607, Test acc 0.843000 \n",
      "Batch 710, Loss: 0.390228, Train acc 0.871094 \n",
      "Test Loss: 0.392866, Test acc 0.851100 \n",
      "Batch 720, Loss: 0.342061, Train acc 0.878906 \n",
      "Test Loss: 0.359801, Test acc 0.869400 \n",
      "Batch 730, Loss: 0.312132, Train acc 0.890625 \n",
      "Test Loss: 0.356401, Test acc 0.870700 \n",
      "Batch 740, Loss: 0.360884, Train acc 0.867188 \n",
      "Test Loss: 0.382230, Test acc 0.857600 \n",
      "Batch 750, Loss: 0.308566, Train acc 0.890625 \n",
      "Test Loss: 0.360677, Test acc 0.866600 \n",
      "Batch 760, Loss: 0.377081, Train acc 0.867188 \n",
      "Test Loss: 0.365977, Test acc 0.864600 \n",
      "Batch 770, Loss: 0.295392, Train acc 0.894531 \n",
      "Test Loss: 0.347480, Test acc 0.871300 \n",
      "Batch 780, Loss: 0.306472, Train acc 0.886719 \n",
      "Test Loss: 0.337117, Test acc 0.877700 \n",
      "Batch 790, Loss: 0.348668, Train acc 0.867188 \n",
      "Test Loss: 0.362673, Test acc 0.865400 \n",
      "Batch 800, Loss: 0.311618, Train acc 0.867188 \n",
      "Test Loss: 0.349811, Test acc 0.869900 \n",
      "Batch 810, Loss: 0.281739, Train acc 0.890625 \n",
      "Test Loss: 0.354589, Test acc 0.870900 \n",
      "Batch 820, Loss: 0.388527, Train acc 0.855469 \n",
      "Test Loss: 0.514057, Test acc 0.802600 \n",
      "Batch 830, Loss: 0.344078, Train acc 0.863281 \n",
      "Test Loss: 0.349930, Test acc 0.874500 \n",
      "Batch 840, Loss: 0.350339, Train acc 0.878906 \n",
      "Test Loss: 0.335600, Test acc 0.876500 \n",
      "Batch 850, Loss: 0.292994, Train acc 0.894531 \n",
      "Test Loss: 0.355916, Test acc 0.871000 \n",
      "Batch 860, Loss: 0.236269, Train acc 0.925781 \n",
      "Test Loss: 0.343578, Test acc 0.871600 \n",
      "Batch 870, Loss: 0.299203, Train acc 0.878906 \n",
      "Test Loss: 0.350475, Test acc 0.874700 \n",
      "Batch 880, Loss: 0.331493, Train acc 0.886719 \n",
      "Test Loss: 0.355538, Test acc 0.870100 \n",
      "Batch 890, Loss: 0.404836, Train acc 0.882812 \n",
      "Test Loss: 0.364420, Test acc 0.867600 \n",
      "Batch 900, Loss: 0.265512, Train acc 0.906250 \n",
      "Test Loss: 0.332182, Test acc 0.877400 \n",
      "Batch 910, Loss: 0.324471, Train acc 0.878906 \n",
      "Test Loss: 0.326837, Test acc 0.880200 \n",
      "Batch 920, Loss: 0.272038, Train acc 0.906250 \n",
      "Test Loss: 0.337733, Test acc 0.875200 \n",
      "Batch 930, Loss: 0.326336, Train acc 0.875000 \n",
      "Test Loss: 0.336126, Test acc 0.876400 \n",
      "Batch 940, Loss: 0.245042, Train acc 0.914062 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.321026, Test acc 0.881400 \n",
      "Batch 950, Loss: 0.228617, Train acc 0.910156 \n",
      "Test Loss: 0.324394, Test acc 0.882200 \n",
      "Batch 960, Loss: 0.263707, Train acc 0.910156 \n",
      "Test Loss: 0.335900, Test acc 0.875700 \n",
      "Batch 970, Loss: 0.293388, Train acc 0.886719 \n",
      "Test Loss: 0.338327, Test acc 0.876600 \n",
      "Batch 980, Loss: 0.245423, Train acc 0.906250 \n",
      "Test Loss: 0.344116, Test acc 0.873300 \n",
      "Batch 990, Loss: 0.308832, Train acc 0.894531 \n",
      "Test Loss: 0.334100, Test acc 0.877500 \n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label, is_training: True}\n",
    "    loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)    \n",
    "    if step % 10 == 0:\n",
    "        print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "        test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_images_reshape / 255.0, gt_placeholder: test_labels, is_training: False})\n",
    "\n",
    "        print (\"Test Loss: %f, Test acc %f \" % (test_loss_, test_acc_))\n",
    "\n",
    "#print (\"Test acc %f \" % (np.mean(test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
