{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更深的卷积神经网络：GoogLeNet\n",
    "在2014年的Imagenet竞赛里，Google的研究人员利用一个新的网络结构取得很大的优先。这个叫做GoogLeNet的网络虽然在名字上是向LeNet致敬，但网络结构里很难看到LeNet的影子。它颠覆的大家对卷积神经网络串联一系列层的固定做法。下图是其论文对GoogLeNet的可视化\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/googlenet.png)\n",
    "\n",
    "### 定义Inception\n",
    "可以看到其中有多个四个并行卷积层的块。这个块一般叫做Inception，其基于Network in network的思想做了很大的改进。我们先看下如何定义一个下图所示的Inception块。\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/inception.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "import pdb\n",
    "\n",
    "def Inception(input, num_channel_1_1, num_channel_2_1, num_channel_2_3, num_channel_3_1, num_channel_3_5, num_channel_4_1, scope='inception'):\n",
    "    with tf.variable_scope(scope):\n",
    "        #path 1\n",
    "        output_path1 = slim.conv2d(input, num_channel_1_1, [1, 1], scope='conv1_1')\n",
    "        #path 2\n",
    "        output_path2_1 = slim.conv2d(input, num_channel_2_1, [1, 1], scope='conv2_1')\n",
    "        output_path2_2 = slim.conv2d(output_path2_1, num_channel_2_3, [3, 3], scope='conv2_2')\n",
    "        #path 3\n",
    "        output_path_3_1 = slim.conv2d(input, num_channel_3_1, [1, 1], scope='conv3_1')\n",
    "        output_path_3_2 = slim.conv2d(output_path_3_1, num_channel_3_5, [5, 5], scope='conv3_2')\n",
    "        #path 4\n",
    "        \n",
    "        output_path_4_1 = slim.max_pool2d(input, [3, 3], 1, padding='SAME', scope='max_pool4_1')\n",
    "        output_path_4_2 = slim.conv2d(output_path_4_1, num_channel_4_1, [1, 1], scope='conv4_2')\n",
    "        return tf.concat([output_path1, output_path2_2, output_path_3_2, output_path_4_2], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到Inception里有四个并行的线路。\n",
    "\n",
    "- 单个$1×1$卷积。\n",
    "- $1×1$卷积接上$3×3$卷积。通常前者的通道数少于输入通道，这样减少后者的计算量。后者加上了padding=1使得输出的长宽的输入一致\n",
    "- 同2，但换成了$5×5$卷积\n",
    "- 和1类似，但卷积前用了最大池化层\n",
    "最后将这四个并行线路的结果在通道这个维度上合并在一起。\n",
    "\n",
    "测试一下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inception/concat:0\", shape=(32, 64, 64, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.uniform(size=(32,64,64,3)).astype(np.float32)\n",
    "\n",
    "print Inception(x, 64, 96, 128, 16, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义GoogLeNet\n",
    "GoogLeNet将数个Inception串联在一起。注意到原论文里使用了多个输出，为了简化我们这里就使用一个输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GoogleNet(input, num_classes, scope='googlenet'):\n",
    "    with tf.variable_scope(scope):\n",
    "        # block 1\n",
    "        b1_pad = tf.pad(input, [[0,0],[3,3],[3,3],[0,0]])\n",
    "        b1_conv = slim.conv2d(b1_pad, 64, [7, 7], stride=2,  padding='VALID', scope='block1_conv')\n",
    "        b1_pool = slim.max_pool2d(b1_conv, [3, 3], stride=2, padding='VALID', scope='block1_pool')\n",
    "        print b1_pool.shape\n",
    "        \n",
    "        # block 2\n",
    "        b2_conv1 = slim.conv2d(b1_pool, 64, [1, 1], scope='block2_conv1')\n",
    "        b2_conv1_pad = tf.pad(b2_conv1, [[0,0],[1,1],[1,1],[0,0]])\n",
    "        b2_conv2 = slim.conv2d(b2_conv1_pad, 192, [3, 3], scope='block2_conv2', padding='VALID')\n",
    "        b2_pool = slim.max_pool2d(b2_conv2, [3, 3], stride=2, scope='block2_pool')\n",
    "        print b2_pool.shape\n",
    "        \n",
    "        # block 3\n",
    "        b3_inception_1 = Inception(b2_pool, 64, 96, 128, 16, 32, 32, scope='block3_inception1')\n",
    "        b3_inception_2 = Inception(b3_inception_1, 128, 128, 192, 32, 96, 64, scope='block3_inception2')\n",
    "        b3_pool = slim.max_pool2d(b3_inception_2, [3, 3], stride=2, scope='block3_pool')\n",
    "        print b3_pool.shape\n",
    "        \n",
    "        # block 4\n",
    "        b4_inception_1 = Inception(b3_pool, 192, 96, 208, 16, 48, 64, scope='block4_inception1')\n",
    "        b4_inception_2 = Inception(b4_inception_1, 160, 112, 224, 24, 64, 64, scope='block4_inception2')\n",
    "        b4_inception_3 = Inception(b4_inception_2, 128, 128, 256, 24, 64, 64, scope='block4_inception3')\n",
    "        b4_inception_4 = Inception(b4_inception_3, 112, 144, 288, 32, 64, 64, scope='block4_inception4')\n",
    "        b4_inception_5 = Inception(b4_inception_4, 256, 160, 320, 32, 128, 128, scope='block4_inception5')\n",
    "        b4_pool = slim.max_pool2d(b4_inception_5, [3, 3], stride=2, scope='block4_pool')\n",
    "        print b4_pool.shape\n",
    "        \n",
    "        # block 5\n",
    "        b5_inception_1 = Inception(b4_pool, 256, 160, 320, 32, 128, 128, scope='block5_inception1')\n",
    "        b5_inception_2 = Inception(b5_inception_1, 384, 192, 384, 48, 128, 128, scope='block5_inception2')\n",
    "        b5_pool = slim.max_pool2d(b5_inception_2, [2, 2], scope='block5_pool')\n",
    "        print b5_pool.shape\n",
    "        \n",
    "        # block 6\n",
    "        b6 = slim.flatten(b5_pool)\n",
    "        return slim.fully_connected(b6, num_classes, scope='fc', activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 23, 23, 64)\n",
      "(4, 11, 11, 192)\n",
      "(4, 5, 5, 480)\n",
      "(4, 2, 2, 832)\n",
      "(4, 1, 1, 1024)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(size=(4, 96, 96, 3)).astype(np.float32)\n",
    "y = GoogleNet(x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据并训练\n",
    "跟VGG一样我们使用了较小的输入$96×96$来加速计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print test_images.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)\n",
    "test_dataset = DataSet(test_images, test_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 23, 23, 64)\n",
      "(?, 11, 11, 192)\n",
      "(?, 5, 5, 480)\n",
      "(?, 2, 2, 832)\n",
      "(?, 1, 1, 1024)\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Batch 0, Loss: 2.303140, Train acc 0.109375 \n",
      "Batch 1, Loss: 2.304469, Train acc 0.093750 \n",
      "Batch 2, Loss: 2.301654, Train acc 0.125000 \n",
      "Batch 3, Loss: 2.301726, Train acc 0.062500 \n",
      "Batch 4, Loss: 2.306557, Train acc 0.125000 \n",
      "Batch 5, Loss: 2.303081, Train acc 0.093750 \n",
      "Batch 6, Loss: 2.299683, Train acc 0.062500 \n",
      "Batch 7, Loss: 2.298925, Train acc 0.203125 \n",
      "Batch 8, Loss: 2.306689, Train acc 0.093750 \n",
      "Batch 9, Loss: 2.303386, Train acc 0.078125 \n",
      "Batch 10, Loss: 2.303192, Train acc 0.062500 \n",
      "Batch 11, Loss: 2.303641, Train acc 0.109375 \n",
      "Batch 12, Loss: 2.300117, Train acc 0.109375 \n",
      "Batch 13, Loss: 2.300389, Train acc 0.140625 \n",
      "Batch 14, Loss: 2.303990, Train acc 0.093750 \n",
      "Batch 15, Loss: 2.305586, Train acc 0.109375 \n",
      "Batch 16, Loss: 2.305924, Train acc 0.046875 \n",
      "Batch 17, Loss: 2.301204, Train acc 0.062500 \n",
      "Batch 18, Loss: 2.302166, Train acc 0.140625 \n",
      "Batch 19, Loss: 2.303674, Train acc 0.078125 \n",
      "Batch 20, Loss: 2.296199, Train acc 0.125000 \n",
      "Batch 21, Loss: 2.299808, Train acc 0.078125 \n",
      "Batch 22, Loss: 2.300760, Train acc 0.125000 \n",
      "Batch 23, Loss: 2.296253, Train acc 0.187500 \n",
      "Batch 24, Loss: 2.299798, Train acc 0.078125 \n",
      "Batch 25, Loss: 2.299427, Train acc 0.125000 \n",
      "Batch 26, Loss: 2.294856, Train acc 0.156250 \n",
      "Batch 27, Loss: 2.289427, Train acc 0.171875 \n",
      "Batch 28, Loss: 2.290074, Train acc 0.125000 \n",
      "Batch 29, Loss: 2.288537, Train acc 0.156250 \n",
      "Batch 30, Loss: 2.296778, Train acc 0.031250 \n",
      "Batch 31, Loss: 2.306114, Train acc 0.078125 \n",
      "Batch 32, Loss: 2.288361, Train acc 0.125000 \n",
      "Batch 33, Loss: 2.290188, Train acc 0.281250 \n",
      "Batch 34, Loss: 2.312792, Train acc 0.125000 \n",
      "Batch 35, Loss: 2.297798, Train acc 0.281250 \n",
      "Batch 36, Loss: 2.291928, Train acc 0.109375 \n",
      "Batch 37, Loss: 2.298912, Train acc 0.140625 \n",
      "Batch 38, Loss: 2.291059, Train acc 0.140625 \n",
      "Batch 39, Loss: 2.284442, Train acc 0.125000 \n",
      "Batch 40, Loss: 2.290784, Train acc 0.093750 \n",
      "Batch 41, Loss: 2.321531, Train acc 0.093750 \n",
      "Batch 42, Loss: 2.297181, Train acc 0.046875 \n",
      "Batch 43, Loss: 2.298952, Train acc 0.109375 \n",
      "Batch 44, Loss: 2.290664, Train acc 0.078125 \n",
      "Batch 45, Loss: 2.296909, Train acc 0.093750 \n",
      "Batch 46, Loss: 2.299456, Train acc 0.203125 \n",
      "Batch 47, Loss: 2.295091, Train acc 0.125000 \n",
      "Batch 48, Loss: 2.269858, Train acc 0.234375 \n",
      "Batch 49, Loss: 2.310276, Train acc 0.078125 \n",
      "Batch 50, Loss: 2.292660, Train acc 0.109375 \n",
      "Batch 51, Loss: 2.285709, Train acc 0.140625 \n",
      "Batch 52, Loss: 2.286651, Train acc 0.109375 \n",
      "Batch 53, Loss: 2.295159, Train acc 0.093750 \n",
      "Batch 54, Loss: 2.299598, Train acc 0.078125 \n",
      "Batch 55, Loss: 2.291023, Train acc 0.046875 \n",
      "Batch 56, Loss: 2.285427, Train acc 0.171875 \n",
      "Batch 57, Loss: 2.291358, Train acc 0.125000 \n",
      "Batch 58, Loss: 2.277099, Train acc 0.156250 \n",
      "Batch 59, Loss: 2.282621, Train acc 0.187500 \n",
      "Batch 60, Loss: 2.287701, Train acc 0.109375 \n",
      "Batch 61, Loss: 2.294815, Train acc 0.093750 \n",
      "Batch 62, Loss: 2.291021, Train acc 0.109375 \n",
      "Batch 63, Loss: 2.283304, Train acc 0.093750 \n",
      "Batch 64, Loss: 2.296218, Train acc 0.046875 \n",
      "Batch 65, Loss: 2.264687, Train acc 0.203125 \n",
      "Batch 66, Loss: 2.283665, Train acc 0.046875 \n",
      "Batch 67, Loss: 2.284940, Train acc 0.203125 \n",
      "Batch 68, Loss: 2.293989, Train acc 0.046875 \n",
      "Batch 69, Loss: 2.288651, Train acc 0.062500 \n",
      "Batch 70, Loss: 2.266733, Train acc 0.218750 \n",
      "Batch 71, Loss: 2.270994, Train acc 0.140625 \n",
      "Batch 72, Loss: 2.285701, Train acc 0.078125 \n",
      "Batch 73, Loss: 2.278115, Train acc 0.156250 \n",
      "Batch 74, Loss: 2.289365, Train acc 0.171875 \n",
      "Batch 75, Loss: 2.256498, Train acc 0.234375 \n",
      "Batch 76, Loss: 2.263316, Train acc 0.156250 \n",
      "Batch 77, Loss: 2.280219, Train acc 0.171875 \n",
      "Batch 78, Loss: 2.292254, Train acc 0.093750 \n",
      "Batch 79, Loss: 2.269144, Train acc 0.203125 \n",
      "Batch 80, Loss: 2.265066, Train acc 0.218750 \n",
      "Batch 81, Loss: 2.258680, Train acc 0.218750 \n",
      "Batch 82, Loss: 2.248228, Train acc 0.312500 \n",
      "Batch 83, Loss: 2.254630, Train acc 0.125000 \n",
      "Batch 84, Loss: 2.271930, Train acc 0.093750 \n",
      "Batch 85, Loss: 2.233081, Train acc 0.125000 \n",
      "Batch 86, Loss: 2.212511, Train acc 0.187500 \n",
      "Batch 87, Loss: 2.242586, Train acc 0.156250 \n",
      "Batch 88, Loss: 2.261963, Train acc 0.125000 \n",
      "Batch 89, Loss: 2.215940, Train acc 0.218750 \n",
      "Batch 90, Loss: 2.236100, Train acc 0.078125 \n",
      "Batch 91, Loss: 2.289922, Train acc 0.125000 \n",
      "Batch 92, Loss: 2.285208, Train acc 0.140625 \n",
      "Batch 93, Loss: 2.257552, Train acc 0.218750 \n",
      "Batch 94, Loss: 2.242296, Train acc 0.156250 \n",
      "Batch 95, Loss: 2.169313, Train acc 0.187500 \n",
      "Batch 96, Loss: 2.059909, Train acc 0.265625 \n",
      "Batch 97, Loss: 2.530611, Train acc 0.046875 \n",
      "Batch 98, Loss: 2.331481, Train acc 0.093750 \n",
      "Batch 99, Loss: 2.315647, Train acc 0.078125 \n",
      "Batch 100, Loss: 2.273186, Train acc 0.125000 \n",
      "Batch 101, Loss: 2.269183, Train acc 0.140625 \n",
      "Batch 102, Loss: 2.293319, Train acc 0.078125 \n",
      "Batch 103, Loss: 2.315209, Train acc 0.140625 \n",
      "Batch 104, Loss: 2.285885, Train acc 0.046875 \n",
      "Batch 105, Loss: 2.277680, Train acc 0.109375 \n",
      "Batch 106, Loss: 2.248438, Train acc 0.125000 \n",
      "Batch 107, Loss: 2.311821, Train acc 0.093750 \n",
      "Batch 108, Loss: 2.272147, Train acc 0.125000 \n",
      "Batch 109, Loss: 2.283210, Train acc 0.203125 \n",
      "Batch 110, Loss: 2.296879, Train acc 0.062500 \n",
      "Batch 111, Loss: 2.296052, Train acc 0.140625 \n",
      "Batch 112, Loss: 2.246118, Train acc 0.281250 \n",
      "Batch 113, Loss: 2.303704, Train acc 0.109375 \n",
      "Batch 114, Loss: 2.269657, Train acc 0.093750 \n",
      "Batch 115, Loss: 2.263023, Train acc 0.156250 \n",
      "Batch 116, Loss: 2.245873, Train acc 0.109375 \n",
      "Batch 117, Loss: 2.271698, Train acc 0.171875 \n",
      "Batch 118, Loss: 2.271338, Train acc 0.078125 \n",
      "Batch 119, Loss: 2.247441, Train acc 0.156250 \n",
      "Batch 120, Loss: 2.246193, Train acc 0.093750 \n",
      "Batch 121, Loss: 2.256804, Train acc 0.140625 \n",
      "Batch 122, Loss: 2.232277, Train acc 0.062500 \n",
      "Batch 123, Loss: 2.188124, Train acc 0.156250 \n",
      "Batch 124, Loss: 2.208637, Train acc 0.234375 \n",
      "Batch 125, Loss: 2.378581, Train acc 0.109375 \n",
      "Batch 126, Loss: 2.271498, Train acc 0.062500 \n",
      "Batch 127, Loss: 2.236291, Train acc 0.203125 \n",
      "Batch 128, Loss: 2.202339, Train acc 0.359375 \n",
      "Batch 129, Loss: 2.232774, Train acc 0.093750 \n",
      "Batch 130, Loss: 2.193880, Train acc 0.203125 \n",
      "Batch 131, Loss: 2.240186, Train acc 0.046875 \n",
      "Batch 132, Loss: 2.327575, Train acc 0.062500 \n",
      "Batch 133, Loss: 2.172504, Train acc 0.250000 \n",
      "Batch 134, Loss: 2.104812, Train acc 0.312500 \n",
      "Batch 135, Loss: 2.147252, Train acc 0.203125 \n",
      "Batch 136, Loss: 2.650162, Train acc 0.046875 \n",
      "Batch 137, Loss: 2.338251, Train acc 0.078125 \n",
      "Batch 138, Loss: 2.300540, Train acc 0.078125 \n",
      "Batch 139, Loss: 2.305184, Train acc 0.109375 \n",
      "Batch 140, Loss: 2.277990, Train acc 0.125000 \n",
      "Batch 141, Loss: 2.305110, Train acc 0.031250 \n",
      "Batch 142, Loss: 2.289066, Train acc 0.156250 \n",
      "Batch 143, Loss: 2.291304, Train acc 0.109375 \n",
      "Batch 144, Loss: 2.270720, Train acc 0.171875 \n",
      "Batch 145, Loss: 2.236290, Train acc 0.171875 \n",
      "Batch 146, Loss: 2.222526, Train acc 0.203125 \n",
      "Batch 147, Loss: 2.197252, Train acc 0.328125 \n",
      "Batch 148, Loss: 2.217877, Train acc 0.109375 \n",
      "Batch 149, Loss: 2.185095, Train acc 0.312500 \n",
      "Batch 150, Loss: 2.173352, Train acc 0.203125 \n",
      "Batch 151, Loss: 2.165028, Train acc 0.296875 \n",
      "Batch 152, Loss: 2.069930, Train acc 0.312500 \n",
      "Batch 153, Loss: 2.163384, Train acc 0.234375 \n",
      "Batch 154, Loss: 2.391543, Train acc 0.046875 \n",
      "Batch 155, Loss: 2.393997, Train acc 0.109375 \n",
      "Batch 156, Loss: 2.247664, Train acc 0.156250 \n",
      "Batch 157, Loss: 2.237202, Train acc 0.203125 \n",
      "Batch 158, Loss: 2.195687, Train acc 0.359375 \n",
      "Batch 159, Loss: 2.192575, Train acc 0.171875 \n",
      "Batch 160, Loss: 2.189088, Train acc 0.218750 \n",
      "Batch 161, Loss: 2.156946, Train acc 0.328125 \n",
      "Batch 162, Loss: 2.037935, Train acc 0.281250 \n",
      "Batch 163, Loss: 2.161591, Train acc 0.156250 \n",
      "Batch 164, Loss: 2.670100, Train acc 0.140625 \n",
      "Batch 165, Loss: 2.322417, Train acc 0.062500 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 166, Loss: 2.283997, Train acc 0.046875 \n",
      "Batch 167, Loss: 2.269356, Train acc 0.156250 \n",
      "Batch 168, Loss: 2.256281, Train acc 0.140625 \n",
      "Batch 169, Loss: 2.234455, Train acc 0.234375 \n",
      "Batch 170, Loss: 2.235799, Train acc 0.156250 \n",
      "Batch 171, Loss: 2.171879, Train acc 0.437500 \n",
      "Batch 172, Loss: 2.139521, Train acc 0.281250 \n",
      "Batch 173, Loss: 2.125352, Train acc 0.218750 \n",
      "Batch 174, Loss: 1.948970, Train acc 0.281250 \n",
      "Batch 175, Loss: 1.970015, Train acc 0.234375 \n",
      "Batch 176, Loss: 4.040786, Train acc 0.015625 \n",
      "Batch 177, Loss: 2.304593, Train acc 0.046875 \n",
      "Batch 178, Loss: 2.300604, Train acc 0.125000 \n",
      "Batch 179, Loss: 2.291907, Train acc 0.187500 \n",
      "Batch 180, Loss: 2.307759, Train acc 0.109375 \n",
      "Batch 181, Loss: 2.307994, Train acc 0.109375 \n",
      "Batch 182, Loss: 2.304214, Train acc 0.062500 \n",
      "Batch 183, Loss: 2.291765, Train acc 0.093750 \n",
      "Batch 184, Loss: 2.301954, Train acc 0.093750 \n",
      "Batch 185, Loss: 2.285004, Train acc 0.125000 \n",
      "Batch 186, Loss: 2.277195, Train acc 0.093750 \n",
      "Batch 187, Loss: 2.287016, Train acc 0.062500 \n",
      "Batch 188, Loss: 2.273267, Train acc 0.078125 \n",
      "Batch 189, Loss: 2.263122, Train acc 0.125000 \n",
      "Batch 190, Loss: 2.280480, Train acc 0.156250 \n",
      "Batch 191, Loss: 2.255005, Train acc 0.218750 \n",
      "Batch 192, Loss: 2.267472, Train acc 0.171875 \n",
      "Batch 193, Loss: 2.249408, Train acc 0.187500 \n",
      "Batch 194, Loss: 2.260908, Train acc 0.125000 \n",
      "Batch 195, Loss: 2.267853, Train acc 0.156250 \n",
      "Batch 196, Loss: 2.237153, Train acc 0.156250 \n",
      "Batch 197, Loss: 2.225700, Train acc 0.187500 \n",
      "Batch 198, Loss: 2.238850, Train acc 0.187500 \n",
      "Batch 199, Loss: 2.259932, Train acc 0.250000 \n",
      "Batch 200, Loss: 2.215056, Train acc 0.171875 \n",
      "Batch 201, Loss: 2.183827, Train acc 0.312500 \n",
      "Batch 202, Loss: 2.200282, Train acc 0.140625 \n",
      "Batch 203, Loss: 2.210902, Train acc 0.218750 \n",
      "Batch 204, Loss: 2.159676, Train acc 0.265625 \n",
      "Batch 205, Loss: 2.144829, Train acc 0.203125 \n",
      "Batch 206, Loss: 2.145217, Train acc 0.187500 \n",
      "Batch 207, Loss: 2.230177, Train acc 0.125000 \n",
      "Batch 208, Loss: 2.173640, Train acc 0.187500 \n",
      "Batch 209, Loss: 2.183844, Train acc 0.234375 \n",
      "Batch 210, Loss: 2.154630, Train acc 0.203125 \n",
      "Batch 211, Loss: 2.203285, Train acc 0.156250 \n",
      "Batch 212, Loss: 2.042917, Train acc 0.296875 \n",
      "Batch 213, Loss: 2.028885, Train acc 0.234375 \n",
      "Batch 214, Loss: 2.008018, Train acc 0.218750 \n",
      "Batch 215, Loss: 2.026130, Train acc 0.265625 \n",
      "Batch 216, Loss: 2.365868, Train acc 0.171875 \n",
      "Batch 217, Loss: 2.496540, Train acc 0.046875 \n",
      "Batch 218, Loss: 2.337147, Train acc 0.140625 \n",
      "Batch 219, Loss: 2.318327, Train acc 0.125000 \n",
      "Batch 220, Loss: 2.308692, Train acc 0.109375 \n",
      "Batch 221, Loss: 2.300611, Train acc 0.093750 \n",
      "Batch 222, Loss: 2.310562, Train acc 0.093750 \n",
      "Batch 223, Loss: 2.322242, Train acc 0.109375 \n",
      "Batch 224, Loss: 2.307408, Train acc 0.046875 \n",
      "Batch 225, Loss: 2.257721, Train acc 0.156250 \n",
      "Batch 226, Loss: 2.304490, Train acc 0.093750 \n",
      "Batch 227, Loss: 2.302164, Train acc 0.125000 \n",
      "Batch 228, Loss: 2.283738, Train acc 0.062500 \n",
      "Batch 229, Loss: 2.275139, Train acc 0.109375 \n",
      "Batch 230, Loss: 2.248342, Train acc 0.125000 \n",
      "Batch 231, Loss: 2.244292, Train acc 0.093750 \n",
      "Batch 232, Loss: 2.239224, Train acc 0.109375 \n",
      "Batch 233, Loss: 2.201311, Train acc 0.140625 \n",
      "Batch 234, Loss: 2.212734, Train acc 0.078125 \n",
      "Batch 235, Loss: 2.264821, Train acc 0.078125 \n",
      "Batch 236, Loss: 2.210489, Train acc 0.312500 \n",
      "Batch 237, Loss: 2.198040, Train acc 0.265625 \n",
      "Batch 238, Loss: 2.129081, Train acc 0.265625 \n",
      "Batch 239, Loss: 2.208476, Train acc 0.093750 \n",
      "Batch 240, Loss: 2.219476, Train acc 0.171875 \n",
      "Batch 241, Loss: 2.100963, Train acc 0.296875 \n",
      "Batch 242, Loss: 2.037933, Train acc 0.312500 \n",
      "Batch 243, Loss: 1.933819, Train acc 0.312500 \n",
      "Batch 244, Loss: 1.838344, Train acc 0.359375 \n",
      "Batch 245, Loss: 1.997783, Train acc 0.171875 \n",
      "Batch 246, Loss: 2.843762, Train acc 0.109375 \n",
      "Batch 247, Loss: 2.271466, Train acc 0.046875 \n",
      "Batch 248, Loss: 2.279708, Train acc 0.125000 \n",
      "Batch 249, Loss: 2.226636, Train acc 0.187500 \n",
      "Batch 250, Loss: 2.192123, Train acc 0.171875 \n",
      "Batch 251, Loss: 2.169589, Train acc 0.234375 \n",
      "Batch 252, Loss: 2.134393, Train acc 0.140625 \n",
      "Batch 253, Loss: 2.053288, Train acc 0.187500 \n",
      "Batch 254, Loss: 1.939383, Train acc 0.281250 \n",
      "Batch 255, Loss: 1.871558, Train acc 0.281250 \n",
      "Batch 256, Loss: 1.780493, Train acc 0.328125 \n",
      "Batch 257, Loss: 1.998037, Train acc 0.171875 \n",
      "Batch 258, Loss: 2.157779, Train acc 0.109375 \n",
      "Batch 259, Loss: 2.340936, Train acc 0.187500 \n",
      "Batch 260, Loss: 2.176485, Train acc 0.218750 \n",
      "Batch 261, Loss: 2.004648, Train acc 0.250000 \n",
      "Batch 262, Loss: 1.897417, Train acc 0.312500 \n",
      "Batch 263, Loss: 1.758936, Train acc 0.281250 \n",
      "Batch 264, Loss: 1.767180, Train acc 0.312500 \n",
      "Batch 265, Loss: 1.747342, Train acc 0.234375 \n",
      "Batch 266, Loss: 1.819854, Train acc 0.328125 \n",
      "Batch 267, Loss: 1.777212, Train acc 0.218750 \n",
      "Batch 268, Loss: 1.679212, Train acc 0.375000 \n",
      "Batch 269, Loss: 1.761535, Train acc 0.296875 \n",
      "Batch 270, Loss: 1.711395, Train acc 0.328125 \n",
      "Batch 271, Loss: 1.770462, Train acc 0.265625 \n",
      "Batch 272, Loss: 1.569263, Train acc 0.328125 \n",
      "Batch 273, Loss: 1.608064, Train acc 0.343750 \n",
      "Batch 274, Loss: 1.944404, Train acc 0.296875 \n",
      "Batch 275, Loss: 2.140003, Train acc 0.156250 \n",
      "Batch 276, Loss: 1.965523, Train acc 0.234375 \n",
      "Batch 277, Loss: 2.331194, Train acc 0.156250 \n",
      "Batch 278, Loss: 2.419845, Train acc 0.109375 \n",
      "Batch 279, Loss: 2.086061, Train acc 0.250000 \n",
      "Batch 280, Loss: 1.971084, Train acc 0.203125 \n",
      "Batch 281, Loss: 1.835897, Train acc 0.359375 \n",
      "Batch 282, Loss: 1.648846, Train acc 0.375000 \n",
      "Batch 283, Loss: 2.010176, Train acc 0.140625 \n",
      "Batch 284, Loss: 2.442083, Train acc 0.062500 \n",
      "Batch 285, Loss: 1.957724, Train acc 0.203125 \n",
      "Batch 286, Loss: 1.885007, Train acc 0.203125 \n",
      "Batch 287, Loss: 1.748164, Train acc 0.265625 \n",
      "Batch 288, Loss: 1.670869, Train acc 0.281250 \n",
      "Batch 289, Loss: 1.658354, Train acc 0.406250 \n",
      "Batch 290, Loss: 1.524254, Train acc 0.328125 \n",
      "Batch 291, Loss: 2.218784, Train acc 0.078125 \n",
      "Batch 292, Loss: 2.317473, Train acc 0.140625 \n",
      "Batch 293, Loss: 1.998700, Train acc 0.203125 \n",
      "Batch 294, Loss: 1.774631, Train acc 0.375000 \n",
      "Batch 295, Loss: 1.691475, Train acc 0.312500 \n",
      "Batch 296, Loss: 1.692262, Train acc 0.359375 \n",
      "Batch 297, Loss: 1.658352, Train acc 0.296875 \n",
      "Batch 298, Loss: 1.614213, Train acc 0.421875 \n",
      "Batch 299, Loss: 1.665402, Train acc 0.203125 \n",
      "Batch 300, Loss: 2.056032, Train acc 0.234375 \n",
      "Batch 301, Loss: 1.824280, Train acc 0.203125 \n",
      "Batch 302, Loss: 1.758251, Train acc 0.296875 \n",
      "Batch 303, Loss: 1.643646, Train acc 0.296875 \n",
      "Batch 304, Loss: 1.671314, Train acc 0.250000 \n",
      "Batch 305, Loss: 1.462167, Train acc 0.406250 \n",
      "Batch 306, Loss: 1.563506, Train acc 0.343750 \n",
      "Batch 307, Loss: 1.464957, Train acc 0.390625 \n",
      "Batch 308, Loss: 1.599811, Train acc 0.328125 \n",
      "Batch 309, Loss: 1.450004, Train acc 0.406250 \n",
      "Batch 310, Loss: 1.453018, Train acc 0.437500 \n",
      "Batch 311, Loss: 1.727686, Train acc 0.296875 \n",
      "Batch 312, Loss: 1.780779, Train acc 0.296875 \n",
      "Batch 313, Loss: 1.538231, Train acc 0.484375 \n",
      "Batch 314, Loss: 1.511746, Train acc 0.359375 \n",
      "Batch 315, Loss: 1.354553, Train acc 0.500000 \n",
      "Batch 316, Loss: 1.041614, Train acc 0.546875 \n",
      "Batch 317, Loss: 1.817692, Train acc 0.421875 \n",
      "Batch 318, Loss: 1.796787, Train acc 0.406250 \n",
      "Batch 319, Loss: 1.753180, Train acc 0.406250 \n",
      "Batch 320, Loss: 1.651171, Train acc 0.515625 \n",
      "Batch 321, Loss: 1.501669, Train acc 0.375000 \n",
      "Batch 322, Loss: 1.754457, Train acc 0.312500 \n",
      "Batch 323, Loss: 1.862434, Train acc 0.343750 \n",
      "Batch 324, Loss: 1.866727, Train acc 0.234375 \n",
      "Batch 325, Loss: 1.608932, Train acc 0.328125 \n",
      "Batch 326, Loss: 1.384569, Train acc 0.375000 \n",
      "Batch 327, Loss: 1.231737, Train acc 0.500000 \n",
      "Batch 328, Loss: 1.546353, Train acc 0.437500 \n",
      "Batch 329, Loss: 2.112809, Train acc 0.234375 \n",
      "Batch 330, Loss: 1.894592, Train acc 0.296875 \n",
      "Batch 331, Loss: 1.835736, Train acc 0.234375 \n",
      "Batch 332, Loss: 1.673122, Train acc 0.328125 \n",
      "Batch 333, Loss: 1.505750, Train acc 0.406250 \n",
      "Batch 334, Loss: 1.366952, Train acc 0.484375 \n",
      "Batch 335, Loss: 1.181491, Train acc 0.531250 \n",
      "Batch 336, Loss: 1.255507, Train acc 0.390625 \n",
      "Batch 337, Loss: 1.317824, Train acc 0.406250 \n",
      "Batch 338, Loss: 1.488732, Train acc 0.281250 \n",
      "Batch 339, Loss: 1.352369, Train acc 0.500000 \n",
      "Batch 340, Loss: 1.256075, Train acc 0.484375 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 341, Loss: 1.161675, Train acc 0.531250 \n",
      "Batch 342, Loss: 1.177228, Train acc 0.421875 \n",
      "Batch 343, Loss: 1.522959, Train acc 0.390625 \n",
      "Batch 344, Loss: 1.284880, Train acc 0.453125 \n",
      "Batch 345, Loss: 1.101187, Train acc 0.578125 \n",
      "Batch 346, Loss: 1.422398, Train acc 0.468750 \n",
      "Batch 347, Loss: 2.260985, Train acc 0.281250 \n",
      "Batch 348, Loss: 1.850880, Train acc 0.234375 \n",
      "Batch 349, Loss: 1.735105, Train acc 0.375000 \n",
      "Batch 350, Loss: 1.713033, Train acc 0.296875 \n",
      "Batch 351, Loss: 1.590950, Train acc 0.390625 \n",
      "Batch 352, Loss: 1.633594, Train acc 0.359375 \n",
      "Batch 353, Loss: 1.540712, Train acc 0.468750 \n",
      "Batch 354, Loss: 1.462834, Train acc 0.375000 \n",
      "Batch 355, Loss: 1.405450, Train acc 0.437500 \n",
      "Batch 356, Loss: 1.429829, Train acc 0.437500 \n",
      "Batch 357, Loss: 1.885025, Train acc 0.250000 \n",
      "Batch 358, Loss: 2.481964, Train acc 0.046875 \n",
      "Batch 359, Loss: 2.035939, Train acc 0.406250 \n",
      "Batch 360, Loss: 1.857112, Train acc 0.296875 \n",
      "Batch 361, Loss: 1.659104, Train acc 0.328125 \n",
      "Batch 362, Loss: 1.425653, Train acc 0.468750 \n",
      "Batch 363, Loss: 1.490185, Train acc 0.312500 \n",
      "Batch 364, Loss: 1.338772, Train acc 0.484375 \n",
      "Batch 365, Loss: 1.085704, Train acc 0.609375 \n",
      "Batch 366, Loss: 1.657269, Train acc 0.328125 \n",
      "Batch 367, Loss: 1.401189, Train acc 0.468750 \n",
      "Batch 368, Loss: 1.384468, Train acc 0.328125 \n",
      "Batch 369, Loss: 1.218501, Train acc 0.515625 \n",
      "Batch 370, Loss: 1.223202, Train acc 0.390625 \n",
      "Batch 371, Loss: 1.339537, Train acc 0.312500 \n",
      "Batch 372, Loss: 1.203759, Train acc 0.531250 \n",
      "Batch 373, Loss: 1.043732, Train acc 0.546875 \n",
      "Batch 374, Loss: 1.380178, Train acc 0.406250 \n",
      "Batch 375, Loss: 1.420736, Train acc 0.484375 \n",
      "Batch 376, Loss: 1.240839, Train acc 0.453125 \n",
      "Batch 377, Loss: 1.001565, Train acc 0.578125 \n",
      "Batch 378, Loss: 1.163210, Train acc 0.437500 \n",
      "Batch 379, Loss: 1.406509, Train acc 0.406250 \n",
      "Batch 380, Loss: 1.403266, Train acc 0.390625 \n",
      "Batch 381, Loss: 0.896599, Train acc 0.562500 \n",
      "Batch 382, Loss: 1.032049, Train acc 0.484375 \n",
      "Batch 383, Loss: 1.355368, Train acc 0.390625 \n",
      "Batch 384, Loss: 1.289616, Train acc 0.421875 \n",
      "Batch 385, Loss: 1.319343, Train acc 0.453125 \n",
      "Batch 386, Loss: 1.081156, Train acc 0.609375 \n",
      "Batch 387, Loss: 1.143947, Train acc 0.625000 \n",
      "Batch 388, Loss: 1.012099, Train acc 0.625000 \n",
      "Batch 389, Loss: 0.955502, Train acc 0.593750 \n",
      "Batch 390, Loss: 0.971031, Train acc 0.593750 \n",
      "Batch 391, Loss: 1.324360, Train acc 0.406250 \n",
      "Batch 392, Loss: 1.092917, Train acc 0.515625 \n",
      "Batch 393, Loss: 1.036486, Train acc 0.546875 \n",
      "Batch 394, Loss: 1.046096, Train acc 0.578125 \n",
      "Batch 395, Loss: 1.032470, Train acc 0.625000 \n",
      "Batch 396, Loss: 0.988982, Train acc 0.578125 \n",
      "Batch 397, Loss: 1.222520, Train acc 0.484375 \n",
      "Batch 398, Loss: 1.180225, Train acc 0.437500 \n",
      "Batch 399, Loss: 1.142159, Train acc 0.484375 \n",
      "Batch 400, Loss: 1.020380, Train acc 0.515625 \n",
      "Batch 401, Loss: 1.006772, Train acc 0.562500 \n",
      "Batch 402, Loss: 1.017660, Train acc 0.578125 \n",
      "Batch 403, Loss: 0.931566, Train acc 0.515625 \n",
      "Batch 404, Loss: 1.136091, Train acc 0.593750 \n",
      "Batch 405, Loss: 1.303952, Train acc 0.531250 \n",
      "Batch 406, Loss: 1.162910, Train acc 0.546875 \n",
      "Batch 407, Loss: 0.949016, Train acc 0.578125 \n",
      "Batch 408, Loss: 1.087895, Train acc 0.453125 \n",
      "Batch 409, Loss: 1.146316, Train acc 0.562500 \n",
      "Batch 410, Loss: 0.838502, Train acc 0.671875 \n",
      "Batch 411, Loss: 1.001379, Train acc 0.546875 \n",
      "Batch 412, Loss: 1.465124, Train acc 0.484375 \n",
      "Batch 413, Loss: 1.384451, Train acc 0.421875 \n",
      "Batch 414, Loss: 1.072573, Train acc 0.531250 \n",
      "Batch 415, Loss: 1.135907, Train acc 0.468750 \n",
      "Batch 416, Loss: 0.905430, Train acc 0.718750 \n",
      "Batch 417, Loss: 0.951269, Train acc 0.593750 \n",
      "Batch 418, Loss: 1.023510, Train acc 0.593750 \n",
      "Batch 419, Loss: 0.927631, Train acc 0.656250 \n",
      "Batch 420, Loss: 1.002497, Train acc 0.578125 \n",
      "Batch 421, Loss: 1.045938, Train acc 0.578125 \n",
      "Batch 422, Loss: 1.009249, Train acc 0.578125 \n",
      "Batch 423, Loss: 0.808769, Train acc 0.656250 \n",
      "Batch 424, Loss: 1.036545, Train acc 0.593750 \n",
      "Batch 425, Loss: 0.957065, Train acc 0.625000 \n",
      "Batch 426, Loss: 1.202486, Train acc 0.562500 \n",
      "Batch 427, Loss: 1.105365, Train acc 0.578125 \n",
      "Batch 428, Loss: 0.935137, Train acc 0.656250 \n",
      "Batch 429, Loss: 0.984449, Train acc 0.578125 \n",
      "Batch 430, Loss: 1.044854, Train acc 0.578125 \n",
      "Batch 431, Loss: 0.967948, Train acc 0.578125 \n",
      "Batch 432, Loss: 0.825847, Train acc 0.671875 \n",
      "Batch 433, Loss: 0.848450, Train acc 0.703125 \n",
      "Batch 434, Loss: 0.782611, Train acc 0.656250 \n",
      "Batch 435, Loss: 0.999092, Train acc 0.687500 \n",
      "Batch 436, Loss: 1.051405, Train acc 0.625000 \n",
      "Batch 437, Loss: 0.836385, Train acc 0.687500 \n",
      "Batch 438, Loss: 0.788338, Train acc 0.625000 \n",
      "Batch 439, Loss: 1.295058, Train acc 0.531250 \n",
      "Batch 440, Loss: 0.889175, Train acc 0.656250 \n",
      "Batch 441, Loss: 0.872438, Train acc 0.703125 \n",
      "Batch 442, Loss: 0.645743, Train acc 0.765625 \n",
      "Batch 443, Loss: 0.754444, Train acc 0.656250 \n",
      "Batch 444, Loss: 1.223893, Train acc 0.546875 \n",
      "Batch 445, Loss: 1.232991, Train acc 0.500000 \n",
      "Batch 446, Loss: 1.005018, Train acc 0.703125 \n",
      "Batch 447, Loss: 0.853082, Train acc 0.703125 \n",
      "Batch 448, Loss: 0.958770, Train acc 0.687500 \n",
      "Batch 449, Loss: 0.716673, Train acc 0.671875 \n",
      "Batch 450, Loss: 0.771342, Train acc 0.671875 \n",
      "Batch 451, Loss: 1.147071, Train acc 0.546875 \n",
      "Batch 452, Loss: 1.343823, Train acc 0.515625 \n",
      "Batch 453, Loss: 0.881642, Train acc 0.718750 \n",
      "Batch 454, Loss: 0.865968, Train acc 0.625000 \n",
      "Batch 455, Loss: 0.789616, Train acc 0.671875 \n",
      "Batch 456, Loss: 0.854390, Train acc 0.671875 \n",
      "Batch 457, Loss: 0.983683, Train acc 0.593750 \n",
      "Batch 458, Loss: 1.073753, Train acc 0.625000 \n",
      "Batch 459, Loss: 0.901576, Train acc 0.671875 \n",
      "Batch 460, Loss: 0.784317, Train acc 0.703125 \n",
      "Batch 461, Loss: 0.729594, Train acc 0.718750 \n",
      "Batch 462, Loss: 0.992376, Train acc 0.609375 \n",
      "Batch 463, Loss: 0.719015, Train acc 0.765625 \n",
      "Batch 464, Loss: 0.850846, Train acc 0.734375 \n",
      "Batch 465, Loss: 1.090320, Train acc 0.578125 \n",
      "Batch 466, Loss: 0.713666, Train acc 0.750000 \n",
      "Batch 467, Loss: 0.759168, Train acc 0.718750 \n",
      "Batch 468, Loss: 0.863889, Train acc 0.609375 \n",
      "Batch 469, Loss: 0.806144, Train acc 0.687500 \n",
      "Batch 470, Loss: 0.700528, Train acc 0.656250 \n",
      "Batch 471, Loss: 0.979719, Train acc 0.546875 \n",
      "Batch 472, Loss: 0.987503, Train acc 0.593750 \n",
      "Batch 473, Loss: 0.938548, Train acc 0.609375 \n",
      "Batch 474, Loss: 1.127697, Train acc 0.531250 \n",
      "Batch 475, Loss: 0.911671, Train acc 0.578125 \n",
      "Batch 476, Loss: 0.764316, Train acc 0.640625 \n",
      "Batch 477, Loss: 0.967295, Train acc 0.609375 \n",
      "Batch 478, Loss: 1.153746, Train acc 0.531250 \n",
      "Batch 479, Loss: 1.135104, Train acc 0.578125 \n",
      "Batch 480, Loss: 1.096000, Train acc 0.531250 \n",
      "Batch 481, Loss: 0.900538, Train acc 0.718750 \n",
      "Batch 482, Loss: 0.764545, Train acc 0.750000 \n",
      "Batch 483, Loss: 0.948337, Train acc 0.656250 \n",
      "Batch 484, Loss: 1.024833, Train acc 0.609375 \n",
      "Batch 485, Loss: 0.882212, Train acc 0.671875 \n",
      "Batch 486, Loss: 0.944534, Train acc 0.656250 \n",
      "Batch 487, Loss: 0.747022, Train acc 0.718750 \n",
      "Batch 488, Loss: 0.826215, Train acc 0.671875 \n",
      "Batch 489, Loss: 0.759499, Train acc 0.718750 \n",
      "Batch 490, Loss: 0.819007, Train acc 0.734375 \n",
      "Batch 491, Loss: 1.113709, Train acc 0.562500 \n",
      "Batch 492, Loss: 1.189660, Train acc 0.578125 \n",
      "Batch 493, Loss: 1.385600, Train acc 0.578125 \n",
      "Batch 494, Loss: 1.844523, Train acc 0.421875 \n",
      "Batch 495, Loss: 1.582558, Train acc 0.453125 \n",
      "Batch 496, Loss: 0.968213, Train acc 0.609375 \n",
      "Batch 497, Loss: 0.991581, Train acc 0.671875 \n",
      "Batch 498, Loss: 0.779523, Train acc 0.781250 \n",
      "Batch 499, Loss: 0.710998, Train acc 0.718750 \n",
      "Batch 500, Loss: 1.241719, Train acc 0.562500 \n",
      "Batch 501, Loss: 1.324417, Train acc 0.531250 \n",
      "Batch 502, Loss: 0.727048, Train acc 0.781250 \n",
      "Batch 503, Loss: 0.748762, Train acc 0.781250 \n",
      "Batch 504, Loss: 0.651712, Train acc 0.734375 \n",
      "Batch 505, Loss: 0.996795, Train acc 0.546875 \n",
      "Batch 506, Loss: 1.055570, Train acc 0.562500 \n",
      "Batch 507, Loss: 0.825957, Train acc 0.734375 \n",
      "Batch 508, Loss: 0.757405, Train acc 0.734375 \n",
      "Batch 509, Loss: 0.836131, Train acc 0.671875 \n",
      "Batch 510, Loss: 0.889737, Train acc 0.625000 \n",
      "Batch 511, Loss: 0.689778, Train acc 0.765625 \n",
      "Batch 512, Loss: 0.706275, Train acc 0.750000 \n",
      "Batch 513, Loss: 0.745814, Train acc 0.734375 \n",
      "Batch 514, Loss: 0.860156, Train acc 0.671875 \n",
      "Batch 515, Loss: 0.788008, Train acc 0.671875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 516, Loss: 0.711783, Train acc 0.750000 \n",
      "Batch 517, Loss: 0.794502, Train acc 0.750000 \n",
      "Batch 518, Loss: 0.864151, Train acc 0.609375 \n",
      "Batch 519, Loss: 0.903185, Train acc 0.562500 \n",
      "Batch 520, Loss: 0.798798, Train acc 0.703125 \n",
      "Batch 521, Loss: 0.816985, Train acc 0.656250 \n",
      "Batch 522, Loss: 0.942076, Train acc 0.593750 \n",
      "Batch 523, Loss: 0.918943, Train acc 0.625000 \n",
      "Batch 524, Loss: 0.922899, Train acc 0.718750 \n",
      "Batch 525, Loss: 0.972572, Train acc 0.593750 \n",
      "Batch 526, Loss: 1.076686, Train acc 0.546875 \n",
      "Batch 527, Loss: 0.606504, Train acc 0.781250 \n",
      "Batch 528, Loss: 0.872513, Train acc 0.671875 \n",
      "Batch 529, Loss: 0.859680, Train acc 0.640625 \n",
      "Batch 530, Loss: 0.682548, Train acc 0.718750 \n",
      "Batch 531, Loss: 0.659631, Train acc 0.765625 \n",
      "Batch 532, Loss: 0.634728, Train acc 0.734375 \n",
      "Batch 533, Loss: 0.907780, Train acc 0.625000 \n",
      "Batch 534, Loss: 0.871456, Train acc 0.671875 \n",
      "Batch 535, Loss: 0.650903, Train acc 0.703125 \n",
      "Batch 536, Loss: 0.743913, Train acc 0.750000 \n",
      "Batch 537, Loss: 0.693976, Train acc 0.750000 \n",
      "Batch 538, Loss: 0.947724, Train acc 0.640625 \n",
      "Batch 539, Loss: 0.897462, Train acc 0.640625 \n",
      "Batch 540, Loss: 0.708730, Train acc 0.703125 \n",
      "Batch 541, Loss: 0.810093, Train acc 0.671875 \n",
      "Batch 542, Loss: 0.611440, Train acc 0.765625 \n",
      "Batch 543, Loss: 0.659137, Train acc 0.734375 \n",
      "Batch 544, Loss: 0.685366, Train acc 0.687500 \n",
      "Batch 545, Loss: 0.671285, Train acc 0.718750 \n",
      "Batch 546, Loss: 0.862729, Train acc 0.671875 \n",
      "Batch 547, Loss: 0.580314, Train acc 0.750000 \n",
      "Batch 548, Loss: 0.826507, Train acc 0.640625 \n",
      "Batch 549, Loss: 0.636487, Train acc 0.796875 \n",
      "Batch 550, Loss: 0.573234, Train acc 0.781250 \n",
      "Batch 551, Loss: 0.814098, Train acc 0.734375 \n",
      "Batch 552, Loss: 1.075953, Train acc 0.656250 \n",
      "Batch 553, Loss: 1.209302, Train acc 0.546875 \n",
      "Batch 554, Loss: 0.827988, Train acc 0.671875 \n",
      "Batch 555, Loss: 0.949159, Train acc 0.578125 \n",
      "Batch 556, Loss: 1.080068, Train acc 0.609375 \n",
      "Batch 557, Loss: 0.567997, Train acc 0.812500 \n",
      "Batch 558, Loss: 0.538572, Train acc 0.765625 \n",
      "Batch 559, Loss: 0.819900, Train acc 0.734375 \n",
      "Batch 560, Loss: 0.754589, Train acc 0.718750 \n",
      "Batch 561, Loss: 0.699581, Train acc 0.671875 \n",
      "Batch 562, Loss: 0.691231, Train acc 0.796875 \n",
      "Batch 563, Loss: 0.918388, Train acc 0.703125 \n",
      "Batch 564, Loss: 0.817102, Train acc 0.687500 \n",
      "Batch 565, Loss: 0.765180, Train acc 0.656250 \n",
      "Batch 566, Loss: 0.984461, Train acc 0.593750 \n",
      "Batch 567, Loss: 0.785696, Train acc 0.734375 \n",
      "Batch 568, Loss: 0.785673, Train acc 0.718750 \n",
      "Batch 569, Loss: 0.653837, Train acc 0.718750 \n",
      "Batch 570, Loss: 0.744814, Train acc 0.703125 \n",
      "Batch 571, Loss: 0.847476, Train acc 0.765625 \n",
      "Batch 572, Loss: 0.824870, Train acc 0.671875 \n",
      "Batch 573, Loss: 0.617454, Train acc 0.765625 \n",
      "Batch 574, Loss: 0.741064, Train acc 0.734375 \n",
      "Batch 575, Loss: 0.717875, Train acc 0.750000 \n",
      "Batch 576, Loss: 0.584096, Train acc 0.828125 \n",
      "Batch 577, Loss: 0.744978, Train acc 0.734375 \n",
      "Batch 578, Loss: 0.606784, Train acc 0.812500 \n",
      "Batch 579, Loss: 0.788331, Train acc 0.687500 \n",
      "Batch 580, Loss: 0.831283, Train acc 0.750000 \n",
      "Batch 581, Loss: 0.653702, Train acc 0.734375 \n",
      "Batch 582, Loss: 0.983610, Train acc 0.640625 \n",
      "Batch 583, Loss: 0.584048, Train acc 0.781250 \n",
      "Batch 584, Loss: 0.841568, Train acc 0.640625 \n",
      "Batch 585, Loss: 0.745656, Train acc 0.750000 \n",
      "Batch 586, Loss: 0.633161, Train acc 0.781250 \n",
      "Batch 587, Loss: 0.637753, Train acc 0.765625 \n",
      "Batch 588, Loss: 0.756561, Train acc 0.718750 \n",
      "Batch 589, Loss: 0.546666, Train acc 0.734375 \n",
      "Batch 590, Loss: 0.665408, Train acc 0.781250 \n",
      "Batch 591, Loss: 0.940684, Train acc 0.625000 \n",
      "Batch 592, Loss: 0.654882, Train acc 0.765625 \n",
      "Batch 593, Loss: 0.595558, Train acc 0.750000 \n",
      "Batch 594, Loss: 0.755766, Train acc 0.765625 \n",
      "Batch 595, Loss: 0.617756, Train acc 0.750000 \n",
      "Batch 596, Loss: 0.756407, Train acc 0.671875 \n",
      "Batch 597, Loss: 0.539487, Train acc 0.796875 \n",
      "Batch 598, Loss: 0.593340, Train acc 0.843750 \n",
      "Batch 599, Loss: 0.651885, Train acc 0.734375 \n",
      "Batch 600, Loss: 0.696307, Train acc 0.687500 \n",
      "Batch 601, Loss: 0.571683, Train acc 0.765625 \n",
      "Batch 602, Loss: 0.728474, Train acc 0.718750 \n",
      "Batch 603, Loss: 0.738244, Train acc 0.718750 \n",
      "Batch 604, Loss: 0.897058, Train acc 0.593750 \n",
      "Batch 605, Loss: 0.811055, Train acc 0.687500 \n",
      "Batch 606, Loss: 0.896912, Train acc 0.671875 \n",
      "Batch 607, Loss: 0.791444, Train acc 0.718750 \n",
      "Batch 608, Loss: 0.707767, Train acc 0.718750 \n",
      "Batch 609, Loss: 0.630868, Train acc 0.781250 \n",
      "Batch 610, Loss: 0.764041, Train acc 0.640625 \n",
      "Batch 611, Loss: 0.644101, Train acc 0.765625 \n",
      "Batch 612, Loss: 0.600494, Train acc 0.734375 \n",
      "Batch 613, Loss: 0.580745, Train acc 0.812500 \n",
      "Batch 614, Loss: 0.379185, Train acc 0.890625 \n",
      "Batch 615, Loss: 0.651079, Train acc 0.687500 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-695ab8ba2b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch %d, Loss: %f, Train acc %f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 904\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    905\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1136\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1137\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1354\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1339\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "max_steps = 1000\n",
    "batch_size = 64\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "resize_input = tf.image.resize_images(input_placeholder, [96, 96])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "logits = GoogleNet(resize_input, num_outputs)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width, 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label, is_training: True}\n",
    "    loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)\n",
    "    print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "        \n",
    "for i in range(100):\n",
    "    test_data, test_label = test_dataset.next_batch(100)\n",
    "    test_data = np.reshape(test_data, (100, height, width, num_channels))\n",
    "    test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_data, gt_placeholder: test_label, is_training: False})\n",
    "    test_acc.append(test_acc_)\n",
    "print (\"Test Loss: %f, Test acc %f \" % (np.mean(test_loss_), np.mean(test_acc_)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
