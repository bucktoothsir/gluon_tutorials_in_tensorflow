{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet：深度残差网络\n",
    "当大家还在惊叹GoogLeNet用结构化的连接纳入了大量卷积层的时候，微软亚洲研究院的研究员已经在设计更深但结构更简单的网络ResNet。他们凭借这个网络在2015年的Imagenet竞赛中大获全胜。\n",
    "\n",
    "ResNet有效的解决了深度卷积神经网络难训练的问题。这是因为在误差反传的过程中，梯度通常变得越来越小，从而权重的更新量也变小。这个导致远离损失函数的层训练缓慢，随着层数的增加这个现象更加明显。之前有两种常用方案来尝试解决这个问题：\n",
    "\n",
    "按层训练。先训练靠近数据的层，然后慢慢的增加后面的层。但效果不是特别好，而且比较麻烦。\n",
    "使用更宽的层（增加输出通道）而不是更深来增加模型复杂度。但更宽的模型经常不如更深的效果好。\n",
    "ResNet通过增加跨层的连接来解决梯度逐层回传时变小的问题。虽然这个想法之前就提出过了，但ResNet真正的把效果做好了。\n",
    "\n",
    "下图演示了一个跨层的连接。\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/residual.svg)\n",
    "\n",
    "最底下那层的输入不仅仅是输出给了中间层，而且其与中间层结果相加进入最上层。这样在梯度反传时，最上层梯度可以直接跳过中间层传到最下层，从而避免最下层梯度过小情况。\n",
    "\n",
    "为什么叫做残差网络呢？我们可以将上面示意图里的结构拆成两个网络的和，一个一层，一个两层，最下面层是共享的。\n",
    "\n",
    "\n",
    "在训练过程中，左边的网络因为更简单所以更容易训练。这个小网络没有拟合到的部分，或者说残差，则被右边的网络抓取住。所以直观上来说，即使加深网络，跨层连接仍然可以使得底层网络可以充分的训练，从而不会让训练更难。\n",
    "![image.png](http://zh.gluon.ai/_images/residual2.svg)\n",
    "\n",
    "### Residual块\n",
    "ResNet沿用了VGG的那种全用$3×3$卷积，但在卷积和池化层之间加入了批量归一层来加速训练。每次跨层连接跨过两层卷积。这里我们定义一个这样的残差块。注意到如果输入的通道数和输出不一样时（`same_shape=False`），我们使用一个额外的$1×1$卷积来做通道变化，同时使用`strides=2`来把长宽减半。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "def Residual(input, num_channels, same_shape=True, scope='residual',is_training=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        # path 1\n",
    "        stride = 1 if same_shape else 2\n",
    "        conv1_1 = slim.conv2d(input, num_channels, [3, 3], stride, scope='conv1_1', activation_fn=None)\n",
    "        bn1_1 = slim.batch_norm(conv1_1, is_training=is_training)\n",
    "        relu1_1 = tf.nn.relu(bn1_1)\n",
    "        conv1_2 = slim.conv2d(relu1_1, num_channels, [3, 3], stride=1, scope='conv1_2', activation_fn=None)\n",
    "        bn1_2 = slim.batch_norm(conv1_2, is_training=is_training)\n",
    "\n",
    "        # path 2\n",
    "        if not same_shape:\n",
    "            conv2_1 = slim.conv2d(input, num_channels, [1, 1], stride, scope='conv2_1', activation_fn=None)\n",
    "        else:\n",
    "            conv2_1 = input\n",
    "        \n",
    "        return tf.nn.relu(conv2_1+bn1_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入输出通道相同：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"test1/Relu_1:0\", shape=(4, 6, 6, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.uniform(size=(4, 6, 6, 3)).astype(np.float32)\n",
    "print Residual(x, 3, scope='test1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入输出通道不同：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"test2/Relu_1:0\", shape=(4, 3, 3, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print Residual(x, 8, same_shape=False, scope='test2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建ResNet\n",
    "类似GoogLeNet主体是由Inception块串联而成，ResNet的主体部分串联多个Residual块。下面我们定义18层的ResNet。另外注意到一点是，这里我们没用池化层来减小数据长宽，而是通过有通道变化的Residual块里面的使用`strides=2`的卷积层。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet(input, num_classes, scope='resnet', is_training=False):\n",
    "    with tf.variable_scope(scope):\n",
    "        # block 1\n",
    "        b1 = slim.conv2d(input, 64, [7, 7], 2, scope='block1')\n",
    "        print b1\n",
    "        \n",
    "        # block 2\n",
    "        b2_pool = slim.max_pool2d(b1, [3, 3], 2, scope='block2_pool')\n",
    "        b2_res_1 = Residual(b2_pool, 64, is_training=is_training, scope='b2_res_1')\n",
    "        b2_res_2 = Residual(b2_res_1, 64, is_training=is_training, scope='b2_res_2')\n",
    "        print b2_res_2\n",
    "        \n",
    "        # block 3\n",
    "        b3_res_1 = Residual(b2_res_2, 128, same_shape=False, is_training=is_training, scope='b3_res_1')\n",
    "        b3_res_2 = Residual(b3_res_1, 128, is_training=is_training, scope='b3_res_2')\n",
    "        print b3_res_2\n",
    "        \n",
    "        \n",
    "        # block 4\n",
    "        b4_res_1 = Residual(b3_res_2, 256, same_shape=False, is_training=is_training, scope='b4_res_1')\n",
    "        b4_res_2 = Residual(b4_res_1, 256, is_training=is_training, scope='b4_res_2')\n",
    "        print b4_res_2\n",
    "        \n",
    "        # block 5\n",
    "        b5_res_1 = Residual(b4_res_2, 512, same_shape=False, is_training=is_training, scope='b5_res_1')\n",
    "        b5_res_2 = Residual(b5_res_1, 512, is_training=is_training, scope='b5_res_2')\n",
    "        print b5_res_2\n",
    "        \n",
    "        # block 6\n",
    "        b6_pool = slim.avg_pool2d(b5_res_2, [3, 3], scope='b6_avg_pool')\n",
    "        return tf.squeeze(slim.fully_connected(b6_pool, num_classes, scope='fc', activation_fn = None))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里演示数据在块之间的形状变化：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"resnet/block1/Relu:0\", shape=(4, 48, 48, 64), dtype=float32)\n",
      "Tensor(\"resnet/b2_res_2/Relu_1:0\", shape=(4, 23, 23, 64), dtype=float32)\n",
      "Tensor(\"resnet/b3_res_2/Relu_1:0\", shape=(4, 12, 12, 128), dtype=float32)\n",
      "Tensor(\"resnet/b4_res_2/Relu_1:0\", shape=(4, 6, 6, 256), dtype=float32)\n",
      "Tensor(\"resnet/b5_res_2/Relu_1:0\", shape=(4, 3, 3, 512), dtype=float32)\n",
      "(4, 10)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(size=(4, 96, 96, 3)).astype(np.float32)\n",
    "y = ResNet(x, 10)\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据并训练\n",
    "跟前面类似，但因为有批量归一化，所以使用了较大的学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print test_images.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)\n",
    "test_dataset = DataSet(test_images, test_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"resnet/block1/Relu:0\", shape=(?, 48, 48, 64), dtype=float32)\n",
      "Tensor(\"resnet/b2_res_2/Relu_1:0\", shape=(?, 23, 23, 64), dtype=float32)\n",
      "Tensor(\"resnet/b3_res_2/Relu_1:0\", shape=(?, 12, 12, 128), dtype=float32)\n",
      "Tensor(\"resnet/b4_res_2/Relu_1:0\", shape=(?, 6, 6, 256), dtype=float32)\n",
      "Tensor(\"resnet/b5_res_2/Relu_1:0\", shape=(?, 3, 3, 512), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Batch 0, Loss: 2.305954, Train acc 0.078125 \n",
      "Batch 1, Loss: 2.291968, Train acc 0.109375 \n",
      "Batch 2, Loss: 2.282601, Train acc 0.078125 \n",
      "Batch 3, Loss: 2.253130, Train acc 0.125000 \n",
      "Batch 4, Loss: 2.269571, Train acc 0.125000 \n",
      "Batch 5, Loss: 2.247785, Train acc 0.187500 \n",
      "Batch 6, Loss: 2.306907, Train acc 0.156250 \n",
      "Batch 7, Loss: 2.293811, Train acc 0.093750 \n",
      "Batch 8, Loss: 2.261243, Train acc 0.078125 \n",
      "Batch 9, Loss: 2.236231, Train acc 0.296875 \n",
      "Batch 10, Loss: 2.208480, Train acc 0.140625 \n",
      "Batch 11, Loss: 2.159352, Train acc 0.296875 \n",
      "Batch 12, Loss: 2.113523, Train acc 0.140625 \n",
      "Batch 13, Loss: 2.200069, Train acc 0.281250 \n",
      "Batch 14, Loss: 2.250731, Train acc 0.265625 \n",
      "Batch 15, Loss: 2.200623, Train acc 0.343750 \n",
      "Batch 16, Loss: 2.077721, Train acc 0.218750 \n",
      "Batch 17, Loss: 2.115963, Train acc 0.093750 \n",
      "Batch 18, Loss: 2.271673, Train acc 0.375000 \n",
      "Batch 19, Loss: 2.208041, Train acc 0.218750 \n",
      "Batch 20, Loss: 2.073440, Train acc 0.265625 \n",
      "Batch 21, Loss: 2.072031, Train acc 0.140625 \n",
      "Batch 22, Loss: 2.086918, Train acc 0.328125 \n",
      "Batch 23, Loss: 2.132628, Train acc 0.250000 \n",
      "Batch 24, Loss: 1.877944, Train acc 0.250000 \n",
      "Batch 25, Loss: 2.135388, Train acc 0.375000 \n",
      "Batch 26, Loss: 2.030756, Train acc 0.484375 \n",
      "Batch 27, Loss: 1.603161, Train acc 0.421875 \n",
      "Batch 28, Loss: 3.624118, Train acc 0.187500 \n",
      "Batch 29, Loss: 2.586297, Train acc 0.187500 \n",
      "Batch 30, Loss: 2.334799, Train acc 0.078125 \n",
      "Batch 31, Loss: 2.331974, Train acc 0.109375 \n",
      "Batch 32, Loss: 2.315196, Train acc 0.109375 \n",
      "Batch 33, Loss: 2.294796, Train acc 0.125000 \n",
      "Batch 34, Loss: 2.288802, Train acc 0.109375 \n",
      "Batch 35, Loss: 2.314779, Train acc 0.156250 \n",
      "Batch 36, Loss: 2.315517, Train acc 0.078125 \n",
      "Batch 37, Loss: 2.301295, Train acc 0.062500 \n",
      "Batch 38, Loss: 2.297508, Train acc 0.078125 \n",
      "Batch 39, Loss: 2.288738, Train acc 0.187500 \n",
      "Batch 40, Loss: 2.319732, Train acc 0.093750 \n",
      "Batch 41, Loss: 2.306086, Train acc 0.109375 \n",
      "Batch 42, Loss: 2.298297, Train acc 0.093750 \n",
      "Batch 43, Loss: 2.301572, Train acc 0.078125 \n",
      "Batch 44, Loss: 2.311475, Train acc 0.046875 \n",
      "Batch 45, Loss: 2.309442, Train acc 0.125000 \n",
      "Batch 46, Loss: 2.306365, Train acc 0.187500 \n",
      "Batch 47, Loss: 2.297334, Train acc 0.093750 \n",
      "Batch 48, Loss: 2.308869, Train acc 0.062500 \n",
      "Batch 49, Loss: 2.320540, Train acc 0.046875 \n",
      "Batch 50, Loss: 2.303451, Train acc 0.125000 \n",
      "Batch 51, Loss: 2.296649, Train acc 0.125000 \n",
      "Batch 52, Loss: 2.296411, Train acc 0.093750 \n",
      "Batch 53, Loss: 2.304736, Train acc 0.078125 \n",
      "Batch 54, Loss: 2.300948, Train acc 0.093750 \n",
      "Batch 55, Loss: 2.300408, Train acc 0.109375 \n",
      "Batch 56, Loss: 2.305263, Train acc 0.062500 \n",
      "Batch 57, Loss: 2.312528, Train acc 0.031250 \n",
      "Batch 58, Loss: 2.298046, Train acc 0.140625 \n",
      "Batch 59, Loss: 2.299988, Train acc 0.125000 \n",
      "Batch 60, Loss: 2.296527, Train acc 0.140625 \n",
      "Batch 61, Loss: 2.308625, Train acc 0.031250 \n",
      "Batch 62, Loss: 2.300481, Train acc 0.078125 \n",
      "Batch 63, Loss: 2.293072, Train acc 0.093750 \n",
      "Batch 64, Loss: 2.283122, Train acc 0.187500 \n",
      "Batch 65, Loss: 2.318875, Train acc 0.078125 \n",
      "Batch 66, Loss: 2.293484, Train acc 0.109375 \n",
      "Batch 67, Loss: 2.307707, Train acc 0.109375 \n",
      "Batch 68, Loss: 2.295023, Train acc 0.140625 \n",
      "Batch 69, Loss: 2.296821, Train acc 0.093750 \n",
      "Batch 70, Loss: 2.297369, Train acc 0.062500 \n",
      "Batch 71, Loss: 2.297388, Train acc 0.156250 \n",
      "Batch 72, Loss: 2.292933, Train acc 0.093750 \n",
      "Batch 73, Loss: 2.300509, Train acc 0.031250 \n",
      "Batch 74, Loss: 2.285196, Train acc 0.093750 \n",
      "Batch 75, Loss: 2.272813, Train acc 0.171875 \n",
      "Batch 76, Loss: 2.315697, Train acc 0.078125 \n",
      "Batch 77, Loss: 2.288377, Train acc 0.093750 \n",
      "Batch 78, Loss: 2.289462, Train acc 0.125000 \n",
      "Batch 79, Loss: 2.294662, Train acc 0.125000 \n",
      "Batch 80, Loss: 2.287084, Train acc 0.093750 \n",
      "Batch 81, Loss: 2.286078, Train acc 0.234375 \n",
      "Batch 82, Loss: 2.280073, Train acc 0.171875 \n",
      "Batch 83, Loss: 2.273041, Train acc 0.125000 \n",
      "Batch 84, Loss: 2.281805, Train acc 0.062500 \n",
      "Batch 85, Loss: 2.280762, Train acc 0.125000 \n",
      "Batch 86, Loss: 2.292792, Train acc 0.109375 \n",
      "Batch 87, Loss: 2.280208, Train acc 0.234375 \n",
      "Batch 88, Loss: 2.279294, Train acc 0.125000 \n",
      "Batch 89, Loss: 2.267267, Train acc 0.125000 \n",
      "Batch 90, Loss: 2.270200, Train acc 0.093750 \n",
      "Batch 91, Loss: 2.255968, Train acc 0.109375 \n",
      "Batch 92, Loss: 2.259073, Train acc 0.125000 \n",
      "Batch 93, Loss: 2.276111, Train acc 0.140625 \n",
      "Batch 94, Loss: 2.244811, Train acc 0.156250 \n",
      "Batch 95, Loss: 2.275165, Train acc 0.109375 \n",
      "Batch 96, Loss: 2.268539, Train acc 0.125000 \n",
      "Batch 97, Loss: 2.236073, Train acc 0.140625 \n",
      "Batch 98, Loss: 2.289701, Train acc 0.031250 \n",
      "Batch 99, Loss: 2.285586, Train acc 0.109375 \n",
      "Batch 100, Loss: 2.297784, Train acc 0.062500 \n",
      "Batch 101, Loss: 2.256907, Train acc 0.296875 \n",
      "Batch 102, Loss: 2.232891, Train acc 0.140625 \n",
      "Batch 103, Loss: 2.218764, Train acc 0.187500 \n",
      "Batch 104, Loss: 2.228630, Train acc 0.203125 \n",
      "Batch 105, Loss: 2.200136, Train acc 0.250000 \n",
      "Batch 106, Loss: 2.466807, Train acc 0.156250 \n",
      "Batch 107, Loss: 2.303032, Train acc 0.218750 \n",
      "Batch 108, Loss: 2.295933, Train acc 0.093750 \n",
      "Batch 109, Loss: 2.293633, Train acc 0.109375 \n",
      "Batch 110, Loss: 2.303192, Train acc 0.125000 \n",
      "Batch 111, Loss: 2.309369, Train acc 0.046875 \n",
      "Batch 112, Loss: 2.298666, Train acc 0.078125 \n",
      "Batch 113, Loss: 2.289744, Train acc 0.109375 \n",
      "Batch 114, Loss: 2.275310, Train acc 0.187500 \n",
      "Batch 115, Loss: 2.273295, Train acc 0.171875 \n",
      "Batch 116, Loss: 2.279951, Train acc 0.187500 \n",
      "Batch 117, Loss: 2.249354, Train acc 0.218750 \n",
      "Batch 118, Loss: 2.328637, Train acc 0.187500 \n",
      "Batch 119, Loss: 2.281932, Train acc 0.109375 \n",
      "Batch 120, Loss: 2.276839, Train acc 0.156250 \n",
      "Batch 121, Loss: 2.251893, Train acc 0.156250 \n",
      "Batch 122, Loss: 2.256718, Train acc 0.171875 \n",
      "Batch 123, Loss: 2.197003, Train acc 0.515625 \n",
      "Batch 124, Loss: 2.216787, Train acc 0.328125 \n",
      "Batch 125, Loss: 2.154241, Train acc 0.343750 \n",
      "Batch 126, Loss: 2.123318, Train acc 0.218750 \n",
      "Batch 127, Loss: 2.152333, Train acc 0.218750 \n",
      "Batch 128, Loss: 2.186910, Train acc 0.093750 \n",
      "Batch 129, Loss: 2.113022, Train acc 0.171875 \n",
      "Batch 130, Loss: 2.082402, Train acc 0.250000 \n",
      "Batch 131, Loss: 2.107223, Train acc 0.140625 \n",
      "Batch 132, Loss: 2.054479, Train acc 0.250000 \n",
      "Batch 133, Loss: 1.846367, Train acc 0.218750 \n",
      "Batch 134, Loss: 1.998874, Train acc 0.265625 \n",
      "Batch 135, Loss: 3.872957, Train acc 0.156250 \n",
      "Batch 136, Loss: 7.974191, Train acc 0.046875 \n",
      "Batch 137, Loss: 2.321653, Train acc 0.156250 \n",
      "Batch 138, Loss: 2.316740, Train acc 0.109375 \n",
      "Batch 139, Loss: 2.314302, Train acc 0.109375 \n",
      "Batch 140, Loss: 2.296882, Train acc 0.109375 \n",
      "Batch 141, Loss: 2.318913, Train acc 0.062500 \n",
      "Batch 142, Loss: 2.321376, Train acc 0.046875 \n",
      "Batch 143, Loss: 2.297128, Train acc 0.062500 \n",
      "Batch 144, Loss: 2.299742, Train acc 0.156250 \n",
      "Batch 145, Loss: 2.287694, Train acc 0.093750 \n",
      "Batch 146, Loss: 2.299805, Train acc 0.125000 \n",
      "Batch 147, Loss: 2.299521, Train acc 0.109375 \n",
      "Batch 148, Loss: 2.301315, Train acc 0.109375 \n",
      "Batch 149, Loss: 2.298990, Train acc 0.078125 \n",
      "Batch 150, Loss: 2.298905, Train acc 0.156250 \n",
      "Batch 151, Loss: 2.300123, Train acc 0.078125 \n",
      "Batch 152, Loss: 2.306776, Train acc 0.156250 \n",
      "Batch 153, Loss: 2.298119, Train acc 0.187500 \n",
      "Batch 154, Loss: 2.317381, Train acc 0.062500 \n",
      "Batch 155, Loss: 2.317002, Train acc 0.078125 \n",
      "Batch 156, Loss: 2.295171, Train acc 0.156250 \n",
      "Batch 157, Loss: 2.327096, Train acc 0.031250 \n",
      "Batch 158, Loss: 2.300880, Train acc 0.109375 \n",
      "Batch 159, Loss: 2.308802, Train acc 0.031250 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 160, Loss: 2.308737, Train acc 0.093750 \n",
      "Batch 161, Loss: 2.302434, Train acc 0.109375 \n",
      "Batch 162, Loss: 2.304134, Train acc 0.093750 \n",
      "Batch 163, Loss: 2.292995, Train acc 0.125000 \n",
      "Batch 164, Loss: 2.300699, Train acc 0.187500 \n",
      "Batch 165, Loss: 2.290703, Train acc 0.187500 \n",
      "Batch 166, Loss: 2.291524, Train acc 0.156250 \n",
      "Batch 167, Loss: 2.312494, Train acc 0.093750 \n",
      "Batch 168, Loss: 2.285382, Train acc 0.156250 \n",
      "Batch 169, Loss: 2.311693, Train acc 0.062500 \n",
      "Batch 170, Loss: 2.310385, Train acc 0.078125 \n",
      "Batch 171, Loss: 2.299496, Train acc 0.093750 \n",
      "Batch 172, Loss: 2.291495, Train acc 0.125000 \n",
      "Batch 173, Loss: 2.310758, Train acc 0.078125 \n",
      "Batch 174, Loss: 2.306971, Train acc 0.062500 \n",
      "Batch 175, Loss: 2.308156, Train acc 0.078125 \n",
      "Batch 176, Loss: 2.298414, Train acc 0.171875 \n",
      "Batch 177, Loss: 2.308212, Train acc 0.109375 \n",
      "Batch 178, Loss: 2.317395, Train acc 0.046875 \n",
      "Batch 179, Loss: 2.307318, Train acc 0.046875 \n",
      "Batch 180, Loss: 2.287111, Train acc 0.171875 \n",
      "Batch 181, Loss: 2.310297, Train acc 0.078125 \n",
      "Batch 182, Loss: 2.295165, Train acc 0.140625 \n",
      "Batch 183, Loss: 2.300599, Train acc 0.093750 \n",
      "Batch 184, Loss: 2.301235, Train acc 0.109375 \n",
      "Batch 185, Loss: 2.298225, Train acc 0.140625 \n",
      "Batch 186, Loss: 2.302785, Train acc 0.046875 \n",
      "Batch 187, Loss: 2.290543, Train acc 0.109375 \n",
      "Batch 188, Loss: 2.310164, Train acc 0.078125 \n",
      "Batch 189, Loss: 2.302851, Train acc 0.093750 \n",
      "Batch 190, Loss: 2.297988, Train acc 0.156250 \n",
      "Batch 191, Loss: 2.308648, Train acc 0.031250 \n",
      "Batch 192, Loss: 2.304314, Train acc 0.078125 \n",
      "Batch 193, Loss: 2.292873, Train acc 0.156250 \n",
      "Batch 194, Loss: 2.298724, Train acc 0.078125 \n",
      "Batch 195, Loss: 2.296289, Train acc 0.062500 \n",
      "Batch 196, Loss: 2.293552, Train acc 0.140625 \n",
      "Batch 197, Loss: 2.304762, Train acc 0.046875 \n",
      "Batch 198, Loss: 2.299660, Train acc 0.093750 \n",
      "Batch 199, Loss: 2.297170, Train acc 0.203125 \n",
      "Batch 200, Loss: 2.302414, Train acc 0.093750 \n",
      "Batch 201, Loss: 2.288958, Train acc 0.140625 \n",
      "Batch 202, Loss: 2.299966, Train acc 0.062500 \n",
      "Batch 203, Loss: 2.288652, Train acc 0.171875 \n",
      "Batch 204, Loss: 2.325801, Train acc 0.031250 \n",
      "Batch 205, Loss: 2.294792, Train acc 0.125000 \n",
      "Batch 206, Loss: 2.292170, Train acc 0.171875 \n",
      "Batch 207, Loss: 2.299629, Train acc 0.078125 \n",
      "Batch 208, Loss: 2.294218, Train acc 0.125000 \n",
      "Batch 209, Loss: 2.287689, Train acc 0.125000 \n",
      "Batch 210, Loss: 2.295649, Train acc 0.125000 \n",
      "Batch 211, Loss: 2.281875, Train acc 0.187500 \n",
      "Batch 212, Loss: 2.301763, Train acc 0.078125 \n",
      "Batch 213, Loss: 2.307600, Train acc 0.093750 \n",
      "Batch 214, Loss: 2.305358, Train acc 0.078125 \n",
      "Batch 215, Loss: 2.302833, Train acc 0.078125 \n",
      "Batch 216, Loss: 2.286267, Train acc 0.109375 \n",
      "Batch 217, Loss: 2.287801, Train acc 0.156250 \n",
      "Batch 218, Loss: 2.292695, Train acc 0.093750 \n",
      "Batch 219, Loss: 2.290414, Train acc 0.093750 \n",
      "Batch 220, Loss: 2.283528, Train acc 0.109375 \n",
      "Batch 221, Loss: 2.301467, Train acc 0.187500 \n",
      "Batch 222, Loss: 2.304995, Train acc 0.062500 \n",
      "Batch 223, Loss: 2.290453, Train acc 0.203125 \n",
      "Batch 224, Loss: 2.291757, Train acc 0.109375 \n",
      "Batch 225, Loss: 2.307030, Train acc 0.046875 \n",
      "Batch 226, Loss: 2.290913, Train acc 0.093750 \n",
      "Batch 227, Loss: 2.290380, Train acc 0.109375 \n",
      "Batch 228, Loss: 2.289741, Train acc 0.078125 \n",
      "Batch 229, Loss: 2.284303, Train acc 0.140625 \n",
      "Batch 230, Loss: 2.287375, Train acc 0.171875 \n",
      "Batch 231, Loss: 2.277517, Train acc 0.140625 \n",
      "Batch 232, Loss: 2.305253, Train acc 0.093750 \n",
      "Batch 233, Loss: 2.298047, Train acc 0.078125 \n",
      "Batch 234, Loss: 2.272869, Train acc 0.203125 \n",
      "Batch 235, Loss: 2.310960, Train acc 0.062500 \n",
      "Batch 236, Loss: 2.284441, Train acc 0.109375 \n",
      "Batch 237, Loss: 2.287679, Train acc 0.109375 \n",
      "Batch 238, Loss: 2.291512, Train acc 0.093750 \n",
      "Batch 239, Loss: 2.288952, Train acc 0.078125 \n",
      "Batch 240, Loss: 2.290425, Train acc 0.093750 \n",
      "Batch 241, Loss: 2.282054, Train acc 0.125000 \n",
      "Batch 242, Loss: 2.283867, Train acc 0.078125 \n",
      "Batch 243, Loss: 2.280508, Train acc 0.109375 \n",
      "Batch 244, Loss: 2.281276, Train acc 0.109375 \n",
      "Batch 245, Loss: 2.286329, Train acc 0.140625 \n",
      "Batch 246, Loss: 2.279071, Train acc 0.187500 \n",
      "Batch 247, Loss: 2.289291, Train acc 0.078125 \n",
      "Batch 248, Loss: 2.286623, Train acc 0.218750 \n",
      "Batch 249, Loss: 2.272588, Train acc 0.234375 \n",
      "Batch 250, Loss: 2.269624, Train acc 0.218750 \n",
      "Batch 251, Loss: 2.273032, Train acc 0.203125 \n",
      "Batch 252, Loss: 2.282263, Train acc 0.109375 \n",
      "Batch 253, Loss: 2.276980, Train acc 0.140625 \n",
      "Batch 254, Loss: 2.266263, Train acc 0.375000 \n",
      "Batch 255, Loss: 2.272304, Train acc 0.250000 \n",
      "Batch 256, Loss: 2.255923, Train acc 0.234375 \n",
      "Batch 257, Loss: 2.261566, Train acc 0.171875 \n",
      "Batch 258, Loss: 2.267315, Train acc 0.250000 \n",
      "Batch 259, Loss: 2.261145, Train acc 0.125000 \n",
      "Batch 260, Loss: 2.255223, Train acc 0.203125 \n",
      "Batch 261, Loss: 2.246073, Train acc 0.218750 \n",
      "Batch 262, Loss: 2.247465, Train acc 0.140625 \n",
      "Batch 263, Loss: 2.254896, Train acc 0.234375 \n",
      "Batch 264, Loss: 2.246077, Train acc 0.187500 \n",
      "Batch 265, Loss: 2.252428, Train acc 0.406250 \n",
      "Batch 266, Loss: 2.232932, Train acc 0.234375 \n",
      "Batch 267, Loss: 2.247334, Train acc 0.187500 \n",
      "Batch 268, Loss: 2.248062, Train acc 0.250000 \n",
      "Batch 269, Loss: 2.263485, Train acc 0.234375 \n",
      "Batch 270, Loss: 2.244535, Train acc 0.265625 \n",
      "Batch 271, Loss: 2.216968, Train acc 0.375000 \n",
      "Batch 272, Loss: 2.211258, Train acc 0.250000 \n",
      "Batch 273, Loss: 2.174002, Train acc 0.156250 \n",
      "Batch 274, Loss: 2.284073, Train acc 0.171875 \n",
      "Batch 275, Loss: 2.280288, Train acc 0.156250 \n",
      "Batch 276, Loss: 2.293770, Train acc 0.109375 \n",
      "Batch 277, Loss: 2.231451, Train acc 0.312500 \n",
      "Batch 278, Loss: 2.231050, Train acc 0.140625 \n",
      "Batch 279, Loss: 2.191222, Train acc 0.265625 \n",
      "Batch 280, Loss: 2.170194, Train acc 0.406250 \n",
      "Batch 281, Loss: 2.190625, Train acc 0.203125 \n",
      "Batch 282, Loss: 2.151730, Train acc 0.359375 \n",
      "Batch 283, Loss: 2.145408, Train acc 0.359375 \n",
      "Batch 284, Loss: 2.149317, Train acc 0.187500 \n",
      "Batch 285, Loss: 2.153172, Train acc 0.531250 \n",
      "Batch 286, Loss: 2.079268, Train acc 0.250000 \n",
      "Batch 287, Loss: 2.080996, Train acc 0.187500 \n",
      "Batch 288, Loss: 2.049787, Train acc 0.578125 \n",
      "Batch 289, Loss: 1.913535, Train acc 0.359375 \n",
      "Batch 290, Loss: 1.959245, Train acc 0.328125 \n",
      "Batch 291, Loss: 2.037146, Train acc 0.234375 \n",
      "Batch 292, Loss: 1.994085, Train acc 0.250000 \n",
      "Batch 293, Loss: 2.338937, Train acc 0.109375 \n",
      "Batch 294, Loss: 2.097043, Train acc 0.390625 \n",
      "Batch 295, Loss: 2.093545, Train acc 0.203125 \n",
      "Batch 296, Loss: 2.101468, Train acc 0.328125 \n",
      "Batch 297, Loss: 1.993762, Train acc 0.359375 \n",
      "Batch 298, Loss: 1.822209, Train acc 0.437500 \n",
      "Batch 299, Loss: 1.753913, Train acc 0.406250 \n",
      "Batch 300, Loss: 2.482166, Train acc 0.093750 \n",
      "Batch 301, Loss: 2.417235, Train acc 0.171875 \n",
      "Batch 302, Loss: 2.399107, Train acc 0.093750 \n",
      "Batch 303, Loss: 2.287473, Train acc 0.093750 \n",
      "Batch 304, Loss: 2.250455, Train acc 0.109375 \n",
      "Batch 305, Loss: 2.182026, Train acc 0.203125 \n",
      "Batch 306, Loss: 2.108061, Train acc 0.312500 \n",
      "Batch 307, Loss: 2.090403, Train acc 0.203125 \n",
      "Batch 308, Loss: 2.038229, Train acc 0.343750 \n",
      "Batch 309, Loss: 1.986988, Train acc 0.359375 \n",
      "Batch 310, Loss: 1.924097, Train acc 0.296875 \n",
      "Batch 311, Loss: 1.761112, Train acc 0.359375 \n",
      "Batch 312, Loss: 1.745955, Train acc 0.250000 \n",
      "Batch 313, Loss: 1.840886, Train acc 0.375000 \n",
      "Batch 314, Loss: 1.736796, Train acc 0.312500 \n",
      "Batch 315, Loss: 2.002559, Train acc 0.265625 \n",
      "Batch 316, Loss: 2.815361, Train acc 0.078125 \n",
      "Batch 317, Loss: 2.258443, Train acc 0.078125 \n",
      "Batch 318, Loss: 1.998045, Train acc 0.234375 \n",
      "Batch 319, Loss: 1.883078, Train acc 0.218750 \n",
      "Batch 320, Loss: 1.739339, Train acc 0.390625 \n",
      "Batch 321, Loss: 1.798621, Train acc 0.343750 \n",
      "Batch 322, Loss: 1.668130, Train acc 0.453125 \n",
      "Batch 323, Loss: 1.655330, Train acc 0.343750 \n",
      "Batch 324, Loss: 1.671866, Train acc 0.500000 \n",
      "Batch 325, Loss: 1.576675, Train acc 0.406250 \n",
      "Batch 326, Loss: 1.766271, Train acc 0.484375 \n",
      "Batch 327, Loss: 1.810823, Train acc 0.343750 \n",
      "Batch 328, Loss: 1.508298, Train acc 0.375000 \n",
      "Batch 329, Loss: 1.739573, Train acc 0.234375 \n",
      "Batch 330, Loss: 1.770618, Train acc 0.406250 \n",
      "Batch 331, Loss: 1.547570, Train acc 0.468750 \n",
      "Batch 332, Loss: 1.415477, Train acc 0.437500 \n",
      "Batch 333, Loss: 1.360279, Train acc 0.531250 \n",
      "Batch 334, Loss: 1.453582, Train acc 0.406250 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 335, Loss: 1.424173, Train acc 0.484375 \n",
      "Batch 336, Loss: 2.047081, Train acc 0.296875 \n",
      "Batch 337, Loss: 3.000291, Train acc 0.062500 \n",
      "Batch 338, Loss: 3.263195, Train acc 0.171875 \n",
      "Batch 339, Loss: 2.402110, Train acc 0.125000 \n",
      "Batch 340, Loss: 2.353728, Train acc 0.062500 \n",
      "Batch 341, Loss: 2.244970, Train acc 0.218750 \n",
      "Batch 342, Loss: 2.267758, Train acc 0.187500 \n",
      "Batch 343, Loss: 2.230744, Train acc 0.187500 \n",
      "Batch 344, Loss: 2.250272, Train acc 0.140625 \n",
      "Batch 345, Loss: 2.262671, Train acc 0.109375 \n",
      "Batch 346, Loss: 2.164300, Train acc 0.125000 \n",
      "Batch 347, Loss: 2.125915, Train acc 0.250000 \n",
      "Batch 348, Loss: 2.089604, Train acc 0.109375 \n",
      "Batch 349, Loss: 2.064756, Train acc 0.203125 \n",
      "Batch 350, Loss: 1.905453, Train acc 0.312500 \n",
      "Batch 351, Loss: 1.704512, Train acc 0.375000 \n",
      "Batch 352, Loss: 1.732079, Train acc 0.250000 \n",
      "Batch 353, Loss: 1.551878, Train acc 0.437500 \n",
      "Batch 354, Loss: 1.979902, Train acc 0.187500 \n",
      "Batch 355, Loss: 3.036088, Train acc 0.218750 \n",
      "Batch 356, Loss: 2.310020, Train acc 0.125000 \n",
      "Batch 357, Loss: 2.137799, Train acc 0.265625 \n",
      "Batch 358, Loss: 2.175394, Train acc 0.125000 \n",
      "Batch 359, Loss: 2.083996, Train acc 0.250000 \n",
      "Batch 360, Loss: 2.085236, Train acc 0.375000 \n",
      "Batch 361, Loss: 2.014398, Train acc 0.312500 \n",
      "Batch 362, Loss: 1.910916, Train acc 0.375000 \n",
      "Batch 363, Loss: 1.910057, Train acc 0.359375 \n",
      "Batch 364, Loss: 1.909270, Train acc 0.234375 \n",
      "Batch 365, Loss: 1.736114, Train acc 0.281250 \n",
      "Batch 366, Loss: 1.478351, Train acc 0.437500 \n",
      "Batch 367, Loss: 1.457702, Train acc 0.390625 \n",
      "Batch 368, Loss: 1.311600, Train acc 0.484375 \n",
      "Batch 369, Loss: 1.211848, Train acc 0.390625 \n",
      "Batch 370, Loss: 1.580178, Train acc 0.390625 \n",
      "Batch 371, Loss: 1.439109, Train acc 0.468750 \n",
      "Batch 372, Loss: 1.530316, Train acc 0.343750 \n",
      "Batch 373, Loss: 1.473942, Train acc 0.484375 \n",
      "Batch 374, Loss: 1.359049, Train acc 0.390625 \n",
      "Batch 375, Loss: 1.340884, Train acc 0.437500 \n",
      "Batch 376, Loss: 1.270106, Train acc 0.421875 \n",
      "Batch 377, Loss: 1.380080, Train acc 0.484375 \n",
      "Batch 378, Loss: 1.143190, Train acc 0.531250 \n",
      "Batch 379, Loss: 1.149026, Train acc 0.593750 \n",
      "Batch 380, Loss: 1.384091, Train acc 0.468750 \n",
      "Batch 381, Loss: 1.673095, Train acc 0.312500 \n",
      "Batch 382, Loss: 1.462540, Train acc 0.531250 \n",
      "Batch 383, Loss: 1.247727, Train acc 0.484375 \n",
      "Batch 384, Loss: 0.993714, Train acc 0.687500 \n",
      "Batch 385, Loss: 1.350953, Train acc 0.515625 \n",
      "Batch 386, Loss: 1.518838, Train acc 0.453125 \n",
      "Batch 387, Loss: 1.438739, Train acc 0.406250 \n",
      "Batch 388, Loss: 1.233610, Train acc 0.578125 \n",
      "Batch 389, Loss: 1.239259, Train acc 0.484375 \n",
      "Batch 390, Loss: 1.054326, Train acc 0.609375 \n",
      "Batch 391, Loss: 1.199101, Train acc 0.515625 \n",
      "Batch 392, Loss: 1.287357, Train acc 0.500000 \n",
      "Batch 393, Loss: 1.128682, Train acc 0.625000 \n",
      "Batch 394, Loss: 0.950569, Train acc 0.593750 \n",
      "Batch 395, Loss: 1.058009, Train acc 0.531250 \n",
      "Batch 396, Loss: 0.990209, Train acc 0.515625 \n",
      "Batch 397, Loss: 1.090409, Train acc 0.593750 \n",
      "Batch 398, Loss: 1.198653, Train acc 0.562500 \n",
      "Batch 399, Loss: 1.004481, Train acc 0.562500 \n",
      "Batch 400, Loss: 1.440407, Train acc 0.546875 \n",
      "Batch 401, Loss: 1.466388, Train acc 0.453125 \n",
      "Batch 402, Loss: 1.294809, Train acc 0.515625 \n",
      "Batch 403, Loss: 1.014262, Train acc 0.687500 \n",
      "Batch 404, Loss: 1.066863, Train acc 0.546875 \n",
      "Batch 405, Loss: 1.000180, Train acc 0.578125 \n",
      "Batch 406, Loss: 1.089563, Train acc 0.593750 \n",
      "Batch 407, Loss: 1.007699, Train acc 0.609375 \n",
      "Batch 408, Loss: 1.183997, Train acc 0.640625 \n",
      "Batch 409, Loss: 0.964147, Train acc 0.687500 \n",
      "Batch 410, Loss: 1.043798, Train acc 0.671875 \n",
      "Batch 411, Loss: 1.254648, Train acc 0.500000 \n",
      "Batch 412, Loss: 0.900213, Train acc 0.640625 \n",
      "Batch 413, Loss: 1.032118, Train acc 0.656250 \n",
      "Batch 414, Loss: 1.337794, Train acc 0.531250 \n",
      "Batch 415, Loss: 1.170405, Train acc 0.609375 \n",
      "Batch 416, Loss: 1.086676, Train acc 0.578125 \n",
      "Batch 417, Loss: 1.035342, Train acc 0.578125 \n",
      "Batch 418, Loss: 0.911030, Train acc 0.671875 \n",
      "Batch 419, Loss: 0.998538, Train acc 0.625000 \n",
      "Batch 420, Loss: 1.047896, Train acc 0.578125 \n",
      "Batch 421, Loss: 1.120970, Train acc 0.593750 \n",
      "Batch 422, Loss: 0.880981, Train acc 0.562500 \n",
      "Batch 423, Loss: 1.095167, Train acc 0.625000 \n",
      "Batch 424, Loss: 0.993759, Train acc 0.640625 \n",
      "Batch 425, Loss: 1.028793, Train acc 0.578125 \n",
      "Batch 426, Loss: 1.036201, Train acc 0.609375 \n",
      "Batch 427, Loss: 1.056310, Train acc 0.640625 \n",
      "Batch 428, Loss: 0.967191, Train acc 0.640625 \n",
      "Batch 429, Loss: 0.961734, Train acc 0.656250 \n",
      "Batch 430, Loss: 0.904011, Train acc 0.640625 \n",
      "Batch 431, Loss: 1.056110, Train acc 0.500000 \n",
      "Batch 432, Loss: 0.953532, Train acc 0.640625 \n",
      "Batch 433, Loss: 1.098210, Train acc 0.515625 \n",
      "Batch 434, Loss: 1.014660, Train acc 0.609375 \n",
      "Batch 435, Loss: 0.862260, Train acc 0.750000 \n",
      "Batch 436, Loss: 0.908061, Train acc 0.640625 \n",
      "Batch 437, Loss: 0.958357, Train acc 0.640625 \n",
      "Batch 438, Loss: 1.249871, Train acc 0.515625 \n",
      "Batch 439, Loss: 1.125579, Train acc 0.546875 \n",
      "Batch 440, Loss: 0.717324, Train acc 0.765625 \n",
      "Batch 441, Loss: 0.755013, Train acc 0.671875 \n",
      "Batch 442, Loss: 1.155447, Train acc 0.609375 \n",
      "Batch 443, Loss: 1.003285, Train acc 0.687500 \n",
      "Batch 444, Loss: 0.719452, Train acc 0.750000 \n",
      "Batch 445, Loss: 0.777577, Train acc 0.687500 \n",
      "Batch 446, Loss: 1.255964, Train acc 0.531250 \n",
      "Batch 447, Loss: 1.034899, Train acc 0.578125 \n",
      "Batch 448, Loss: 0.958352, Train acc 0.671875 \n",
      "Batch 449, Loss: 0.927131, Train acc 0.578125 \n",
      "Batch 450, Loss: 0.947234, Train acc 0.671875 \n",
      "Batch 451, Loss: 0.789989, Train acc 0.671875 \n",
      "Batch 452, Loss: 0.787428, Train acc 0.734375 \n",
      "Batch 453, Loss: 1.059911, Train acc 0.671875 \n",
      "Batch 454, Loss: 1.047372, Train acc 0.546875 \n",
      "Batch 455, Loss: 1.029809, Train acc 0.593750 \n",
      "Batch 456, Loss: 0.908350, Train acc 0.625000 \n",
      "Batch 457, Loss: 0.774097, Train acc 0.703125 \n",
      "Batch 458, Loss: 0.714311, Train acc 0.750000 \n",
      "Batch 459, Loss: 1.228778, Train acc 0.578125 \n",
      "Batch 460, Loss: 1.371123, Train acc 0.500000 \n",
      "Batch 461, Loss: 1.029702, Train acc 0.687500 \n",
      "Batch 462, Loss: 0.812794, Train acc 0.734375 \n",
      "Batch 463, Loss: 0.680117, Train acc 0.734375 \n",
      "Batch 464, Loss: 0.871418, Train acc 0.593750 \n",
      "Batch 465, Loss: 1.092291, Train acc 0.453125 \n",
      "Batch 466, Loss: 0.902419, Train acc 0.671875 \n",
      "Batch 467, Loss: 0.936151, Train acc 0.640625 \n",
      "Batch 468, Loss: 1.034160, Train acc 0.609375 \n",
      "Batch 469, Loss: 0.937708, Train acc 0.656250 \n",
      "Batch 470, Loss: 1.000455, Train acc 0.609375 \n",
      "Batch 471, Loss: 0.762937, Train acc 0.703125 \n",
      "Batch 472, Loss: 0.756564, Train acc 0.656250 \n",
      "Batch 473, Loss: 0.758541, Train acc 0.625000 \n",
      "Batch 474, Loss: 0.791923, Train acc 0.750000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-62d40acac656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch %d, Loss: %f, Train acc %f \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 904\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    905\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1136\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1137\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1354\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1358\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1339\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-1\n",
    "max_steps = 1000\n",
    "batch_size = 64\n",
    "height = width = 28\n",
    "num_channels = 1\n",
    "num_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, height, width, num_channels])\n",
    "resize_input = tf.image.resize_images(input_placeholder, [96, 96])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "logits = ResNet(resize_input, num_outputs)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], height, width, 1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "test_acc = []\n",
    "\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, height, width, num_channels))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label, is_training: True}\n",
    "    loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)\n",
    "    print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "        \n",
    "for i in range(100):\n",
    "    test_data, test_label = test_dataset.next_batch(100)\n",
    "    test_data = np.reshape(test_data, (100, height, width, num_channels))\n",
    "    test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_data, gt_placeholder: test_label, is_training: False})\n",
    "    test_acc.append(test_acc_)\n",
    "print (\"Test Loss: %f, Test acc %f \" % (np.mean(test_loss_), np.mean(test_acc_)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
