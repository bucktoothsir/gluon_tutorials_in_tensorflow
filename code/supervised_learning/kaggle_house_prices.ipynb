{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战Kaggle比赛——使用Gluon预测房价和K折交叉验证\n",
    "本章介绍如何使用Gluon来实战Kaggle比赛。我们以[房价预测](http://zh.gluon.ai/chapter_supervised-learning/kaggle-gluon-kfold.html#%E5%AE%9E%E6%88%98Kaggle%E6%AF%94%E8%B5%9B%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8Gluon%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%E5%92%8CK%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81)问题为例，为大家提供一整套实战中常常需要的工具，例如K折交叉验证。我们还以pandas为工具介绍如何对真实世界中的数据进行重要的预处理，例如：\n",
    "\n",
    "- 处理离散数据\n",
    "\n",
    "- 处理丢失的数据特征\n",
    "\n",
    "- 对数据进行标准化\n",
    "\n",
    "需要注意的是，本章仅提供一些基本实战流程供大家参考。对于数据的预处理、模型的设计和参数的选择等，我们特意只提供最基础的版本。希望大家一定要通过动手实战、仔细观察实验现象、认真分析实验结果并不断调整方法，从而得到令自己满意的结果。\n",
    "\n",
    "这是一次宝贵的实战机会，我们相信你一定能从动手的过程中学到很多。\n",
    "\n",
    "Get your hands dirty。\n",
    "\n",
    "## Kaggle中的房价预测问题\n",
    "\n",
    "Kaggle是一个著名的供机器学习爱好者交流的平台。为了便于提交结果，请大家注册Kaggle账号。请注意，目前Kaggle仅限每个账号一天以内10次提交结果的机会。所以提交结果前务必三思\n",
    "\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/kaggle.png)\n",
    "我们以房价预测问题为例教大家如何实战一次Kaggle比赛。请大家在动手开始之前点击房价预测问题了解相关信息。\n",
    "![image.png](http://zh.gluon.ai/_images/house_pricing.png)\n",
    "\n",
    "### 读入数据\n",
    "比赛数据分为训练数据集和测试数据集。两个数据集都包括每个房子的特征，例如街道类型、建造年份、房顶类型、地下室状况等特征值。这些特征值有连续的数字、离散的标签甚至是缺失值’na’。只有训练数据集包括了我们需要在测试数据集中预测的每个房子的价格。数据可以从房价预测问题中下载。\n",
    "\n",
    "[训练数据集下载地址](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/train.csv) [测试数据集下载地址](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/download/test.csv)\n",
    "\n",
    "我们通过使用pandas读入数据。请确保安装了pandas (pip install pandas)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"../../data/train.csv\")\n",
    "test = pd.read_csv(\"../../data/test.csv\")\n",
    "all_X = pd.concat((train.loc[:, 'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:, 'MSSubClass':'SaleCondition']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看数据长什么样子。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据大小如下。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据\n",
    "我们使用pandas对数值特征做标准化处理：\n",
    "\n",
    "$x_i = \\frac{x_i - \\mathbb{E} x_i}{\\text{std}(x_i)}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MSSubClass MSZoning  LotFrontage   LotArea Street Alley LotShape  \\\n",
      "0    0.067320       RL    -0.184443 -0.217841   Pave   NaN      Reg   \n",
      "1   -0.873466       RL     0.458096 -0.072032   Pave   NaN      Reg   \n",
      "2    0.067320       RL    -0.055935  0.137173   Pave   NaN      IR1   \n",
      "3    0.302516       RL    -0.398622 -0.078371   Pave   NaN      IR1   \n",
      "4    0.067320       RL     0.629439  0.518814   Pave   NaN      IR1   \n",
      "\n",
      "  LandContour Utilities LotConfig      ...       ScreenPorch  PoolArea PoolQC  \\\n",
      "0         Lvl    AllPub    Inside      ...         -0.285886 -0.063139    NaN   \n",
      "1         Lvl    AllPub       FR2      ...         -0.285886 -0.063139    NaN   \n",
      "2         Lvl    AllPub    Inside      ...         -0.285886 -0.063139    NaN   \n",
      "3         Lvl    AllPub    Corner      ...         -0.285886 -0.063139    NaN   \n",
      "4         Lvl    AllPub       FR2      ...         -0.285886 -0.063139    NaN   \n",
      "\n",
      "  Fence MiscFeature   MiscVal    MoSold    YrSold  SaleType  SaleCondition  \n",
      "0   NaN         NaN -0.089577 -1.551918  0.157619        WD         Normal  \n",
      "1   NaN         NaN -0.089577 -0.446848 -0.602858        WD         Normal  \n",
      "2   NaN         NaN -0.089577  1.026577  0.157619        WD         Normal  \n",
      "3   NaN         NaN -0.089577 -1.551918 -1.363335        WD        Abnorml  \n",
      "4   NaN         NaN -0.089577  2.131647  0.157619        WD         Normal  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "numeric_feats = all_X.dtypes[all_X.dtypes != \"object\"].index\n",
    "all_X[numeric_feats] = all_X[numeric_feats].apply(lambda x: (x - x.mean()) / (x.std()))\n",
    "print all_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在把离散数据点转换成数值标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_X = pd.get_dummies(all_X, dummy_na=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把缺失数据用本特征的平均值估计。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSSubClass               9.015570e-17\n",
      "LotFrontage              3.058133e-16\n",
      "LotArea                  8.286737e-18\n",
      "OverallQual             -4.176173e-16\n",
      "OverallCond              1.500075e-16\n",
      "YearBuilt               -6.544953e-16\n",
      "YearRemodAdd             5.639925e-16\n",
      "MasVnrArea              -4.573297e-16\n",
      "BsmtFinSF1               9.793400e-17\n",
      "BsmtFinSF2              -8.971576e-17\n",
      "BsmtUnfSF                7.495337e-17\n",
      "TotalBsmtSF             -1.108130e-17\n",
      "1stFlrSF                -1.286322e-16\n",
      "2ndFlrSF                -1.991479e-16\n",
      "LowQualFinSF            -1.871081e-15\n",
      "GrLivArea                1.416019e-16\n",
      "BsmtFullBath             1.010124e-16\n",
      "BsmtHalfBath             2.993644e-16\n",
      "FullBath                 1.428951e-16\n",
      "HalfBath                 8.702262e-17\n",
      "BedroomAbvGr             2.030655e-16\n",
      "KitchenAbvGr            -4.203177e-16\n",
      "TotRmsAbvGrd             3.128707e-16\n",
      "Fireplaces              -9.558035e-17\n",
      "GarageYrBlt             -9.770767e-17\n",
      "GarageCars               4.734618e-16\n",
      "GarageArea              -2.815507e-17\n",
      "WoodDeckSF              -6.694048e-18\n",
      "OpenPorchSF              1.157763e-16\n",
      "EnclosedPorch           -1.872432e-16\n",
      "                             ...     \n",
      "PoolQC_Fa                6.851662e-04\n",
      "PoolQC_Gd                1.370332e-03\n",
      "PoolQC_nan               9.965742e-01\n",
      "Fence_GdPrv              4.042480e-02\n",
      "Fence_GdWo               3.836930e-02\n",
      "Fence_MnPrv              1.127098e-01\n",
      "Fence_MnWw               4.110997e-03\n",
      "Fence_nan                8.043851e-01\n",
      "MiscFeature_Gar2         1.712915e-03\n",
      "MiscFeature_Othr         1.370332e-03\n",
      "MiscFeature_Shed         3.254539e-02\n",
      "MiscFeature_TenC         3.425831e-04\n",
      "MiscFeature_nan          9.640288e-01\n",
      "SaleType_COD             2.980473e-02\n",
      "SaleType_CWD             4.110997e-03\n",
      "SaleType_Con             1.712915e-03\n",
      "SaleType_ConLD           8.907160e-03\n",
      "SaleType_ConLI           3.083248e-03\n",
      "SaleType_ConLw           2.740665e-03\n",
      "SaleType_New             8.187736e-02\n",
      "SaleType_Oth             2.398082e-03\n",
      "SaleType_WD              8.650223e-01\n",
      "SaleType_nan             3.425831e-04\n",
      "SaleCondition_Abnorml    6.509078e-02\n",
      "SaleCondition_AdjLand    4.110997e-03\n",
      "SaleCondition_Alloca     8.221994e-03\n",
      "SaleCondition_Family     1.575882e-02\n",
      "SaleCondition_Normal     8.228845e-01\n",
      "SaleCondition_Partial    8.393285e-02\n",
      "SaleCondition_nan        0.000000e+00\n",
      "Length: 331, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print all_X.mean()\n",
    "all_X = all_X.fillna(all_X.mean())\n",
    "#print all_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06731988 -0.18444262 -0.21784137  0.6460727  -0.5071973   1.04607837\n",
      "  0.8966793   0.52303833  0.58070781 -0.29302957 -0.93454221 -0.44417553\n",
      " -0.77372846  1.20717172 -0.10117968  0.41347637  1.08646381 -0.24976672\n",
      "  0.78123196  1.23238772  0.16989798 -0.20766289  0.98668031 -0.92415287\n",
      "  0.97311008  0.30642276  0.34878012 -0.74063351  0.19997175 -0.35953915\n",
      " -0.10331283 -0.28588648 -0.06313936 -0.08957661 -1.55191764  0.15761852\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          1.          0.          0.          0.          1.\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          1.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          1.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          0.          0.\n",
      "  1.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          1.          0.          0.\n",
      "  0.          0.          1.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "num_train = train.shape[0]\n",
    "\n",
    "X_train = all_X[:num_train].as_matrix()\n",
    "X_test = all_X[num_train:].as_matrix()\n",
    "y_train = train.SalePrice.as_matrix()\n",
    "print X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入NDArray格式数据\n",
    "为了便于和Gluon交互，我们需要导入NDArray格式数据。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train.reshape((num_train, 1))\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "def data_iter(X, y, batch_size, num_epochs, shuffle=True):\n",
    "    # 产生一个随机索引\n",
    "    num_examples = X.shape[0]\n",
    "    idx = list(range(num_examples*num_epochs))\n",
    "    if shuffle:\n",
    "        random.shuffle(idx)\n",
    "    batch_X = []\n",
    "    batch_y = []\n",
    "    for i in idx:\n",
    "        i %= num_examples\n",
    "        batch_X.append(X[i])\n",
    "        batch_y.append(y[i])\n",
    "        if len(batch_X) == batch_size:\n",
    "            yield np.array(batch_X), np.array(batch_y)\n",
    "            batch_X = []\n",
    "            batch_y = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义比赛中测量结果用的函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_rmse_log(yhat, y):\n",
    "    num_train = y.shape[0]\n",
    "    yhat = tf.clip_by_value(yhat, 1, float('inf'))\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.log(yhat) - tf.log(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "我们将模型的定义放在一个函数里供多次调用。这是一个基本的线性回归模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    with tf.name_scope('linear_regression'):\n",
    "        yhat = tf.contrib.layers.fully_connected(X, 1, activation_fn=None)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义一个训练的函数，这样在跑不同的实验时不需要重复实现相同的步骤。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(X_train, y_train, X_test, y_test, epochs,\n",
    "          verbose_step, learning_rate, weight_decay):\n",
    "    tf.reset_default_graph()\n",
    "    batch_size = 100\n",
    "    train_loss = []\n",
    "    if X_test is not None:\n",
    "        test_loss = []\n",
    "    data_iter_train = data_iter(X_train, y_train, batch_size, epochs, shuffle=True)\n",
    "    input_placeholder = tf.placeholder(tf.float32, [None, X_train.shape[1]])\n",
    "    gt_placeholder = tf.placeholder(tf.float32, [None])\n",
    "    yhat = net(input_placeholder)\n",
    "    mse = tf.losses.mean_squared_error(gt_placeholder, tf.squeeze(yhat))\n",
    "    for var in tf.all_variables():\n",
    "        mse += weight_decay * tf.nn.l2_loss(var)\n",
    "    #train_op = tf.train.AdamOptimizer(learning_rate).minimize(mse)\n",
    "    rmse = get_rmse_log(tf.squeeze(yhat), gt_placeholder)\n",
    "\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(mse)\n",
    "    \n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    for data, label in data_iter_train:\n",
    "        mse_, rmse_, yhat_,  _ = sess.run([mse, rmse, yhat, train_op], feed_dict={input_placeholder: data, gt_placeholder: label})\n",
    "        if step % verbose_step == 0:\n",
    "            print(\"step %d, train loss: %f\" % (step, rmse_))\n",
    "            train_loss.append(rmse_)\n",
    "            if X_test is not None and y_test is not None:\n",
    "                test_rmse_, test_yhat_ = sess.run([rmse, yhat], feed_dict={input_placeholder: X_test, gt_placeholder: y_test})\n",
    "                test_loss.append(test_rmse_)\n",
    "        step += 1\n",
    "    plt.plot(train_loss)\n",
    "    plt.legend(['train'])\n",
    "    if X_test is not None:\n",
    "        plt.plot(test_loss)\n",
    "        plt.legend(['train','test'])\n",
    "    plt.show()\n",
    "    if X_test is not None and y_test is not None:\n",
    "        return rmse_, test_rmse_, test_yhat_\n",
    "\n",
    "    if X_test is not None:\n",
    "        predict_yhat = sess.run(yhat, feed_dict={input_placeholder: X_test})\n",
    "        return predict_yhat\n",
    "    else:\n",
    "        return rmse_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K折交叉验证\n",
    "\n",
    "在过拟合中我们讲过，过度依赖训练数据集的误差来推断测试数据集的误差容易导致过拟合。事实上，当我们调参时，往往需要基于K折交叉验证。\n",
    "\n",
    "在K折交叉验证中，我们把初始采样分割成KK个子样本，一个单独的子样本被保留作为验证模型的数据，其他K−1个样本用来训练。\n",
    "\n",
    "我们关心K次验证模型的测试结果的平均值和训练误差的平均值，因此我们定义K折交叉验证函数如下。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_valid(k, epochs, verbose_epoch, X_train, y_train,\n",
    "                       learning_rate, weight_decay):\n",
    "    assert k > 1\n",
    "    fold_size = X_train.shape[0] // k\n",
    "    train_loss_sum = 0.0\n",
    "    test_loss_sum = 0.0\n",
    "    for test_i in range(k):\n",
    "        X_val_test = X_train[test_i * fold_size: (test_i + 1) * fold_size, :]\n",
    "        y_val_test = y_train[test_i * fold_size: (test_i + 1) * fold_size]\n",
    "\n",
    "        val_train_defined = False\n",
    "        for i in range(k):\n",
    "            if i != test_i:\n",
    "                X_cur_fold = X_train[i * fold_size: (i + 1) * fold_size, :]\n",
    "                y_cur_fold = y_train[i * fold_size: (i + 1) * fold_size]\n",
    "                if not val_train_defined:\n",
    "                    X_val_train = X_cur_fold\n",
    "                    y_val_train = y_cur_fold\n",
    "                    val_train_defined = True\n",
    "                else:\n",
    "                    X_val_train = np.concatenate((X_val_train, X_cur_fold), 0)\n",
    "                    y_val_train = np.concatenate((y_val_train, y_cur_fold), 0)\n",
    "        train_loss, test_loss, test_yhat = train(\n",
    "            X_val_train, y_val_train, X_val_test, y_val_test,\n",
    "            epochs, verbose_epoch, learning_rate, weight_decay)\n",
    "        train_loss_sum += train_loss\n",
    "        print(\"Test loss: %f\" % test_loss)\n",
    "        test_loss_sum += test_loss\n",
    "    return train_loss_sum / k, test_loss_sum / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型并交叉验证\n",
    "以下的模型参数都是可以调的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "epochs = 2000\n",
    "verbose_epoch = 100\n",
    "learning_rate = 0.005\n",
    "weight_decay = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定以上调好的参数，接下来我们训练并交叉验证我们的模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-277ee36c80b1>:18: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "step 0, train loss: 12.041246\n",
      "step 100, train loss: 0.151103\n",
      "step 200, train loss: 0.138934\n",
      "step 300, train loss: 1.071251\n",
      "step 400, train loss: 0.113411\n",
      "step 500, train loss: 1.071612\n",
      "step 600, train loss: 0.124958\n",
      "step 700, train loss: 0.151833\n",
      "step 800, train loss: 0.132270\n",
      "step 900, train loss: 0.160640\n",
      "step 1000, train loss: 0.178125\n",
      "step 1100, train loss: 0.135677\n",
      "step 1200, train loss: 0.138064\n",
      "step 1300, train loss: 0.126237\n",
      "step 1400, train loss: 0.136134\n",
      "step 1500, train loss: 0.132249\n",
      "step 1600, train loss: 0.169968\n",
      "step 1700, train loss: 0.122050\n",
      "step 1800, train loss: 0.123914\n",
      "step 1900, train loss: 0.149102\n",
      "step 2000, train loss: 0.110362\n",
      "step 2100, train loss: 0.171239\n",
      "step 2200, train loss: 0.128134\n",
      "step 2300, train loss: 0.151768\n",
      "step 2400, train loss: 0.091769\n",
      "step 2500, train loss: 0.105059\n",
      "step 2600, train loss: 0.111639\n",
      "step 2700, train loss: 0.108661\n",
      "step 2800, train loss: 0.103281\n",
      "step 2900, train loss: 0.151356\n",
      "step 3000, train loss: 0.104849\n",
      "step 3100, train loss: 0.128077\n",
      "step 3200, train loss: 0.121522\n",
      "step 3300, train loss: 0.123356\n",
      "step 3400, train loss: 0.118288\n",
      "step 3500, train loss: 0.131061\n",
      "step 3600, train loss: 0.120720\n",
      "step 3700, train loss: 0.180463\n",
      "step 3800, train loss: 0.132399\n",
      "step 3900, train loss: 0.157744\n",
      "step 4000, train loss: 0.130648\n",
      "step 4100, train loss: 0.178857\n",
      "step 4200, train loss: 0.136025\n",
      "step 4300, train loss: 0.144059\n",
      "step 4400, train loss: 0.129096\n",
      "step 4500, train loss: 0.124510\n",
      "step 4600, train loss: 0.120456\n",
      "step 4700, train loss: 0.191400\n",
      "step 4800, train loss: 0.167905\n",
      "step 4900, train loss: 0.187005\n",
      "step 5000, train loss: 0.111351\n",
      "step 5100, train loss: 0.121026\n",
      "step 5200, train loss: 0.115754\n",
      "step 5300, train loss: 0.117963\n",
      "step 5400, train loss: 0.161073\n",
      "step 5500, train loss: 0.150208\n",
      "step 5600, train loss: 0.135936\n",
      "step 5700, train loss: 0.096738\n",
      "step 5800, train loss: 0.146998\n",
      "step 5900, train loss: 0.145524\n",
      "step 6000, train loss: 0.121586\n",
      "step 6100, train loss: 0.158101\n",
      "step 6200, train loss: 0.115338\n",
      "step 6300, train loss: 0.159598\n",
      "step 6400, train loss: 0.125872\n",
      "step 6500, train loss: 0.120637\n",
      "step 6600, train loss: 0.125264\n",
      "step 6700, train loss: 0.119100\n",
      "step 6800, train loss: 0.125454\n",
      "step 6900, train loss: 0.122267\n",
      "step 7000, train loss: 0.121637\n",
      "step 7100, train loss: 0.104270\n",
      "step 7200, train loss: 0.160151\n",
      "step 7300, train loss: 0.104307\n",
      "step 7400, train loss: 0.123152\n",
      "step 7500, train loss: 0.142477\n",
      "step 7600, train loss: 0.179033\n",
      "step 7700, train loss: 0.113615\n",
      "step 7800, train loss: 0.152367\n",
      "step 7900, train loss: 0.140420\n",
      "step 8000, train loss: 0.144857\n",
      "step 8100, train loss: 0.129156\n",
      "step 8200, train loss: 0.141174\n",
      "step 8300, train loss: 0.155670\n",
      "step 8400, train loss: 0.118471\n",
      "step 8500, train loss: 0.128580\n",
      "step 8600, train loss: 0.155095\n",
      "step 8700, train loss: 0.104221\n",
      "step 8800, train loss: 0.127850\n",
      "step 8900, train loss: 0.122436\n",
      "step 9000, train loss: 0.098806\n",
      "step 9100, train loss: 0.164193\n",
      "step 9200, train loss: 0.175718\n",
      "step 9300, train loss: 0.136571\n",
      "step 9400, train loss: 0.107376\n",
      "step 9500, train loss: 0.132081\n",
      "step 9600, train loss: 0.170176\n",
      "step 9700, train loss: 0.116842\n",
      "step 9800, train loss: 0.125989\n",
      "step 9900, train loss: 0.108443\n",
      "step 10000, train loss: 0.123289\n",
      "step 10100, train loss: 0.128936\n",
      "step 10200, train loss: 0.114817\n",
      "step 10300, train loss: 0.119593\n",
      "step 10400, train loss: 0.151172\n",
      "step 10500, train loss: 0.124264\n",
      "step 10600, train loss: 0.138842\n",
      "step 10700, train loss: 0.149761\n",
      "step 10800, train loss: 0.117238\n",
      "step 10900, train loss: 0.098209\n",
      "step 11000, train loss: 0.108932\n",
      "step 11100, train loss: 0.140708\n",
      "step 11200, train loss: 0.130123\n",
      "step 11300, train loss: 0.160231\n",
      "step 11400, train loss: 0.129970\n",
      "step 11500, train loss: 0.098218\n",
      "step 11600, train loss: 0.134826\n",
      "step 11700, train loss: 0.128845\n",
      "step 11800, train loss: 0.120739\n",
      "step 11900, train loss: 0.151080\n",
      "step 12000, train loss: 0.131226\n",
      "step 12100, train loss: 0.116893\n",
      "step 12200, train loss: 0.109053\n",
      "step 12300, train loss: 0.139675\n",
      "step 12400, train loss: 0.115595\n",
      "step 12500, train loss: 0.114123\n",
      "step 12600, train loss: 0.129714\n",
      "step 12700, train loss: 0.130549\n",
      "step 12800, train loss: 0.110714\n",
      "step 12900, train loss: 0.140004\n",
      "step 13000, train loss: 0.116774\n",
      "step 13100, train loss: 0.112416\n",
      "step 13200, train loss: 0.155205\n",
      "step 13300, train loss: 0.147702\n",
      "step 13400, train loss: 0.107538\n",
      "step 13500, train loss: 0.209093\n",
      "step 13600, train loss: 0.151194\n",
      "step 13700, train loss: 0.112391\n",
      "step 13800, train loss: 0.142348\n",
      "step 13900, train loss: 0.157977\n",
      "step 14000, train loss: 0.125502\n",
      "step 14100, train loss: 0.133570\n",
      "step 14200, train loss: 0.167640\n",
      "step 14300, train loss: 0.109857\n",
      "step 14400, train loss: 0.102223\n",
      "step 14500, train loss: 0.118877\n",
      "step 14600, train loss: 0.115565\n",
      "step 14700, train loss: 0.144580\n",
      "step 14800, train loss: 0.120965\n",
      "step 14900, train loss: 0.124966\n",
      "step 15000, train loss: 0.098526\n",
      "step 15100, train loss: 0.143551\n",
      "step 15200, train loss: 0.114252\n",
      "step 15300, train loss: 0.128022\n",
      "step 15400, train loss: 0.169669\n",
      "step 15500, train loss: 0.109305\n",
      "step 15600, train loss: 0.119960\n",
      "step 15700, train loss: 0.121975\n",
      "step 15800, train loss: 0.146991\n",
      "step 15900, train loss: 0.100526\n",
      "step 16000, train loss: 0.127863\n",
      "step 16100, train loss: 0.137872\n",
      "step 16200, train loss: 0.107465\n",
      "step 16300, train loss: 0.118112\n",
      "step 16400, train loss: 0.110804\n",
      "step 16500, train loss: 0.119950\n",
      "step 16600, train loss: 0.142804\n",
      "step 16700, train loss: 0.119591\n",
      "step 16800, train loss: 0.131904\n",
      "step 16900, train loss: 0.120175\n",
      "step 17000, train loss: 0.145640\n",
      "step 17100, train loss: 0.112125\n",
      "step 17200, train loss: 0.092028\n",
      "step 17300, train loss: 0.124851\n",
      "step 17400, train loss: 0.147279\n",
      "step 17500, train loss: 0.145368\n",
      "step 17600, train loss: 0.113576\n",
      "step 17700, train loss: 0.109846\n",
      "step 17800, train loss: 0.106604\n",
      "step 17900, train loss: 0.160870\n",
      "step 18000, train loss: 0.114752\n",
      "step 18100, train loss: 0.096667\n",
      "step 18200, train loss: 0.119739\n",
      "step 18300, train loss: 0.135381\n",
      "step 18400, train loss: 0.140430\n",
      "step 18500, train loss: 0.118840\n",
      "step 18600, train loss: 0.150074\n",
      "step 18700, train loss: 0.118528\n",
      "step 18800, train loss: 0.159870\n",
      "step 18900, train loss: 0.126526\n",
      "step 19000, train loss: 0.104557\n",
      "step 19100, train loss: 0.130285\n",
      "step 19200, train loss: 0.160821\n",
      "step 19300, train loss: 0.113021\n",
      "step 19400, train loss: 0.114432\n",
      "step 19500, train loss: 0.098121\n",
      "step 19600, train loss: 0.178502\n",
      "step 19700, train loss: 0.110199\n",
      "step 19800, train loss: 0.125492\n",
      "step 19900, train loss: 0.115488\n",
      "step 20000, train loss: 0.130962\n",
      "step 20100, train loss: 0.189255\n",
      "step 20200, train loss: 0.126592\n",
      "step 20300, train loss: 0.136132\n",
      "step 20400, train loss: 0.114238\n",
      "step 20500, train loss: 0.106958\n",
      "step 20600, train loss: 0.150825\n",
      "step 20700, train loss: 0.111160\n",
      "step 20800, train loss: 0.131212\n",
      "step 20900, train loss: 0.128493\n",
      "step 21000, train loss: 0.118624\n",
      "step 21100, train loss: 0.132639\n",
      "step 21200, train loss: 0.129531\n",
      "step 21300, train loss: 0.148390\n",
      "step 21400, train loss: 0.147905\n",
      "step 21500, train loss: 0.137828\n",
      "step 21600, train loss: 0.126730\n",
      "step 21700, train loss: 0.124787\n",
      "step 21800, train loss: 0.147281\n",
      "step 21900, train loss: 0.104684\n",
      "step 22000, train loss: 0.143936\n",
      "step 22100, train loss: 0.141698\n",
      "step 22200, train loss: 0.094510\n",
      "step 22300, train loss: 0.145527\n",
      "step 22400, train loss: 0.105466\n",
      "step 22500, train loss: 0.123673\n",
      "step 22600, train loss: 0.120642\n",
      "step 22700, train loss: 0.117356\n",
      "step 22800, train loss: 0.133652\n",
      "step 22900, train loss: 0.102395\n",
      "step 23000, train loss: 0.110266\n",
      "step 23100, train loss: 0.099540\n",
      "step 23200, train loss: 0.118685\n",
      "step 23300, train loss: 0.110243\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11cf4cb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.175609\n",
      "step 0, train loss: 12.048615\n",
      "step 100, train loss: 0.230225\n",
      "step 200, train loss: 0.274341\n",
      "step 300, train loss: 0.100598\n",
      "step 400, train loss: 0.134173\n",
      "step 500, train loss: 0.181296\n",
      "step 600, train loss: 0.104731\n",
      "step 700, train loss: 0.133255\n",
      "step 800, train loss: 0.183080\n",
      "step 900, train loss: 0.135950\n",
      "step 1000, train loss: 0.112655\n",
      "step 1100, train loss: 0.136337\n",
      "step 1200, train loss: 0.158480\n",
      "step 1300, train loss: 0.132105\n",
      "step 1400, train loss: 0.132699\n",
      "step 1500, train loss: 0.142625\n",
      "step 1600, train loss: 0.114097\n",
      "step 1700, train loss: 0.127950\n",
      "step 1800, train loss: 0.164097\n",
      "step 1900, train loss: 0.153213\n",
      "step 2000, train loss: 0.141671\n",
      "step 2100, train loss: 0.147271\n",
      "step 2200, train loss: 0.150746\n",
      "step 2300, train loss: 0.123627\n",
      "step 2400, train loss: 0.092863\n",
      "step 2500, train loss: 0.106809\n",
      "step 2600, train loss: 0.108614\n",
      "step 2700, train loss: 0.127648\n",
      "step 2800, train loss: 0.128616\n",
      "step 2900, train loss: 0.114564\n",
      "step 3000, train loss: 0.152505\n",
      "step 3100, train loss: 0.131443\n",
      "step 3200, train loss: 0.130326\n",
      "step 3300, train loss: 0.109925\n",
      "step 3400, train loss: 0.100939\n",
      "step 3500, train loss: 0.112313\n",
      "step 3600, train loss: 0.105901\n",
      "step 3700, train loss: 0.180377\n",
      "step 3800, train loss: 0.137729\n",
      "step 3900, train loss: 0.143123\n",
      "step 4000, train loss: 0.114049\n",
      "step 4100, train loss: 0.110199\n",
      "step 4200, train loss: 0.127564\n",
      "step 4300, train loss: 0.117678\n",
      "step 4400, train loss: 0.112560\n",
      "step 4500, train loss: 0.138116\n",
      "step 4600, train loss: 0.112981\n",
      "step 4700, train loss: 0.141073\n",
      "step 4800, train loss: 0.142425\n",
      "step 4900, train loss: 0.131966\n",
      "step 5000, train loss: 0.121466\n",
      "step 5100, train loss: 0.130486\n",
      "step 5200, train loss: 0.123407\n",
      "step 5300, train loss: 0.114722\n",
      "step 5400, train loss: 0.105812\n",
      "step 5500, train loss: 0.114307\n",
      "step 5600, train loss: 0.104357\n",
      "step 5700, train loss: 0.139306\n",
      "step 5800, train loss: 0.142087\n",
      "step 5900, train loss: 0.120049\n",
      "step 6000, train loss: 0.114167\n",
      "step 6100, train loss: 0.127932\n",
      "step 6200, train loss: 0.125135\n",
      "step 6300, train loss: 0.119158\n",
      "step 6400, train loss: 0.120801\n",
      "step 6500, train loss: 0.118817\n",
      "step 6600, train loss: 0.124224\n",
      "step 6700, train loss: 0.112065\n",
      "step 6800, train loss: 0.197279\n",
      "step 6900, train loss: 0.142808\n",
      "step 7000, train loss: 0.101268\n",
      "step 7100, train loss: 0.158984\n",
      "step 7200, train loss: 0.153328\n",
      "step 7300, train loss: 0.161028\n",
      "step 7400, train loss: 0.152669\n",
      "step 7500, train loss: 0.117019\n",
      "step 7600, train loss: 0.133724\n",
      "step 7700, train loss: 0.128400\n",
      "step 7800, train loss: 0.116122\n",
      "step 7900, train loss: 0.120977\n",
      "step 8000, train loss: 0.112088\n",
      "step 8100, train loss: 0.105592\n",
      "step 8200, train loss: 0.137530\n",
      "step 8300, train loss: 0.147282\n",
      "step 8400, train loss: 0.096007\n",
      "step 8500, train loss: 0.118617\n",
      "step 8600, train loss: 0.133674\n",
      "step 8700, train loss: 0.126643\n",
      "step 8800, train loss: 0.113115\n",
      "step 8900, train loss: 0.120801\n",
      "step 9000, train loss: 0.109841\n",
      "step 9100, train loss: 0.115221\n",
      "step 9200, train loss: 0.121634\n",
      "step 9300, train loss: 0.146807\n",
      "step 9400, train loss: 0.136107\n",
      "step 9500, train loss: 0.119348\n",
      "step 9600, train loss: 0.120888\n",
      "step 9700, train loss: 0.146756\n",
      "step 9800, train loss: 0.117152\n",
      "step 9900, train loss: 0.103420\n",
      "step 10000, train loss: 0.105704\n",
      "step 10100, train loss: 0.113192\n",
      "step 10200, train loss: 0.148792\n",
      "step 10300, train loss: 0.158743\n",
      "step 10400, train loss: 0.088139\n",
      "step 10500, train loss: 0.094575\n",
      "step 10600, train loss: 0.122062\n",
      "step 10700, train loss: 0.116734\n",
      "step 10800, train loss: 0.118267\n",
      "step 10900, train loss: 0.099407\n",
      "step 11000, train loss: 0.103423\n",
      "step 11100, train loss: 0.109240\n",
      "step 11200, train loss: 0.121309\n",
      "step 11300, train loss: 0.108994\n",
      "step 11400, train loss: 0.109431\n",
      "step 11500, train loss: 0.128298\n",
      "step 11600, train loss: 0.109894\n",
      "step 11700, train loss: 0.100197\n",
      "step 11800, train loss: 0.125069\n",
      "step 11900, train loss: 0.099693\n",
      "step 12000, train loss: 0.110266\n",
      "step 12100, train loss: 0.102024\n",
      "step 12200, train loss: 0.104268\n",
      "step 12300, train loss: 0.150524\n",
      "step 12400, train loss: 0.119001\n",
      "step 12500, train loss: 0.117105\n",
      "step 12600, train loss: 0.109912\n",
      "step 12700, train loss: 0.116047\n",
      "step 12800, train loss: 0.118547\n",
      "step 12900, train loss: 0.102879\n",
      "step 13000, train loss: 0.126503\n",
      "step 13100, train loss: 0.118407\n",
      "step 13200, train loss: 0.121802\n",
      "step 13300, train loss: 0.125725\n",
      "step 13400, train loss: 0.111341\n",
      "step 13500, train loss: 0.098846\n",
      "step 13600, train loss: 0.100401\n",
      "step 13700, train loss: 0.100019\n",
      "step 13800, train loss: 0.120168\n",
      "step 13900, train loss: 0.105203\n",
      "step 14000, train loss: 0.115010\n",
      "step 14100, train loss: 0.121133\n",
      "step 14200, train loss: 0.114303\n",
      "step 14300, train loss: 0.141202\n",
      "step 14400, train loss: 0.101161\n",
      "step 14500, train loss: 0.097085\n",
      "step 14600, train loss: 0.141982\n",
      "step 14700, train loss: 0.119374\n",
      "step 14800, train loss: 0.123326\n",
      "step 14900, train loss: 0.121426\n",
      "step 15000, train loss: 0.113181\n",
      "step 15100, train loss: 0.113018\n",
      "step 15200, train loss: 0.118192\n",
      "step 15300, train loss: 0.145814\n",
      "step 15400, train loss: 0.102555\n",
      "step 15500, train loss: 0.143078\n",
      "step 15600, train loss: 0.145611\n",
      "step 15700, train loss: 0.105391\n",
      "step 15800, train loss: 0.125325\n",
      "step 15900, train loss: 0.143661\n",
      "step 16000, train loss: 0.136510\n",
      "step 16100, train loss: 0.164442\n",
      "step 16200, train loss: 0.136037\n",
      "step 16300, train loss: 0.109969\n",
      "step 16400, train loss: 0.127215\n",
      "step 16500, train loss: 0.101758\n",
      "step 16600, train loss: 0.088241\n",
      "step 16700, train loss: 0.100964\n",
      "step 16800, train loss: 0.108655\n",
      "step 16900, train loss: 0.119212\n",
      "step 17000, train loss: 0.134066\n",
      "step 17100, train loss: 0.111845\n",
      "step 17200, train loss: 0.104966\n",
      "step 17300, train loss: 0.097619\n",
      "step 17400, train loss: 0.125425\n",
      "step 17500, train loss: 0.103039\n",
      "step 17600, train loss: 0.110735\n",
      "step 17700, train loss: 0.089105\n",
      "step 17800, train loss: 0.096078\n",
      "step 17900, train loss: 0.137112\n",
      "step 18000, train loss: 0.116039\n",
      "step 18100, train loss: 0.119320\n",
      "step 18200, train loss: 0.110153\n",
      "step 18300, train loss: 0.101400\n",
      "step 18400, train loss: 0.100762\n",
      "step 18500, train loss: 0.144666\n",
      "step 18600, train loss: 0.133415\n",
      "step 18700, train loss: 0.111979\n",
      "step 18800, train loss: 0.087548\n",
      "step 18900, train loss: 0.095718\n",
      "step 19000, train loss: 0.100635\n",
      "step 19100, train loss: 0.098575\n",
      "step 19200, train loss: 0.117532\n",
      "step 19300, train loss: 0.104443\n",
      "step 19400, train loss: 0.107238\n",
      "step 19500, train loss: 0.122159\n",
      "step 19600, train loss: 0.149206\n",
      "step 19700, train loss: 0.144046\n",
      "step 19800, train loss: 0.115315\n",
      "step 19900, train loss: 0.112182\n",
      "step 20000, train loss: 0.107601\n",
      "step 20100, train loss: 0.099956\n",
      "step 20200, train loss: 0.104951\n",
      "step 20300, train loss: 0.122393\n",
      "step 20400, train loss: 0.110493\n",
      "step 20500, train loss: 0.160551\n",
      "step 20600, train loss: 0.100502\n",
      "step 20700, train loss: 0.115323\n",
      "step 20800, train loss: 0.152009\n",
      "step 20900, train loss: 0.101951\n",
      "step 21000, train loss: 0.127076\n",
      "step 21100, train loss: 0.133182\n",
      "step 21200, train loss: 0.139280\n",
      "step 21300, train loss: 0.100828\n",
      "step 21400, train loss: 0.115644\n",
      "step 21500, train loss: 0.124507\n",
      "step 21600, train loss: 0.128695\n",
      "step 21700, train loss: 0.128037\n",
      "step 21800, train loss: 0.106775\n",
      "step 21900, train loss: 0.139741\n",
      "step 22000, train loss: 0.115501\n",
      "step 22100, train loss: 0.113772\n",
      "step 22200, train loss: 0.145448\n",
      "step 22300, train loss: 0.088203\n",
      "step 22400, train loss: 0.136502\n",
      "step 22500, train loss: 0.116888\n",
      "step 22600, train loss: 0.123354\n",
      "step 22700, train loss: 0.120486\n",
      "step 22800, train loss: 0.145376\n",
      "step 22900, train loss: 0.093862\n",
      "step 23000, train loss: 0.129661\n",
      "step 23100, train loss: 0.126074\n",
      "step 23200, train loss: 0.143368\n",
      "step 23300, train loss: 0.122984\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11faa5bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.171426\n",
      "step 0, train loss: 11.945149\n",
      "step 100, train loss: 0.194747\n",
      "step 200, train loss: 0.128341\n",
      "step 300, train loss: 0.180620\n",
      "step 400, train loss: 0.480232\n",
      "step 500, train loss: 0.120390\n",
      "step 600, train loss: 0.153840\n",
      "step 700, train loss: 0.124008\n",
      "step 800, train loss: 0.122985\n",
      "step 900, train loss: 0.152455\n",
      "step 1000, train loss: 0.118578\n",
      "step 1100, train loss: 0.132838\n",
      "step 1200, train loss: 0.121395\n",
      "step 1300, train loss: 0.122798\n",
      "step 1400, train loss: 0.131325\n",
      "step 1500, train loss: 0.150240\n",
      "step 1600, train loss: 0.137654\n",
      "step 1700, train loss: 0.159353\n",
      "step 1800, train loss: 0.161505\n",
      "step 1900, train loss: 0.112229\n",
      "step 2000, train loss: 0.132513\n",
      "step 2100, train loss: 0.111177\n",
      "step 2200, train loss: 0.113116\n",
      "step 2300, train loss: 0.127969\n",
      "step 2400, train loss: 0.166040\n",
      "step 2500, train loss: 0.132066\n",
      "step 2600, train loss: 0.119497\n",
      "step 2700, train loss: 0.113026\n",
      "step 2800, train loss: 0.103234\n",
      "step 2900, train loss: 0.133381\n",
      "step 3000, train loss: 0.135016\n",
      "step 3100, train loss: 0.141623\n",
      "step 3200, train loss: 0.114799\n",
      "step 3300, train loss: 0.128944\n",
      "step 3400, train loss: 0.172430\n",
      "step 3500, train loss: 0.164908\n",
      "step 3600, train loss: 0.117711\n",
      "step 3700, train loss: 0.133322\n",
      "step 3800, train loss: 0.168475\n",
      "step 3900, train loss: 0.135208\n",
      "step 4000, train loss: 0.115191\n",
      "step 4100, train loss: 0.128302\n",
      "step 4200, train loss: 0.107884\n",
      "step 4300, train loss: 0.123674\n",
      "step 4400, train loss: 0.102720\n",
      "step 4500, train loss: 0.118605\n",
      "step 4600, train loss: 0.110549\n",
      "step 4700, train loss: 0.161088\n",
      "step 4800, train loss: 0.119778\n",
      "step 4900, train loss: 0.127933\n",
      "step 5000, train loss: 0.162178\n",
      "step 5100, train loss: 0.102726\n",
      "step 5200, train loss: 0.125967\n",
      "step 5300, train loss: 0.097404\n",
      "step 5400, train loss: 0.118293\n",
      "step 5500, train loss: 0.112702\n",
      "step 5600, train loss: 0.131867\n",
      "step 5700, train loss: 0.109075\n",
      "step 5800, train loss: 0.129826\n",
      "step 5900, train loss: 0.132670\n",
      "step 6000, train loss: 0.132405\n",
      "step 6100, train loss: 0.128257\n",
      "step 6200, train loss: 0.137484\n",
      "step 6300, train loss: 0.120217\n",
      "step 6400, train loss: 0.112904\n",
      "step 6500, train loss: 0.172161\n",
      "step 6600, train loss: 0.125658\n",
      "step 6700, train loss: 0.142462\n",
      "step 6800, train loss: 0.110003\n",
      "step 6900, train loss: 0.157613\n",
      "step 7000, train loss: 0.130961\n",
      "step 7100, train loss: 0.098064\n",
      "step 7200, train loss: 0.129682\n",
      "step 7300, train loss: 0.124988\n",
      "step 7400, train loss: 0.115234\n",
      "step 7500, train loss: 0.107327\n",
      "step 7600, train loss: 0.116713\n",
      "step 7700, train loss: 0.111700\n",
      "step 7800, train loss: 0.123740\n",
      "step 7900, train loss: 0.114826\n",
      "step 8000, train loss: 0.126988\n",
      "step 8100, train loss: 0.111397\n",
      "step 8200, train loss: 0.121393\n",
      "step 8300, train loss: 0.103320\n",
      "step 8400, train loss: 0.135975\n",
      "step 8500, train loss: 0.109499\n",
      "step 8600, train loss: 0.125794\n",
      "step 8700, train loss: 0.115198\n",
      "step 8800, train loss: 0.126982\n",
      "step 8900, train loss: 0.111285\n",
      "step 9000, train loss: 0.106027\n",
      "step 9100, train loss: 0.087449\n",
      "step 9200, train loss: 0.149176\n",
      "step 9300, train loss: 0.137561\n",
      "step 9400, train loss: 0.106080\n",
      "step 9500, train loss: 0.120523\n",
      "step 9600, train loss: 0.107898\n",
      "step 9700, train loss: 0.131227\n",
      "step 9800, train loss: 0.103697\n",
      "step 9900, train loss: 0.106187\n",
      "step 10000, train loss: 0.128769\n",
      "step 10100, train loss: 0.101950\n",
      "step 10200, train loss: 0.087734\n",
      "step 10300, train loss: 0.098478\n",
      "step 10400, train loss: 0.191777\n",
      "step 10500, train loss: 0.118977\n",
      "step 10600, train loss: 0.125141\n",
      "step 10700, train loss: 0.098731\n",
      "step 10800, train loss: 0.118755\n",
      "step 10900, train loss: 0.101256\n",
      "step 11000, train loss: 0.152947\n",
      "step 11100, train loss: 0.111332\n",
      "step 11200, train loss: 0.096228\n",
      "step 11300, train loss: 0.117649\n",
      "step 11400, train loss: 0.160605\n",
      "step 11500, train loss: 0.138061\n",
      "step 11600, train loss: 0.101938\n",
      "step 11700, train loss: 0.125893\n",
      "step 11800, train loss: 0.130936\n",
      "step 11900, train loss: 0.118620\n",
      "step 12000, train loss: 0.167365\n",
      "step 12100, train loss: 0.140289\n",
      "step 12200, train loss: 0.107628\n",
      "step 12300, train loss: 0.116032\n",
      "step 12400, train loss: 0.124043\n",
      "step 12500, train loss: 0.084874\n",
      "step 12600, train loss: 0.150992\n",
      "step 12700, train loss: 0.116808\n",
      "step 12800, train loss: 0.130551\n",
      "step 12900, train loss: 0.111552\n",
      "step 13000, train loss: 0.129923\n",
      "step 13100, train loss: 0.118669\n",
      "step 13200, train loss: 0.123476\n",
      "step 13300, train loss: 0.130664\n",
      "step 13400, train loss: 0.113308\n",
      "step 13500, train loss: 0.137902\n",
      "step 13600, train loss: 0.166386\n",
      "step 13700, train loss: 0.105843\n",
      "step 13800, train loss: 0.112316\n",
      "step 13900, train loss: 0.106274\n",
      "step 14000, train loss: 0.124626\n",
      "step 14100, train loss: 0.131533\n",
      "step 14200, train loss: 0.129391\n",
      "step 14300, train loss: 0.110752\n",
      "step 14400, train loss: 0.150948\n",
      "step 14500, train loss: 0.180853\n",
      "step 14600, train loss: 0.112788\n",
      "step 14700, train loss: 0.124320\n",
      "step 14800, train loss: 0.136690\n",
      "step 14900, train loss: 0.161309\n",
      "step 15000, train loss: 0.088720\n",
      "step 15100, train loss: 0.107485\n",
      "step 15200, train loss: 0.092445\n",
      "step 15300, train loss: 0.111281\n",
      "step 15400, train loss: 0.099159\n",
      "step 15500, train loss: 0.111976\n",
      "step 15600, train loss: 0.101642\n",
      "step 15700, train loss: 0.118062\n",
      "step 15800, train loss: 0.124233\n",
      "step 15900, train loss: 0.181182\n",
      "step 16000, train loss: 0.120171\n",
      "step 16100, train loss: 0.117792\n",
      "step 16200, train loss: 0.126903\n",
      "step 16300, train loss: 0.129252\n",
      "step 16400, train loss: 0.109049\n",
      "step 16500, train loss: 0.138242\n",
      "step 16600, train loss: 0.108579\n",
      "step 16700, train loss: 0.110068\n",
      "step 16800, train loss: 0.114713\n",
      "step 16900, train loss: 0.102515\n",
      "step 17000, train loss: 0.126957\n",
      "step 17100, train loss: 0.104431\n",
      "step 17200, train loss: 0.154612\n",
      "step 17300, train loss: 0.102851\n",
      "step 17400, train loss: 0.129551\n",
      "step 17500, train loss: 0.119835\n",
      "step 17600, train loss: 0.124433\n",
      "step 17700, train loss: 0.093377\n",
      "step 17800, train loss: 0.134547\n",
      "step 17900, train loss: 0.118375\n",
      "step 18000, train loss: 0.108458\n",
      "step 18100, train loss: 0.108672\n",
      "step 18200, train loss: 0.110733\n",
      "step 18300, train loss: 0.102982\n",
      "step 18400, train loss: 0.104264\n",
      "step 18500, train loss: 0.136637\n",
      "step 18600, train loss: 0.115291\n",
      "step 18700, train loss: 0.108604\n",
      "step 18800, train loss: 0.107625\n",
      "step 18900, train loss: 0.126173\n",
      "step 19000, train loss: 0.129194\n",
      "step 19100, train loss: 0.126377\n",
      "step 19200, train loss: 0.106690\n",
      "step 19300, train loss: 0.128245\n",
      "step 19400, train loss: 0.109177\n",
      "step 19500, train loss: 0.095371\n",
      "step 19600, train loss: 0.102950\n",
      "step 19700, train loss: 0.118496\n",
      "step 19800, train loss: 0.116015\n",
      "step 19900, train loss: 0.129008\n",
      "step 20000, train loss: 0.149884\n",
      "step 20100, train loss: 0.156844\n",
      "step 20200, train loss: 0.124202\n",
      "step 20300, train loss: 0.128658\n",
      "step 20400, train loss: 0.097793\n",
      "step 20500, train loss: 0.115356\n",
      "step 20600, train loss: 0.111028\n",
      "step 20700, train loss: 0.131016\n",
      "step 20800, train loss: 0.107718\n",
      "step 20900, train loss: 0.131942\n",
      "step 21000, train loss: 0.099707\n",
      "step 21100, train loss: 0.120330\n",
      "step 21200, train loss: 0.126144\n",
      "step 21300, train loss: 0.106068\n",
      "step 21400, train loss: 0.158909\n",
      "step 21500, train loss: 0.134821\n",
      "step 21600, train loss: 0.140468\n",
      "step 21700, train loss: 0.109174\n",
      "step 21800, train loss: 0.099682\n",
      "step 21900, train loss: 0.113391\n",
      "step 22000, train loss: 0.124860\n",
      "step 22100, train loss: 0.109710\n",
      "step 22200, train loss: 0.143532\n",
      "step 22300, train loss: 0.092781\n",
      "step 22400, train loss: 0.112240\n",
      "step 22500, train loss: 0.106465\n",
      "step 22600, train loss: 0.139757\n",
      "step 22700, train loss: 0.128340\n",
      "step 22800, train loss: 0.094924\n",
      "step 22900, train loss: 0.148387\n",
      "step 23000, train loss: 0.108684\n",
      "step 23100, train loss: 0.129749\n",
      "step 23200, train loss: 0.135331\n",
      "step 23300, train loss: 0.112848\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fe19a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.162571\n",
      "step 0, train loss: 12.063858\n",
      "step 100, train loss: 0.162381\n",
      "step 200, train loss: 0.207259\n",
      "step 300, train loss: 0.210021\n",
      "step 400, train loss: 0.134055\n",
      "step 500, train loss: 0.131868\n",
      "step 600, train loss: 0.137100\n",
      "step 700, train loss: 0.223233\n",
      "step 800, train loss: 0.171171\n",
      "step 900, train loss: 0.126005\n",
      "step 1000, train loss: 0.166022\n",
      "step 1100, train loss: 0.174076\n",
      "step 1200, train loss: 0.166218\n",
      "step 1300, train loss: 0.103834\n",
      "step 1400, train loss: 0.127297\n",
      "step 1500, train loss: 0.184083\n",
      "step 1600, train loss: 0.145292\n",
      "step 1700, train loss: 0.137035\n",
      "step 1800, train loss: 0.123510\n",
      "step 1900, train loss: 0.122180\n",
      "step 2000, train loss: 0.159772\n",
      "step 2100, train loss: 0.163271\n",
      "step 2200, train loss: 0.151566\n",
      "step 2300, train loss: 0.144381\n",
      "step 2400, train loss: 0.164252\n",
      "step 2500, train loss: 0.124821\n",
      "step 2600, train loss: 0.145619\n",
      "step 2700, train loss: 0.158542\n",
      "step 2800, train loss: 0.135301\n",
      "step 2900, train loss: 0.154223\n",
      "step 3000, train loss: 0.107341\n",
      "step 3100, train loss: 0.109662\n",
      "step 3200, train loss: 0.131147\n",
      "step 3300, train loss: 0.120546\n",
      "step 3400, train loss: 0.172567\n",
      "step 3500, train loss: 0.145835\n",
      "step 3600, train loss: 0.104705\n",
      "step 3700, train loss: 0.118697\n",
      "step 3800, train loss: 0.111846\n",
      "step 3900, train loss: 0.168672\n",
      "step 4000, train loss: 0.142692\n",
      "step 4100, train loss: 0.129966\n",
      "step 4200, train loss: 0.128216\n",
      "step 4300, train loss: 0.155087\n",
      "step 4400, train loss: 0.111103\n",
      "step 4500, train loss: 0.116431\n",
      "step 4600, train loss: 0.142366\n",
      "step 4700, train loss: 0.132687\n",
      "step 4800, train loss: 0.123010\n",
      "step 4900, train loss: 0.155202\n",
      "step 5000, train loss: 0.120771\n",
      "step 5100, train loss: 0.148184\n",
      "step 5200, train loss: 0.133947\n",
      "step 5300, train loss: 0.135452\n",
      "step 5400, train loss: 0.144507\n",
      "step 5500, train loss: 0.141121\n",
      "step 5600, train loss: 0.145001\n",
      "step 5700, train loss: 0.113989\n",
      "step 5800, train loss: 0.163105\n",
      "step 5900, train loss: 0.127857\n",
      "step 6000, train loss: 0.154445\n",
      "step 6100, train loss: 0.121214\n",
      "step 6200, train loss: 0.129341\n",
      "step 6300, train loss: 0.143263\n",
      "step 6400, train loss: 0.108043\n",
      "step 6500, train loss: 0.144877\n",
      "step 6600, train loss: 0.161090\n",
      "step 6700, train loss: 0.136762\n",
      "step 6800, train loss: 0.117752\n",
      "step 6900, train loss: 0.148376\n",
      "step 7000, train loss: 0.134197\n",
      "step 7100, train loss: 0.115913\n",
      "step 7200, train loss: 0.121264\n",
      "step 7300, train loss: 0.094935\n",
      "step 7400, train loss: 0.156619\n",
      "step 7500, train loss: 0.121172\n",
      "step 7600, train loss: 0.111636\n",
      "step 7700, train loss: 0.129737\n",
      "step 7800, train loss: 0.118920\n",
      "step 7900, train loss: 0.139277\n",
      "step 8000, train loss: 0.110792\n",
      "step 8100, train loss: 0.124187\n",
      "step 8200, train loss: 0.141734\n",
      "step 8300, train loss: 0.126107\n",
      "step 8400, train loss: 0.138785\n",
      "step 8500, train loss: 0.125959\n",
      "step 8600, train loss: 0.111363\n",
      "step 8700, train loss: 0.142204\n",
      "step 8800, train loss: 0.132941\n",
      "step 8900, train loss: 0.130699\n",
      "step 9000, train loss: 0.168269\n",
      "step 9100, train loss: 0.128555\n",
      "step 9200, train loss: 0.118361\n",
      "step 9300, train loss: 0.158893\n",
      "step 9400, train loss: 0.113432\n",
      "step 9500, train loss: 0.107261\n",
      "step 9600, train loss: 0.153244\n",
      "step 9700, train loss: 0.164734\n",
      "step 9800, train loss: 0.125529\n",
      "step 9900, train loss: 0.156040\n",
      "step 10000, train loss: 0.105689\n",
      "step 10100, train loss: 0.155618\n",
      "step 10200, train loss: 0.115737\n",
      "step 10300, train loss: 0.134794\n",
      "step 10400, train loss: 0.109580\n",
      "step 10500, train loss: 0.116550\n",
      "step 10600, train loss: 0.116423\n",
      "step 10700, train loss: 0.147393\n",
      "step 10800, train loss: 0.096416\n",
      "step 10900, train loss: 0.119673\n",
      "step 11000, train loss: 0.104913\n",
      "step 11100, train loss: 0.090034\n",
      "step 11200, train loss: 0.108493\n",
      "step 11300, train loss: 0.155973\n",
      "step 11400, train loss: 0.147281\n",
      "step 11500, train loss: 0.129445\n",
      "step 11600, train loss: 0.179217\n",
      "step 11700, train loss: 0.110000\n",
      "step 11800, train loss: 0.132492\n",
      "step 11900, train loss: 0.123143\n",
      "step 12000, train loss: 0.136385\n",
      "step 12100, train loss: 0.124655\n",
      "step 12200, train loss: 0.121838\n",
      "step 12300, train loss: 0.147799\n",
      "step 12400, train loss: 0.133610\n",
      "step 12500, train loss: 0.120048\n",
      "step 12600, train loss: 0.165487\n",
      "step 12700, train loss: 0.162945\n",
      "step 12800, train loss: 0.122710\n",
      "step 12900, train loss: 0.141069\n",
      "step 13000, train loss: 0.149965\n",
      "step 13100, train loss: 0.106075\n",
      "step 13200, train loss: 0.118343\n",
      "step 13300, train loss: 0.111984\n",
      "step 13400, train loss: 0.138650\n",
      "step 13500, train loss: 0.123278\n",
      "step 13600, train loss: 0.114806\n",
      "step 13700, train loss: 0.125258\n",
      "step 13800, train loss: 0.123226\n",
      "step 13900, train loss: 0.115543\n",
      "step 14000, train loss: 0.092991\n",
      "step 14100, train loss: 0.112250\n",
      "step 14200, train loss: 0.181850\n",
      "step 14300, train loss: 0.091751\n",
      "step 14400, train loss: 0.132112\n",
      "step 14500, train loss: 0.113175\n",
      "step 14600, train loss: 0.119097\n",
      "step 14700, train loss: 0.138913\n",
      "step 14800, train loss: 0.127765\n",
      "step 14900, train loss: 0.119231\n",
      "step 15000, train loss: 0.135002\n",
      "step 15100, train loss: 0.146508\n",
      "step 15200, train loss: 0.117543\n",
      "step 15300, train loss: 0.108777\n",
      "step 15400, train loss: 0.168585\n",
      "step 15500, train loss: 0.128118\n",
      "step 15600, train loss: 0.104350\n",
      "step 15700, train loss: 0.142463\n",
      "step 15800, train loss: 0.146719\n",
      "step 15900, train loss: 0.131889\n",
      "step 16000, train loss: 0.102376\n",
      "step 16100, train loss: 0.137489\n",
      "step 16200, train loss: 0.141210\n",
      "step 16300, train loss: 0.139419\n",
      "step 16400, train loss: 0.107524\n",
      "step 16500, train loss: 0.117860\n",
      "step 16600, train loss: 0.144945\n",
      "step 16700, train loss: 0.116292\n",
      "step 16800, train loss: 0.131891\n",
      "step 16900, train loss: 0.132747\n",
      "step 17000, train loss: 0.147178\n",
      "step 17100, train loss: 0.154449\n",
      "step 17200, train loss: 0.113140\n",
      "step 17300, train loss: 0.120930\n",
      "step 17400, train loss: 0.136143\n",
      "step 17500, train loss: 0.114750\n",
      "step 17600, train loss: 0.111843\n",
      "step 17700, train loss: 0.141854\n",
      "step 17800, train loss: 0.121177\n",
      "step 17900, train loss: 0.128418\n",
      "step 18000, train loss: 0.163735\n",
      "step 18100, train loss: 0.124746\n",
      "step 18200, train loss: 0.129670\n",
      "step 18300, train loss: 0.108735\n",
      "step 18400, train loss: 0.100615\n",
      "step 18500, train loss: 0.126041\n",
      "step 18600, train loss: 0.123862\n",
      "step 18700, train loss: 0.098091\n",
      "step 18800, train loss: 0.126347\n",
      "step 18900, train loss: 0.133326\n",
      "step 19000, train loss: 0.138227\n",
      "step 19100, train loss: 0.113286\n",
      "step 19200, train loss: 0.106743\n",
      "step 19300, train loss: 0.139160\n",
      "step 19400, train loss: 0.118265\n",
      "step 19500, train loss: 0.110966\n",
      "step 19600, train loss: 0.131900\n",
      "step 19700, train loss: 0.108274\n",
      "step 19800, train loss: 0.123834\n",
      "step 19900, train loss: 0.106855\n",
      "step 20000, train loss: 0.128398\n",
      "step 20100, train loss: 0.106927\n",
      "step 20200, train loss: 0.099765\n",
      "step 20300, train loss: 0.104985\n",
      "step 20400, train loss: 0.111723\n",
      "step 20500, train loss: 0.126377\n",
      "step 20600, train loss: 0.146243\n",
      "step 20700, train loss: 0.098832\n",
      "step 20800, train loss: 0.151309\n",
      "step 20900, train loss: 0.100215\n",
      "step 21000, train loss: 0.126190\n",
      "step 21100, train loss: 0.089833\n",
      "step 21200, train loss: 0.125399\n",
      "step 21300, train loss: 0.130681\n",
      "step 21400, train loss: 0.110037\n",
      "step 21500, train loss: 0.123995\n",
      "step 21600, train loss: 0.147738\n",
      "step 21700, train loss: 0.115244\n",
      "step 21800, train loss: 0.115720\n",
      "step 21900, train loss: 0.114458\n",
      "step 22000, train loss: 0.117785\n",
      "step 22100, train loss: 0.119487\n",
      "step 22200, train loss: 0.129358\n",
      "step 22300, train loss: 0.119404\n",
      "step 22400, train loss: 0.109390\n",
      "step 22500, train loss: 0.117509\n",
      "step 22600, train loss: 0.131174\n",
      "step 22700, train loss: 0.116833\n",
      "step 22800, train loss: 0.153288\n",
      "step 22900, train loss: 0.125038\n",
      "step 23000, train loss: 0.137360\n",
      "step 23100, train loss: 0.128723\n",
      "step 23200, train loss: 0.149708\n",
      "step 23300, train loss: 0.145723\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fefe210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.150814\n",
      "step 0, train loss: 12.000143\n",
      "step 100, train loss: 0.147951\n",
      "step 200, train loss: 0.206299\n",
      "step 300, train loss: 1.066505\n",
      "step 400, train loss: 0.124806\n",
      "step 500, train loss: 0.108524\n",
      "step 600, train loss: 0.124775\n",
      "step 700, train loss: 0.116571\n",
      "step 800, train loss: 0.115733\n",
      "step 900, train loss: 0.144928\n",
      "step 1000, train loss: 0.131320\n",
      "step 1100, train loss: 0.127065\n",
      "step 1200, train loss: 0.124333\n",
      "step 1300, train loss: 0.154700\n",
      "step 1400, train loss: 0.125209\n",
      "step 1500, train loss: 0.141372\n",
      "step 1600, train loss: 0.121844\n",
      "step 1700, train loss: 0.127371\n",
      "step 1800, train loss: 0.128685\n",
      "step 1900, train loss: 0.122759\n",
      "step 2000, train loss: 0.107953\n",
      "step 2100, train loss: 0.113234\n",
      "step 2200, train loss: 0.109282\n",
      "step 2300, train loss: 0.093187\n",
      "step 2400, train loss: 0.122329\n",
      "step 2500, train loss: 0.110884\n",
      "step 2600, train loss: 0.139881\n",
      "step 2700, train loss: 0.098876\n",
      "step 2800, train loss: 0.161091\n",
      "step 2900, train loss: 0.121864\n",
      "step 3000, train loss: 0.128922\n",
      "step 3100, train loss: 0.107054\n",
      "step 3200, train loss: 0.153219\n",
      "step 3300, train loss: 0.152509\n",
      "step 3400, train loss: 0.112038\n",
      "step 3500, train loss: 0.114901\n",
      "step 3600, train loss: 0.133872\n",
      "step 3700, train loss: 0.118211\n",
      "step 3800, train loss: 0.145657\n",
      "step 3900, train loss: 0.100706\n",
      "step 4000, train loss: 0.105012\n",
      "step 4100, train loss: 0.115845\n",
      "step 4200, train loss: 0.104435\n",
      "step 4300, train loss: 0.117973\n",
      "step 4400, train loss: 0.103075\n",
      "step 4500, train loss: 0.111690\n",
      "step 4600, train loss: 0.120271\n",
      "step 4700, train loss: 0.089705\n",
      "step 4800, train loss: 0.140547\n",
      "step 4900, train loss: 0.125911\n",
      "step 5000, train loss: 0.110537\n",
      "step 5100, train loss: 0.119655\n",
      "step 5200, train loss: 0.119119\n",
      "step 5300, train loss: 0.120594\n",
      "step 5400, train loss: 0.118370\n",
      "step 5500, train loss: 0.160657\n",
      "step 5600, train loss: 0.159522\n",
      "step 5700, train loss: 0.084477\n",
      "step 5800, train loss: 0.105631\n",
      "step 5900, train loss: 0.131965\n",
      "step 6000, train loss: 0.172200\n",
      "step 6100, train loss: 0.121393\n",
      "step 6200, train loss: 0.110776\n",
      "step 6300, train loss: 0.124113\n",
      "step 6400, train loss: 0.110780\n",
      "step 6500, train loss: 0.141776\n",
      "step 6600, train loss: 0.101371\n",
      "step 6700, train loss: 0.122041\n",
      "step 6800, train loss: 0.094943\n",
      "step 6900, train loss: 0.123498\n",
      "step 7000, train loss: 0.089742\n",
      "step 7100, train loss: 0.144841\n",
      "step 7200, train loss: 0.115107\n",
      "step 7300, train loss: 0.090853\n",
      "step 7400, train loss: 0.101183\n",
      "step 7500, train loss: 0.117398\n",
      "step 7600, train loss: 0.107600\n",
      "step 7700, train loss: 0.112251\n",
      "step 7800, train loss: 0.122995\n",
      "step 7900, train loss: 0.117783\n",
      "step 8000, train loss: 0.120422\n",
      "step 8100, train loss: 0.160472\n",
      "step 8200, train loss: 0.095712\n",
      "step 8300, train loss: 0.090343\n",
      "step 8400, train loss: 0.089184\n",
      "step 8500, train loss: 0.127950\n",
      "step 8600, train loss: 0.151316\n",
      "step 8700, train loss: 0.128886\n",
      "step 8800, train loss: 0.126151\n",
      "step 8900, train loss: 0.124228\n",
      "step 9000, train loss: 0.110702\n",
      "step 9100, train loss: 0.139832\n",
      "step 9200, train loss: 0.101657\n",
      "step 9300, train loss: 0.102651\n",
      "step 9400, train loss: 0.145363\n",
      "step 9500, train loss: 0.136668\n",
      "step 9600, train loss: 0.109597\n",
      "step 9700, train loss: 0.112473\n",
      "step 9800, train loss: 0.107554\n",
      "step 9900, train loss: 0.105601\n",
      "step 10000, train loss: 0.119087\n",
      "step 10100, train loss: 0.120727\n",
      "step 10200, train loss: 0.111650\n",
      "step 10300, train loss: 0.120746\n",
      "step 10400, train loss: 0.115465\n",
      "step 10500, train loss: 0.115360\n",
      "step 10600, train loss: 0.128879\n",
      "step 10700, train loss: 0.126610\n",
      "step 10800, train loss: 0.127075\n",
      "step 10900, train loss: 0.138783\n",
      "step 11000, train loss: 0.096158\n",
      "step 11100, train loss: 0.100644\n",
      "step 11200, train loss: 0.105142\n",
      "step 11300, train loss: 0.084151\n",
      "step 11400, train loss: 0.145196\n",
      "step 11500, train loss: 0.153017\n",
      "step 11600, train loss: 0.128967\n",
      "step 11700, train loss: 0.134948\n",
      "step 11800, train loss: 0.154185\n",
      "step 11900, train loss: 0.131123\n",
      "step 12000, train loss: 0.122635\n",
      "step 12100, train loss: 0.108172\n",
      "step 12200, train loss: 0.114815\n",
      "step 12300, train loss: 0.135689\n",
      "step 12400, train loss: 0.124313\n",
      "step 12500, train loss: 0.095171\n",
      "step 12600, train loss: 0.097169\n",
      "step 12700, train loss: 0.116649\n",
      "step 12800, train loss: 0.127137\n",
      "step 12900, train loss: 0.145016\n",
      "step 13000, train loss: 0.123543\n",
      "step 13100, train loss: 0.139991\n",
      "step 13200, train loss: 0.116007\n",
      "step 13300, train loss: 0.102217\n",
      "step 13400, train loss: 0.096594\n",
      "step 13500, train loss: 0.120219\n",
      "step 13600, train loss: 0.093129\n",
      "step 13700, train loss: 0.125025\n",
      "step 13800, train loss: 0.109282\n",
      "step 13900, train loss: 0.102467\n",
      "step 14000, train loss: 0.096446\n",
      "step 14100, train loss: 0.091639\n",
      "step 14200, train loss: 0.088610\n",
      "step 14300, train loss: 0.110813\n",
      "step 14400, train loss: 0.093566\n",
      "step 14500, train loss: 0.095269\n",
      "step 14600, train loss: 0.139872\n",
      "step 14700, train loss: 0.093027\n",
      "step 14800, train loss: 0.088679\n",
      "step 14900, train loss: 0.131579\n",
      "step 15000, train loss: 0.128996\n",
      "step 15100, train loss: 0.123477\n",
      "step 15200, train loss: 0.113185\n",
      "step 15300, train loss: 0.113399\n",
      "step 15400, train loss: 0.134983\n",
      "step 15500, train loss: 0.116299\n",
      "step 15600, train loss: 0.158124\n",
      "step 15700, train loss: 0.125809\n",
      "step 15800, train loss: 0.107302\n",
      "step 15900, train loss: 0.101614\n",
      "step 16000, train loss: 0.146504\n",
      "step 16100, train loss: 0.145995\n",
      "step 16200, train loss: 0.115232\n",
      "step 16300, train loss: 0.111604\n",
      "step 16400, train loss: 0.075424\n",
      "step 16500, train loss: 0.094646\n",
      "step 16600, train loss: 0.093348\n",
      "step 16700, train loss: 0.118106\n",
      "step 16800, train loss: 0.139460\n",
      "step 16900, train loss: 0.111614\n",
      "step 17000, train loss: 0.136130\n",
      "step 17100, train loss: 0.097528\n",
      "step 17200, train loss: 0.099180\n",
      "step 17300, train loss: 0.108525\n",
      "step 17400, train loss: 0.140682\n",
      "step 17500, train loss: 0.114689\n",
      "step 17600, train loss: 0.142732\n",
      "step 17700, train loss: 0.105128\n",
      "step 17800, train loss: 0.106170\n",
      "step 17900, train loss: 0.141008\n",
      "step 18000, train loss: 0.112774\n",
      "step 18100, train loss: 0.098390\n",
      "step 18200, train loss: 0.112608\n",
      "step 18300, train loss: 0.136559\n",
      "step 18400, train loss: 0.112113\n",
      "step 18500, train loss: 0.117794\n",
      "step 18600, train loss: 0.104454\n",
      "step 18700, train loss: 0.116619\n",
      "step 18800, train loss: 0.107749\n",
      "step 18900, train loss: 0.145811\n",
      "step 19000, train loss: 0.118009\n",
      "step 19100, train loss: 0.088116\n",
      "step 19200, train loss: 0.099981\n",
      "step 19300, train loss: 0.122223\n",
      "step 19400, train loss: 0.098299\n",
      "step 19500, train loss: 0.113267\n",
      "step 19600, train loss: 0.114276\n",
      "step 19700, train loss: 0.094267\n",
      "step 19800, train loss: 0.104132\n",
      "step 19900, train loss: 0.096433\n",
      "step 20000, train loss: 0.102866\n",
      "step 20100, train loss: 0.109503\n",
      "step 20200, train loss: 0.105345\n",
      "step 20300, train loss: 0.126952\n",
      "step 20400, train loss: 0.157086\n",
      "step 20500, train loss: 0.129729\n",
      "step 20600, train loss: 0.121355\n",
      "step 20700, train loss: 0.137211\n",
      "step 20800, train loss: 0.105973\n",
      "step 20900, train loss: 0.103272\n",
      "step 21000, train loss: 0.096779\n",
      "step 21100, train loss: 0.112616\n",
      "step 21200, train loss: 0.102350\n",
      "step 21300, train loss: 0.102547\n",
      "step 21400, train loss: 0.099746\n",
      "step 21500, train loss: 0.115996\n",
      "step 21600, train loss: 0.119691\n",
      "step 21700, train loss: 0.100143\n",
      "step 21800, train loss: 0.098604\n",
      "step 21900, train loss: 0.102130\n",
      "step 22000, train loss: 0.141305\n",
      "step 22100, train loss: 0.113938\n",
      "step 22200, train loss: 0.129610\n",
      "step 22300, train loss: 0.091957\n",
      "step 22400, train loss: 0.141128\n",
      "step 22500, train loss: 0.095997\n",
      "step 22600, train loss: 0.125381\n",
      "step 22700, train loss: 0.098445\n",
      "step 22800, train loss: 0.113010\n",
      "step 22900, train loss: 0.092665\n",
      "step 23000, train loss: 0.108604\n",
      "step 23100, train loss: 0.131692\n",
      "step 23200, train loss: 0.096939\n",
      "step 23300, train loss: 0.094886\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAHBVJREFUeJzt3WtwXHeZ5/Hv0xd162ZZt/hKxo4nhNyGJIiMAyy3TCZOmCWhmMoCldmwy5Z5MWQzW0MgKW7D7otN1e4yDLVLqABesguEpRIomCHsOGTiCiyBRHEyiR3bcW6OZTuWLNu6S3179kW3ZFlWdzvdLbX/0u9Tpep7n+cct34+evr/P8fcHRERCV+k3gWIiEhtKNBFRJYIBbqIyBKhQBcRWSIU6CIiS4QCXURkiVCgi4gsEQp0EZElQoEuIrJExBZzYV1dXb5hw4bFXKSISPCefvrpY+7eXe55ixroGzZsoLe3dzEXKSISPDM7cDbPU8tFRGSJUKCLiCwRCnQRkSViUXvoIiJvVjqdpq+vj8nJyXqXsuCSySTr168nHo9X9HoFuoic0/r6+mhtbWXDhg2YWb3LWTDuzuDgIH19fWzcuLGi9yjbcjGzbWbWb2a7Zt33X8xsr5k9Z2Y/NbOVFS1dRKSMyclJOjs7l3SYA5gZnZ2dVf0lcjY99O8BW+bc9whwmbv/EfAicHfFFYiIlLHUw3xatetZNtDd/XHg+Jz7trt7pnDzd8D6qqoo49E9R/nmjpcWchEiIsGrxSiXfwv8stiDZrbVzHrNrHdgYKCiBezYN8B3fv1qpfWJiFTl5MmTfPOb33zTr7vxxhs5efLkAlQ0v6oC3cy+AGSAHxR7jrvf5+497t7T3V125uq8IgY5ncxaROqkWKBnMpl5nn3Kww8/zMqVi/cVY8WjXMzsk8CfAde6L2zamhm5nAJdROrjrrvu4uWXX+aKK64gHo+TTCZpb29n7969vPjii9x8880cPHiQyclJ7rjjDrZu3QqcOtzJ6OgoN9xwA+95z3v47W9/y7p16/jZz35GY2NjTeusKNDNbAvwOeB97j5e04rmXR4ozkXkq3+/mxcOD9f0PS9Zu4Kv/MtLSz7nnnvuYdeuXTz77LPs2LGDD33oQ+zatWtmeOG2bdvo6OhgYmKCd77znXz0ox+ls7PztPfYv38/DzzwAN/+9re55ZZbeOihh7j11ltrui5nM2zxAeAJ4CIz6zOzTwH/HWgFHjGzZ83sWzWtam6RZqjjIiLniquvvvq0seLf+MY3ePvb387mzZs5ePAg+/fvP+M1Gzdu5IorrgDgHe94B6+99lrN6yq7h+7uH5/n7u/WvJIS1EMXEaDsnvRiaW5unrm+Y8cOfvWrX/HEE0/Q1NTE+9///nnHkicSiZnr0WiUiYmJmtcVxLFczEyBLiJ109raysjIyLyPDQ0N0d7eTlNTE3v37uV3v/vdIld3ShBT/81Qy0VE6qazs5N3v/vdXHbZZTQ2NrJq1aqZx7Zs2cK3vvUtLr74Yi666CI2b95ctzqDCHT10EWk3n74wx/Oe38ikeCXv5x/Ks50n7yrq4tdu2aOnsJnP/vZmtcHobRcUA9dRKScIAI9YqZhiyIiZQQS6NpDFxEpJ4hARz10EZGyggj0SOGIkgt8hAERkaAFEuj5RNfhXEREigsk0POX6qOLSD1UevhcgK9//euMjy/4Ia+AQALdZvbQFegisvhCCfQgJhbZTA+9vnWIyPI0+/C51113Heeddx4//vGPmZqa4iMf+Qhf/epXGRsb45ZbbqGvr49sNsuXvvQljh49yuHDh/nABz5AV1cXjz322ILWGUSgT/fQFegiy9wv74I3nq/te66+HG64p+RTZh8+d/v27Tz44IM8+eSTuDsf/vCHefzxxxkYGGDt2rX84he/APLHeGlra+NrX/sajz32GF1dXbWtex5htFwKl2q5iEi9bd++ne3bt3PllVdy1VVXsXfvXvbv38/ll1/OI488wuc//3l+/etf09bWtui1hbWHXuc6RKTOyuxJLwZ35+677+bTn/70GY/t3LmThx9+mC9+8Ytce+21fPnLX17U2sLYQ9coFxGpo9mHz73++uvZtm0bo6OjABw6dIj+/n4OHz5MU1MTt956K3feeSc7d+4847ULLYg99OlRLp6rcyEisizNPnzuDTfcwCc+8QmuueYaAFpaWvj+97/PSy+9xJ133kkkEiEej3PvvfcCsHXrVrZs2cLatWv1pSjMmimqpouI1Mncw+fecccdp93etGkT119//Rmvu/3227n99tsXtLZpQbRcNFNURKS8QAI9f6keuohIcUEEOpopKrKsLZcD81W7nkEE+vQeulroIstPMplkcHBwyYe6uzM4OEgymaz4PQL5UlQ9dJHlav369fT19TEwMFDvUhZcMplk/fr1Fb8+iEDXTFGR5Ssej7Nx48Z6lxGEsi0XM9tmZv1mtmvWfR1m9oiZ7S9cti9okZopKiJS1tn00L8HbJlz313Ao+5+IfBo4faCmZkpqp6LiEhRZQPd3R8Hjs+5+ybg/sL1+4Gba1zXaUxHWxQRKavSUS6r3P1I4fobwKpiTzSzrWbWa2a9lX6poZmiIiLlVT1s0fNjiYomrbvf5+497t7T3d1d0TI0ykVEpLxKA/2oma0BKFz2166kM+loiyIi5VUa6D8Hbitcvw34WW3Kmd+pHroCXUSkmLMZtvgA8ARwkZn1mdmngHuA68xsP/AnhdsLV6TOKSoiUlbZiUXu/vEiD11b41qKUg9dRKS8II7lopmiIiLlhRHoGocuIlJWEIGu46GLiJQXRKBrD11EpLwgAl0zRUVEygsk0DXKRUSknCACHfXQRUTKCiLQI5opKiJSViCBnr9UnouIFBdIoKuHLiJSThCBrpmiIiLlhRHoGocuIlJWEIF+qoeuRBcRKSaIQDf10EVEygoi0DVTVESkvCACXXvoIiLlBRLo+UuNchERKS6IQJ8eh66Oi4hIcYEEev5Se+giIsUFEujqoYuIlBNEoE/THrqISHFBBHpEM0VFRMoKI9ALVWqmqIhIcUEEuqEeuohIOVUFupn9BzPbbWa7zOwBM0vWqrDZNFNURKS8igPdzNYB/x7ocffLgCjwsVoVNmdZgPbQRURKqbblEgMazSwGNAGHqy/pTKajLYqIlFVxoLv7IeC/Aq8DR4Ahd99eq8Jm0ygXEZHyqmm5tAM3ARuBtUCzmd06z/O2mlmvmfUODAxUVqRmioqIlFVNy+VPgFfdfcDd08BPgHfNfZK73+fuPe7e093dXVmR6qGLiJRVTaC/Dmw2sybLf2t5LbCnNmXNT3voIiLFVdND/z3wILATeL7wXvfVqK7TRCI62qKISDmxal7s7l8BvlKjWopSD11EpDzNFBURWSKCCHTNFBURKS+IQNdMURGR8gIJ9PylZoqKiBQXRKBrpqiISHmBBHr+UqNcRESKCyLQ1UMXESkvkEDPX6qHLiJSXBCBrh66iEh5gQR6/lI9dBGR4oIIdM0UFREpL4xA10xREZGyggh09dBFRMoLItCn99Bz6rmIiBQVRKDP7KHXuQ4RkXNZIIGev9QoFxGR4oIIdM0UFREpL4hAh3wfXTNFRUSKCybQI2Ya5SIiUkJAga4euohIKcEEumHqoYuIlBBOoJtmioqIlBJMoKuHLiJSWjCBbqaZoiIipVQV6Ga20sweNLO9ZrbHzK6pVWFzRczUcBERKSFW5ev/Dvi/7v7nZtYANNWgpnmZRrmIiJRUcaCbWRvwXuCTAO6eAlK1KetM6qGLiJRWTctlIzAA/E8ze8bMvmNmzTWq6wzaQxcRKa2aQI8BVwH3uvuVwBhw19wnmdlWM+s1s96BgYGKF6Y9dBGR0qoJ9D6gz91/X7j9IPmAP4273+fuPe7e093dXfHCNFNURKS0igPd3d8ADprZRYW7rgVeqElV89JMURGRUqod5XI78IPCCJdXgH9TfUnzyx8TXYkuIlJMVYHu7s8CPTWqpaSIGbncYixJRCRMYc0UVQ9dRKSoYAJdM0VFREoLJtC1hy4iUlpQga48FxEpLphAj5hpD11EpISgAl15LiJSXDCBrh66iEhp4QQ66qGLiJQSTKDnhy0q0UVEigkq0DVTVESkuGACXT10EZHSAgp0zRQVESklmECPGLj20EVEigom0PMtl3pXISJy7gom0DVTVESktGAC3TRTVESkpGACXecUFREpLZhA10xREZHSggl0zRQVESktqEDXTFERkeKCCXTUQxcRKSmYQI8YariIiJQQUKCbZoqKiJQQTKBrpqiISGlVB7qZRc3sGTP7h1oUVIz20EVESqvFHvodwJ4avE9JZqY9dBGREqoKdDNbD3wI+E5tyilOR1sUESmt2j30rwOfAxZ8hLihHrqISCkVB7qZ/RnQ7+5Pl3neVjPrNbPegYGBShenmaIiImVUs4f+buDDZvYa8CPgg2b2/blPcvf73L3H3Xu6u7srXphppqiISEkVB7q73+3u6919A/Ax4J/c/daaVTaHzikqIlJaMOPQI1bvCkREzm2xWryJu+8AdtTivYrRGYtEREoLZg9dM0VFREoLKNA1U1REpJRgAj2ic4qKiJQUUKBrlIuISCnBBLpmioqIlBZMoGumqIhIacEEumaKioiUFlCg62iLIiKlBBPoOqeoiEhpAQW6ZoqKiJQSTKBrpqiISGkBBbomFomIlBJMoOsUdCIipQUU6Oqhi4iUEkyga6aoiEhp4QS6jrYoIlJSMIGuoy2KiJQWTKDrnKIiIqUFE+iaKSoiUlpAga5RLiIipQQT6GimqIhIScEEesTUcxERKSWgQNeXoiIipQQU6Oqhi4iUUnGgm9lbzOwxM3vBzHab2R21LOyM5aEeuohIKbEqXpsB/trdd5pZK/C0mT3i7i/UqLbTmBmQP0DX9HURETml4j10dz/i7jsL10eAPcC6WhU2V2Qm0BdqCSIiYatJD93MNgBXAr+vxfvNv4z8pfroIiLzqzrQzawFeAj4K3cfnufxrWbWa2a9AwMDFS8nUgh0xbmIyPyqCnQzi5MP8x+4+0/me4673+fuPe7e093dXc2yAO2hi4gUU80oFwO+C+xx96/VrqRiy8tfKs9FROZXzR76u4G/AD5oZs8Wfm6sUV1n0JeiIiKlVTxs0d1/Q354+KKI6EtREZGSgpopCgp0EZFiggn0aZotKiIyv2ACPWIatygiUkpAgZ6/VMtFRGR+wQS6xqGLiJQWTKBrpqiISGnBBLr20EVESgsj0H/zt1z71KcBTSwSESkmjEAfH6T7xDOAAl1EpJgwAr2pk1hukiRT5NwZnkwzMDJV76pERM4pgQR6FwAdjJBz5z/+/Qt86v6n6lyUiMi5JZBA7wSgw4ZxhwODY7x+fLzORYmInFsCC/QR3GFgZIqT42ky2VydCxMROXcEFejthZbLsdEUAMfHU/WsSkTknBJGoDef2kMfT2UZncoAcHxMgS4iMi2MQE+0kbMoHTZC/8jkzN3HRxXoIiLTwgj0SIR0w0o6GKF/1nDFY9pDFxGZEUagA6mGlbTbyGnjz4+Paiy6iMi0cAI90Z5vuQyfarkMag9dRGRGMIGeTnTQwfBMy2VFMqZAFxGZpeKTRC+2VKKddsv30Nub4nS1JPSlqIjILMHsoWcS7bQzysDwOF0tCTpbGhgcUw9dRGRaMIGeTnQQsxzjIyfobk3Q2ZxQy0VEZJZgAj2T7ABgRfYkXS0JOpobGFTLRURkRjA99KmmNQD8n4b/xED/VYw3rcdSETK9R4jFkxCJlnx9OpcjnXWa4vM8r3A2pDfj6PAkbU0JkrFT/yemsjlGJjM0NkTnX04Vyyu8cN57hybT9A9P0tbYwHmtiZn7x1JZ9r0xzEWr22hOlN4+b2Z5xQxNpIkYtDY2nPHYVCZLIlauhkq3y2zzHTDfwCKFn1PLmMxkSWdytCbjAGSzWcbTOVoTc38tShyEv+gB+n3W4/NcP6M2K1wvsg3Kngjg1ONjUxkwaG44fT2yuSwRz2G5zOn1zLy3z3Mf+d8ti+Zrm77uWchlC6tQWIfZ63La+3iR5TFn3edsh5n3nD7/ZC6/TM/ll+85yOXylzhEYhyfyLCiKUksMt++arltaKfWZ77bM/Xnzrx+2r/trJpnr8/574LWVWVqqE5VgW5mW4C/A6LAd9z9nppUNY/x1X/MJ1Of45boDq7z14kc/g098TT8w/1n9fp44adW5vtnaQA6a7iMs9VW+JmrGbhqkesoJlHisXpJFn6mRYHWOtVSS81F7q/kv/TQdNS7gBIm/9WPSV58/YIuo+JAN7Mo8D+A64A+4Ckz+7m7v1Cr4mbr2djBGx+9jfdd9nniiRh7Dp/ks/f/EyeGRlgRz9HcYAxPpAG48vyVdDYnODI8yVQ6y0Qqy6bzWjivNcE/HzzJ6pWNHB2a5PhYisZ4hIZYhMl0lqNDU6RyZx7BsSEaIZXNsao1gRm8MTzF+9/axVQ6x+snxnGHI0MTXLp2Bddfspp9R4f5x11HybjT3ZKgsyXB0eFJLl7TynN9JxmbypZcV7P8vkHOoaM5zgVdLbwyMMKJ8TTxSISGuOV3Dsg/75oLOvngxav46TN9PPXaCQCSsQhXX9DJ+9/ayYNP97Hv6CjuTlsyzsmJNOvbG8m5c/jkJJGIkcvl9y4ill9uZ3OcNW2N7D48XPbfpiFqNDZEedemTp7vG+LQyYnTHr907Qr+aH0bL/WPcmI8RTrrjE5mGBxLsW5lI1OZLIOFSWLr2hu56i0rebF/lH1vjJyxrEjEaIpHmcrkSGdzNMYjmEVIZbIYxtr2RhpiEXLuxCMRYtEIuw6dxHAiOBFyOEY8GuHmK9fSGI/yXN8QZsaatiQdLQl2Hxqi78QEA6NTuOe3yabzWolGjJeOjpKe9RlpTsRYu7KRC7qaOXhigmzOaUnEeP7QEFOZ/DbN77tZ4brR2BBjPJUhv+/mp/3M1dHcwOhkhlQ2X3dzIsr57U1MpLOMTGY4Pp7CHVqTMda3N3HwxARXb2inMR7j/708wMnx/N54azLGn166hrE0PPbiMU5O5j+DiXiUiXRupr6mhhgXr21jz5ERopEIyViEgZHxmW2XjMLb17Wy7+gY3W1NpDLOgeNjRHDWr2ygMRbltWMjM2vU3txAZ0uCF/vHoPCZ9ll/iZ3fnmTNigTPHDyB5/LLMCAWgWwuR2TWdskWqsgWnpUtPJolggGXrGrmvRd28Fr/MK8OjpLKOt0tDew+PMz0gVmL7aMbEI8YjpPNnVqmAZeuW8GBY2NEI8ZkxhlPO92tSQbH0qRyTjwaZSrrM+s1/W/Z1dLA8HiaTC6H4XwxeyHvLbL8WjGv8JxuZnYN8Dfufn3h9t0A7v6fi72mp6fHe3t7K1peMSfHU7QkYsSi1X8dkM7mGBxN0ZSIMpXO8eqxMcZTGd61qYucOw3RCJmcs++NES5bt2LmxNUAQ+NpVjTGZu6bTGcx44w2w9B4mtcGx+huzX8PcHwshQOdzQ0cGBynJRnjvNYEJ8fT/P7VQf7FH3bT1hTH3RmdytCSiJ223NlSmRwP7exjdVuSay7oJDmr7ZPJ5sjknGQ8ymQ6SyKWX5dXBsbY2NXMniPDRCPGW9qb+O3Lx9h8QSftzQ0cGZrgiZcHWd2W5K2rWmlNxjg2mqIhGqGpIUoyHiUaOVXPyGSaX+8/xuhUholUlsvXt3HV+e3z1juRypKMR0hnnd2Hh4hGjEvWrJj5txyaSOOe/wUbT2XJ5Jy1K5MkYlHcnYl0lsZ4dGZ7uPu822bXoSEGRqdoTcRw4Pm+Ia7Z1MnFa1aU/DxMprMMT6RZ2dRAQ6G1NjSe5olXBjGDt65qZUNn07zLTGdzHBgc442hKeJR47J1bTx94ARrVybZ1N1C/8gUJ8ZTDI2nGZ7MMDSRZnQyzcVrVpCIR3nt2BjJeJT3vbWbwbEpfrP/GFl3PnT5GlY2NZy2nGwu/9mMRE6vw905dHKC42MpNnY1z7SWprfdjn0DPP7iAG9b3UpbU5xVK5L0/EEHDbFIIdTyJ2V/8tXjpLI5OpsTbOhqmnkfgFzOeeXYGGawobOZaMToH57k5YEx1q1sZH17I5GIMZnO0hCNMDiWYiKVZXVbkqGJNJ3NDUQiRv/IJI/u6aclEePydW2sbkvyXN8Qa9qSdLcmGBiZYvfhIdqbGsjknFQmR2dLw8ztlY1x2pvPbPUBnBhL0XdigpHJNFOZHH94XgvpbI4T4ykyWSfrzobOZta0JZnK5HjhyDCpTI5YxOhqSbChq5npnByeyLDz9RNcs6mTwbEUJ8ZSvG11K88dGqKtMc5EKsuBwXHetqaVC7qaOTaaYufrJ5jK5Ni8sYPzViTnrbEcM3va3XvKPq+KQP9zYIu7/7vC7b8A/tjdPzPneVuBrQDnn3/+Ow4cOFDR8kRElquzDfQFH+Xi7ve5e4+793R3dy/04kRElq1qAv0Q8JZZt9cX7hMRkTqoJtCfAi40s41m1gB8DPh5bcoSEZE3q+JRLu6eMbPPAP9IfkTUNnffXbPKRETkTalqHLq7Pww8XKNaRESkCsFM/RcRkdIU6CIiS4QCXURkiah4YlFFCzMbACqdWdQFHKthOSHSNsjTdtA2gOW1Df7A3ctO5FnUQK+GmfWezUyppUzbIE/bQdsAtA3mo5aLiMgSoUAXEVkiQgr0++pdwDlA2yBP20HbALQNzhBMD11EREoLaQ9dRERKCCLQzWyLme0zs5fM7K5617NYzOw1M3vezJ41s97CfR1m9oiZ7S9czn/2iECZ2TYz6zezXbPum3edLe8bhc/Fc2a2mGfcW1BFtsPfmNmhwufhWTO7cdZjdxe2wz4zW9jznC0SM3uLmT1mZi+Y2W4zu6Nw/7L7PJytcz7QZ53q7gbgEuDjZnZJfataVB9w9ytmDc+6C3jU3S8EHi3cXkq+B2yZc1+xdb4BuLDwsxW4d5FqXAzf48ztAPC3hc/DFYVjKVH4ffgYcGnhNd8s/N6ELgP8tbtfAmwG/rKwrsvx83BWzvlAB64GXnL3V9w9BfwIuKnONdXTTcD0mbHvB26uYy015+6PA8fn3F1snW8C/pfn/Q5YaWZrFqfShVVkOxRzE/Ajd59y91eBl8j/3gTN3Y+4+87C9RFgD7COZfh5OFshBPo64OCs232F+5YDB7ab2dOFU/kBrHL3I4XrbwCr6lPaoiq2zsvxs/GZQjth26x225LfDma2AbgS+D36PBQVQqAvZ+9x96vI/yn5l2Z22knDPT9EaVkNU1qO6zzLvcAm4ArgCPDf6lvO4jCzFuAh4K/cfXj2Y8v883CGEAJ92Z7qzt0PFS77gZ+S/zP66PSfkYXL/vpVuGiKrfOy+my4+1F3z7p7Dvg2p9oqS3Y7mFmcfJj/wN1/Urhbn4ciQgj0ZXmqOzNrNrPW6evAnwK7yK/7bYWn3Qb8rD4VLqpi6/xz4F8XRjdsBoZm/Sm+5MzpB3+E/OcB8tvhY2aWMLON5L8UfHKx66s1MzPgu8Aed//arIf0eSjG3c/5H+BG4EXgZeAL9a5nkdb5AuCfCz+7p9cb6CT/zf5+4FdAR71rrfF6P0C+nZAm3wP9VLF1Boz8CKiXgeeBnnrXv8Db4X8X1vM58uG1Ztbzv1DYDvuAG+pdf422wXvIt1OeA54t/Ny4HD8PZ/ujmaIiIktECC0XERE5Cwp0EZElQoEuIrJEKNBFRJYIBbqIyBKhQBcRWSIU6CIiS4QCXURkifj/je2uFUM+kskAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1204c5490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.165495\n",
      "5-fold validation: Avg train loss: 0.112949, Avg test loss: 0.165183\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train,\n",
    "                                           y_train, learning_rate, weight_decay)\n",
    "print(\"%d-fold validation: Avg train loss: %f, Avg test loss: %f\" %\n",
    "      (k, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即便训练误差可以达到很低（调好参数之后），但是K折交叉验证上的误差可能更高。当训练误差特别低时，要观察K折交叉验证上的误差是否同时降低并小心过拟合。我们通常依赖K折交叉验证误差结果来调节参数。\n",
    "### 预测并在Kaggle提交预测结果（选学）\n",
    "本部分为选学内容。网络不好的同学可以通过上述K折交叉验证的方法来评测自己训练的模型。\n",
    "\n",
    "我们首先定义预测函数。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n",
    "          weight_decay):\n",
    "    test_yhat= train(X_train, y_train, X_test, None, epochs, verbose_epoch,\n",
    "          learning_rate, weight_decay)\n",
    "    preds = test_yhat\n",
    "    test['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test['Id'], test['SalePrice']], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调好参数以后，下面我们预测并在Kaggle提交预测结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train loss: 12.082478\n",
      "step 100, train loss: 0.219424\n",
      "step 200, train loss: 1.120250\n",
      "step 300, train loss: 0.152886\n",
      "step 400, train loss: 0.138535\n",
      "step 500, train loss: 0.146012\n",
      "step 600, train loss: 0.129160\n",
      "step 700, train loss: 0.119089\n",
      "step 800, train loss: 0.142142\n",
      "step 900, train loss: 0.103008\n",
      "step 1000, train loss: 0.119038\n",
      "step 1100, train loss: 0.183590\n",
      "step 1200, train loss: 0.111835\n",
      "step 1300, train loss: 0.101762\n",
      "step 1400, train loss: 0.147491\n",
      "step 1500, train loss: 0.127234\n",
      "step 1600, train loss: 0.144823\n",
      "step 1700, train loss: 0.121905\n",
      "step 1800, train loss: 0.145800\n",
      "step 1900, train loss: 0.188396\n",
      "step 2000, train loss: 0.135064\n",
      "step 2100, train loss: 0.119648\n",
      "step 2200, train loss: 0.162503\n",
      "step 2300, train loss: 0.112073\n",
      "step 2400, train loss: 0.108659\n",
      "step 2500, train loss: 0.123440\n",
      "step 2600, train loss: 0.134616\n",
      "step 2700, train loss: 0.125547\n",
      "step 2800, train loss: 0.117411\n",
      "step 2900, train loss: 0.134315\n",
      "step 3000, train loss: 0.097096\n",
      "step 3100, train loss: 0.107308\n",
      "step 3200, train loss: 0.188730\n",
      "step 3300, train loss: 0.145633\n",
      "step 3400, train loss: 0.173149\n",
      "step 3500, train loss: 0.175921\n",
      "step 3600, train loss: 0.145047\n",
      "step 3700, train loss: 0.114169\n",
      "step 3800, train loss: 0.139750\n",
      "step 3900, train loss: 0.166318\n",
      "step 4000, train loss: 0.127769\n",
      "step 4100, train loss: 0.097589\n",
      "step 4200, train loss: 0.120661\n",
      "step 4300, train loss: 0.177627\n",
      "step 4400, train loss: 0.104414\n",
      "step 4500, train loss: 0.126483\n",
      "step 4600, train loss: 0.146912\n",
      "step 4700, train loss: 0.125742\n",
      "step 4800, train loss: 0.121221\n",
      "step 4900, train loss: 0.092396\n",
      "step 5000, train loss: 0.116572\n",
      "step 5100, train loss: 0.119650\n",
      "step 5200, train loss: 0.150291\n",
      "step 5300, train loss: 0.095338\n",
      "step 5400, train loss: 0.111521\n",
      "step 5500, train loss: 0.173143\n",
      "step 5600, train loss: 0.131528\n",
      "step 5700, train loss: 0.106994\n",
      "step 5800, train loss: 0.181914\n",
      "step 5900, train loss: 0.126364\n",
      "step 6000, train loss: 0.114820\n",
      "step 6100, train loss: 0.112751\n",
      "step 6200, train loss: 0.138514\n",
      "step 6300, train loss: 0.097336\n",
      "step 6400, train loss: 0.103436\n",
      "step 6500, train loss: 0.144783\n",
      "step 6600, train loss: 0.132681\n",
      "step 6700, train loss: 0.103949\n",
      "step 6800, train loss: 0.114984\n",
      "step 6900, train loss: 0.112653\n",
      "step 7000, train loss: 0.147037\n",
      "step 7100, train loss: 0.142894\n",
      "step 7200, train loss: 0.142886\n",
      "step 7300, train loss: 0.127692\n",
      "step 7400, train loss: 0.113883\n",
      "step 7500, train loss: 0.148335\n",
      "step 7600, train loss: 0.110738\n",
      "step 7700, train loss: 0.121035\n",
      "step 7800, train loss: 0.117108\n",
      "step 7900, train loss: 0.152139\n",
      "step 8000, train loss: 0.121201\n",
      "step 8100, train loss: 0.125237\n",
      "step 8200, train loss: 0.116339\n",
      "step 8300, train loss: 0.120049\n",
      "step 8400, train loss: 0.128373\n",
      "step 8500, train loss: 0.148230\n",
      "step 8600, train loss: 0.122388\n",
      "step 8700, train loss: 0.111536\n",
      "step 8800, train loss: 0.107398\n",
      "step 8900, train loss: 0.112700\n",
      "step 9000, train loss: 0.154448\n",
      "step 9100, train loss: 0.133403\n",
      "step 9200, train loss: 0.134058\n",
      "step 9300, train loss: 0.101802\n",
      "step 9400, train loss: 0.122963\n",
      "step 9500, train loss: 0.130270\n",
      "step 9600, train loss: 0.110319\n",
      "step 9700, train loss: 0.135887\n",
      "step 9800, train loss: 0.127638\n",
      "step 9900, train loss: 0.130609\n",
      "step 10000, train loss: 0.131905\n",
      "step 10100, train loss: 0.129680\n",
      "step 10200, train loss: 0.154581\n",
      "step 10300, train loss: 0.158319\n",
      "step 10400, train loss: 0.096224\n",
      "step 10500, train loss: 0.151562\n",
      "step 10600, train loss: 0.154602\n",
      "step 10700, train loss: 0.133022\n",
      "step 10800, train loss: 0.124185\n",
      "step 10900, train loss: 0.096120\n",
      "step 11000, train loss: 0.133348\n",
      "step 11100, train loss: 0.151478\n",
      "step 11200, train loss: 0.113319\n",
      "step 11300, train loss: 0.116312\n",
      "step 11400, train loss: 0.126720\n",
      "step 11500, train loss: 0.091412\n",
      "step 11600, train loss: 0.115465\n",
      "step 11700, train loss: 0.150483\n",
      "step 11800, train loss: 0.132895\n",
      "step 11900, train loss: 0.126159\n",
      "step 12000, train loss: 0.112345\n",
      "step 12100, train loss: 0.094097\n",
      "step 12200, train loss: 0.110623\n",
      "step 12300, train loss: 0.113130\n",
      "step 12400, train loss: 0.105526\n",
      "step 12500, train loss: 0.139660\n",
      "step 12600, train loss: 0.160687\n",
      "step 12700, train loss: 0.104921\n",
      "step 12800, train loss: 0.126729\n",
      "step 12900, train loss: 0.156981\n",
      "step 13000, train loss: 0.122052\n",
      "step 13100, train loss: 0.110352\n",
      "step 13200, train loss: 0.130330\n",
      "step 13300, train loss: 0.160774\n",
      "step 13400, train loss: 0.132129\n",
      "step 13500, train loss: 0.139830\n",
      "step 13600, train loss: 0.118174\n",
      "step 13700, train loss: 0.120684\n",
      "step 13800, train loss: 0.109645\n",
      "step 13900, train loss: 0.159620\n",
      "step 14000, train loss: 0.116459\n",
      "step 14100, train loss: 0.109247\n",
      "step 14200, train loss: 0.150321\n",
      "step 14300, train loss: 0.117934\n",
      "step 14400, train loss: 0.145833\n",
      "step 14500, train loss: 0.100489\n",
      "step 14600, train loss: 0.152978\n",
      "step 14700, train loss: 0.114491\n",
      "step 14800, train loss: 0.097298\n",
      "step 14900, train loss: 0.102331\n",
      "step 15000, train loss: 0.141344\n",
      "step 15100, train loss: 0.107416\n",
      "step 15200, train loss: 0.129471\n",
      "step 15300, train loss: 0.113319\n",
      "step 15400, train loss: 0.122976\n",
      "step 15500, train loss: 0.124294\n",
      "step 15600, train loss: 0.133397\n",
      "step 15700, train loss: 0.115498\n",
      "step 15800, train loss: 0.126251\n",
      "step 15900, train loss: 0.103235\n",
      "step 16000, train loss: 0.128482\n",
      "step 16100, train loss: 0.107123\n",
      "step 16200, train loss: 0.134649\n",
      "step 16300, train loss: 0.113713\n",
      "step 16400, train loss: 0.131933\n",
      "step 16500, train loss: 0.171581\n",
      "step 16600, train loss: 0.123928\n",
      "step 16700, train loss: 0.161938\n",
      "step 16800, train loss: 0.097062\n",
      "step 16900, train loss: 0.116043\n",
      "step 17000, train loss: 0.113616\n",
      "step 17100, train loss: 0.118916\n",
      "step 17200, train loss: 0.115906\n",
      "step 17300, train loss: 0.134084\n",
      "step 17400, train loss: 0.117667\n",
      "step 17500, train loss: 0.135055\n",
      "step 17600, train loss: 0.152149\n",
      "step 17700, train loss: 0.098566\n",
      "step 17800, train loss: 0.151142\n",
      "step 17900, train loss: 0.118835\n",
      "step 18000, train loss: 0.096039\n",
      "step 18100, train loss: 0.108332\n",
      "step 18200, train loss: 0.121610\n",
      "step 18300, train loss: 0.116165\n",
      "step 18400, train loss: 0.144361\n",
      "step 18500, train loss: 0.151456\n",
      "step 18600, train loss: 0.122517\n",
      "step 18700, train loss: 0.097064\n",
      "step 18800, train loss: 0.107016\n",
      "step 18900, train loss: 0.111583\n",
      "step 19000, train loss: 0.155765\n",
      "step 19100, train loss: 0.122192\n",
      "step 19200, train loss: 0.143055\n",
      "step 19300, train loss: 0.119571\n",
      "step 19400, train loss: 0.126054\n",
      "step 19500, train loss: 0.119795\n",
      "step 19600, train loss: 0.114009\n",
      "step 19700, train loss: 0.138095\n",
      "step 19800, train loss: 0.106592\n",
      "step 19900, train loss: 0.153475\n",
      "step 20000, train loss: 0.135173\n",
      "step 20100, train loss: 0.164028\n",
      "step 20200, train loss: 0.102252\n",
      "step 20300, train loss: 0.104355\n",
      "step 20400, train loss: 0.106296\n",
      "step 20500, train loss: 0.125619\n",
      "step 20600, train loss: 0.135016\n",
      "step 20700, train loss: 0.133035\n",
      "step 20800, train loss: 0.109910\n",
      "step 20900, train loss: 0.115239\n",
      "step 21000, train loss: 0.110907\n",
      "step 21100, train loss: 0.147583\n",
      "step 21200, train loss: 0.167094\n",
      "step 21300, train loss: 0.110398\n",
      "step 21400, train loss: 0.116146\n",
      "step 21500, train loss: 0.095513\n",
      "step 21600, train loss: 0.136444\n",
      "step 21700, train loss: 0.133853\n",
      "step 21800, train loss: 0.119016\n",
      "step 21900, train loss: 0.092888\n",
      "step 22000, train loss: 0.108021\n",
      "step 22100, train loss: 0.147231\n",
      "step 22200, train loss: 0.143492\n",
      "step 22300, train loss: 0.149791\n",
      "step 22400, train loss: 0.151892\n",
      "step 22500, train loss: 0.117366\n",
      "step 22600, train loss: 0.105303\n",
      "step 22700, train loss: 0.153597\n",
      "step 22800, train loss: 0.105709\n",
      "step 22900, train loss: 0.103726\n",
      "step 23000, train loss: 0.113929\n",
      "step 23100, train loss: 0.111016\n",
      "step 23200, train loss: 0.094432\n",
      "step 23300, train loss: 0.118476\n",
      "step 23400, train loss: 0.099589\n",
      "step 23500, train loss: 0.116188\n",
      "step 23600, train loss: 0.115459\n",
      "step 23700, train loss: 0.110386\n",
      "step 23800, train loss: 0.110461\n",
      "step 23900, train loss: 0.114323\n",
      "step 24000, train loss: 0.124189\n",
      "step 24100, train loss: 0.111306\n",
      "step 24200, train loss: 0.130869\n",
      "step 24300, train loss: 0.113346\n",
      "step 24400, train loss: 0.116855\n",
      "step 24500, train loss: 0.111683\n",
      "step 24600, train loss: 0.096060\n",
      "step 24700, train loss: 0.115008\n",
      "step 24800, train loss: 0.145250\n",
      "step 24900, train loss: 0.124770\n",
      "step 25000, train loss: 0.089785\n",
      "step 25100, train loss: 0.125818\n",
      "step 25200, train loss: 0.140787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 25300, train loss: 0.116296\n",
      "step 25400, train loss: 0.148814\n",
      "step 25500, train loss: 0.096156\n",
      "step 25600, train loss: 0.114050\n",
      "step 25700, train loss: 0.114724\n",
      "step 25800, train loss: 0.115034\n",
      "step 25900, train loss: 0.164720\n",
      "step 26000, train loss: 0.100686\n",
      "step 26100, train loss: 0.134332\n",
      "step 26200, train loss: 0.120155\n",
      "step 26300, train loss: 0.119674\n",
      "step 26400, train loss: 0.093240\n",
      "step 26500, train loss: 0.103451\n",
      "step 26600, train loss: 0.137472\n",
      "step 26700, train loss: 0.162250\n",
      "step 26800, train loss: 0.109099\n",
      "step 26900, train loss: 0.113986\n",
      "step 27000, train loss: 0.160048\n",
      "step 27100, train loss: 0.146144\n",
      "step 27200, train loss: 0.130530\n",
      "step 27300, train loss: 0.118316\n",
      "step 27400, train loss: 0.123653\n",
      "step 27500, train loss: 0.100588\n",
      "step 27600, train loss: 0.133973\n",
      "step 27700, train loss: 0.104179\n",
      "step 27800, train loss: 0.107815\n",
      "step 27900, train loss: 0.101661\n",
      "step 28000, train loss: 0.136123\n",
      "step 28100, train loss: 0.137129\n",
      "step 28200, train loss: 0.120553\n",
      "step 28300, train loss: 0.092263\n",
      "step 28400, train loss: 0.148970\n",
      "step 28500, train loss: 0.151941\n",
      "step 28600, train loss: 0.122349\n",
      "step 28700, train loss: 0.123238\n",
      "step 28800, train loss: 0.119183\n",
      "step 28900, train loss: 0.138359\n",
      "step 29000, train loss: 0.091624\n",
      "step 29100, train loss: 0.129240\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120af4a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n",
    "      weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行完上述代码后，会生成一个submission.csv文件。这是Kaggle要求的提交格式。这时我们可以在Kaggle上把我们预测得出的结果提交并查看与测试数据集上真实房价的误差。你需要登录Kaggle网站，打开[房价预测问题地址](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)，并点击下方右侧Submit Predictions按钮提交。\n",
    "![image.png](http://zh.gluon.ai/_images/kaggle_submit.png)\n",
    "请点击下方Upload Submission File选择需要提交的预测结果。然后点击下方的Make Submission按钮就可以查看结果啦！\n",
    "![image.png](http://zh.gluon.ai/_images/kaggle_submit2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
