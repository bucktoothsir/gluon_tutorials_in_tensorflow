{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多类逻辑回归 — 从0开始\n",
    "对应的gluon教程http://zh.gluon.ai/chapter_supervised-learning/softmax-regression-scratch.html\n",
    "\n",
    "如果你读过了从0开始的线性回归，那么最难的部分已经过去了。现在你知道如果读取和操作数据，如何构造目标函数和对它求导，如果定义损失函数，模型和求解。\n",
    "\n",
    "下面我们来看一个稍微有意思一点的问题，如何使用多类逻辑回归进行多类分类。这个模型跟线性回归的主要区别在于输出节点从一个变成了多个。\n",
    "![image.png](http://zh.gluon.ai/_images/simple-softmax-net.png)\n",
    "\n",
    "### 获取数据\n",
    "演示这个模型的常见数据集是手写数字识别MNIST，它长这个样子。\n",
    "![image.png](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/example/mnist.png)\n",
    "\n",
    "这里我们用了一个稍微复杂点的数据集，它跟MNIST非常像，但是内容不再是分类数字，而是服饰。gluon可以用过data.vision模块自动下载这个数据。但是因为我用tensorflow，所以在https://github.com/zalandoresearch/fashion-mnist 手动下载了数据。\n",
    "\n",
    "分别是：\n",
    "\n",
    "train-images-idx3-ubyte.gz\t\n",
    "train-labels-idx1-ubyte.gz\t\n",
    "t10k-images-idx3-ubyte.gz\t\n",
    "t10k-labels-idx1-ubyte.gz\t\n",
    "\n",
    "然后用tensorflow定义好的接口读取上面的文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "train_image_file = '../../data/fashion_mnist/train-images-idx3-ubyte.gz'\n",
    "train_labels_file = '../../data/fashion_mnist/train-labels-idx1-ubyte.gz'\n",
    "test_image_file = '../../data/fashion_mnist/t10k-images-idx3-ubyte.gz'\n",
    "test_labels_file = '../../data/fashion_mnist/t10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "with gfile.Open(train_image_file, 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "\n",
    "with gfile.Open(train_labels_file, 'rb') as f:\n",
    "    train_labels = extract_labels(f, one_hot=True, num_classes=10)\n",
    "\n",
    "with gfile.Open(test_image_file, 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "    \n",
    "with gfile.Open(test_labels_file, 'rb') as f:\n",
    "    test_labels = extract_labels(f, one_hot=True, num_classes=10)\n",
    "\n",
    "print train_images.shape\n",
    "print train_labels[0]\n",
    "\n",
    "print test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们画出前几个样本的内容，和对应的文本标号\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bf3c090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ankle boot', 't-shirt', 't-shirt', 'dress,', 't-shirt', 'pullover', 'sneaker', 'pullover', 'sandal']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_images(images):\n",
    "    n = images.shape[0]\n",
    "    _, figs = plt.subplots(1, n, figsize=(15, 15))\n",
    "    for i in range(n):\n",
    "        figs[i].imshow(images[i].reshape((28, 28)))\n",
    "        #隐藏x,y坐标\n",
    "        figs[i].axes.get_xaxis().set_visible(False)\n",
    "        figs[i].axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "def get_text_labels(label):\n",
    "    text_labels = [\n",
    "        't-shirt', 'trouser', 'pullover', 'dress,', 'coat',\n",
    "        'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "    ]\n",
    "    return [text_labels[int(i)] for i in label]\n",
    "\n",
    "data = train_images[0:9]\n",
    "label = np.argmax(train_labels,axis=1)[0:9]\n",
    "show_images(data)\n",
    "print(get_text_labels(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据读取\n",
    "虽然我们可以像前面那样通过yield来定义获取批量数据函数，原文使用了gluon.data的DataLoader函数，它每次yield一个批量。在tensorflow中同样可以使用定义好的接口，创建Dataset对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型参数\n",
    "跟线性模型一样，每个样本会表示成一个向量。我们这里数据是 28 * 28 大小的图片，所以输入向量的长度是 28 * 28 = 784。因为我们要做多类分类，我们需要对每一个类预测这个样本属于此类的概率。因为这个数据集有10个类型，所以输出应该是长为10的向量。这样，我们需要的权重将是一个 784 * 10 的矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "with tf.name_scope('lr'):\n",
    "    W = tf.Variable(tf.random_normal([num_inputs, num_outputs], mean=0.0, stddev=1.0, seed=None, dtype=tf.float32), name='weights')\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_outputs]), name='bias')\n",
    "    #W = tf.Variable(tf.zeros([784, 10]), name='weights')\n",
    "    #b = tf.Variable(tf.zeros([10]), name='bias')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "在线性回归教程里，我们只需要输出一个标量$yhat$使得尽可能的靠近目标值。但在这里的分类里，我们需要属于每个类别的概率。这些概率需要值为正，而且加起来等于1. 而如果简单的使用 $\\boldsymbol{\\hat y} = \\boldsymbol{W} \\boldsymbol{x}$, 我们不能保证这一点。一个通常的做法是通过softmax函数来将任意的输入归一化成合法的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_softmax(X):\n",
    "    eps = 1e-18\n",
    "    exp = np.exp(X + eps)\n",
    "    # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，\n",
    "    # 就是返回 (nrows, 1) 形状的矩阵\n",
    "    partition = np.sum(exp, axis=1, keepdims=True)\n",
    "    return exp / partition\n",
    "\n",
    "def tf_softmax(X):\n",
    "    eps=1e-18\n",
    "    exp = tf.exp(X)\n",
    "    # 假设exp是矩阵，这里对行进行求和，并要求保留axis 1，\n",
    "    # 就是返回 (nrows, 1) 形状的矩阵\n",
    "    partition = tf.reduce_sum(exp, axis=1, keep_dims=True)\n",
    "    return exp / partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，对于随机输入，我们将每个元素变成了非负数，而且每一行加起来为1。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.11481442  1.33921703  2.73831238 -1.30705263 -0.52536617]\n",
      " [ 2.69731241  1.12879095 -0.40406067  0.59043229  0.59941436]]\n",
      "[[0.01602474 0.18645044 0.75541013 0.01322218 0.02889251]\n",
      " [0.66770325 0.13911798 0.03003823 0.08120394 0.0819366 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.normal(size=(2,5))\n",
    "print X\n",
    "X_prob = np_softmax(X)\n",
    "print(X_prob)\n",
    "#print(X_prob.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以定义模型了：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, weights, bias):\n",
    "    z = tf.matmul(X, weights) + bias\n",
    "    return tf_softmax(z), z\n",
    "    #return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵损失函数\n",
    "我们需要定义一个针对预测为概率值的损失函数。其中最常见的是交叉熵损失函数，它将两个概率分布的负交叉熵作为目标值，最小化这个值等价于最大化这两个概率的相似度。\n",
    "\n",
    "具体来说，我们先将真实标号表示成一个概率分布，例如如果``y=1``，那么其对应的分布就是一个除了第二个元素为1其他全为0的长为10的向量，也就是 `yvec=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`。那么交叉熵就是`yvec[0]*log(yhat[0])+...+yvec[n]*log(yhat[n])`。注意到yvec里面只有一个1，那么前面等价于`log(yhat[y])`。所以我们可以定义这个损失函数了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(yhat, y):\n",
    "    #yhat和y都是batch_size*num_classes的向量\n",
    "    eps = 1e-18\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    return tf.reduce_mean(-y * tf.log(yhat+eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算精度\n",
    "给定一个概率输出，我们将预测概率最高的那个类作为预测的类，然后通过比较真实标号我们可以计算精度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "    #output, label都是batch_size*1的向量\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output, 1),tf.argmax(label, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "训练代码跟前面的线性回归非常相似：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr/weights\n",
      "lr/bias\n",
      "Epoch 0, Batch 0, Loss: 1.372785, Train acc 0.171875 \n",
      "Epoch 0, Batch 100, Loss: 0.222798, Train acc 0.546875 \n",
      "Epoch 0, Batch 200, Loss: 0.279642, Train acc 0.468750 \n",
      "Epoch 0, Batch 300, Loss: 0.216706, Train acc 0.531250 \n",
      "Epoch 0, Batch 400, Loss: 0.139442, Train acc 0.750000 \n",
      "Epoch 0, Batch 500, Loss: 0.103321, Train acc 0.656250 \n",
      "Epoch 0, Batch 600, Loss: 0.107369, Train acc 0.750000 \n",
      "Epoch 0, Batch 700, Loss: 0.093431, Train acc 0.750000 \n",
      "Epoch 0, Batch 800, Loss: 0.136332, Train acc 0.765625 \n",
      "Epoch 0, Batch 900, Loss: 0.091566, Train acc 0.796875 \n",
      "Epoch 0, Test Loss: 0.138095, Test acc 0.697000 \n",
      "Epoch 1, Batch 0, Loss: 0.120773, Train acc 0.687500 \n",
      "Epoch 1, Batch 100, Loss: 0.096433, Train acc 0.718750 \n",
      "Epoch 1, Batch 200, Loss: 0.096708, Train acc 0.734375 \n",
      "Epoch 1, Batch 300, Loss: 0.107570, Train acc 0.687500 \n",
      "Epoch 1, Batch 400, Loss: 0.075544, Train acc 0.765625 \n",
      "Epoch 1, Batch 500, Loss: 0.146705, Train acc 0.718750 \n",
      "Epoch 1, Batch 600, Loss: 0.117502, Train acc 0.796875 \n",
      "Epoch 1, Batch 700, Loss: 0.132929, Train acc 0.671875 \n",
      "Epoch 1, Batch 800, Loss: 0.113375, Train acc 0.718750 \n",
      "Epoch 1, Batch 900, Loss: 0.098504, Train acc 0.750000 \n",
      "Epoch 1, Test Loss: 0.112316, Test acc 0.740200 \n",
      "Epoch 2, Batch 0, Loss: 0.089768, Train acc 0.828125 \n",
      "Epoch 2, Batch 100, Loss: 0.087423, Train acc 0.750000 \n",
      "Epoch 2, Batch 200, Loss: 0.134769, Train acc 0.703125 \n",
      "Epoch 2, Batch 300, Loss: 0.105854, Train acc 0.750000 \n",
      "Epoch 2, Batch 400, Loss: 0.078306, Train acc 0.781250 \n",
      "Epoch 2, Batch 500, Loss: 0.077610, Train acc 0.765625 \n",
      "Epoch 2, Batch 600, Loss: 0.110254, Train acc 0.765625 \n",
      "Epoch 2, Batch 700, Loss: 0.101377, Train acc 0.750000 \n",
      "Epoch 2, Batch 800, Loss: 0.085824, Train acc 0.765625 \n",
      "Epoch 2, Batch 900, Loss: 0.044902, Train acc 0.828125 \n",
      "Epoch 2, Test Loss: 0.100735, Test acc 0.764900 \n",
      "Epoch 3, Batch 0, Loss: 0.119468, Train acc 0.750000 \n",
      "Epoch 3, Batch 100, Loss: 0.083641, Train acc 0.781250 \n",
      "Epoch 3, Batch 200, Loss: 0.091307, Train acc 0.812500 \n",
      "Epoch 3, Batch 300, Loss: 0.088998, Train acc 0.734375 \n",
      "Epoch 3, Batch 400, Loss: 0.075029, Train acc 0.812500 \n",
      "Epoch 3, Batch 500, Loss: 0.104040, Train acc 0.765625 \n",
      "Epoch 3, Batch 600, Loss: 0.077661, Train acc 0.828125 \n",
      "Epoch 3, Batch 700, Loss: 0.134442, Train acc 0.656250 \n",
      "Epoch 3, Batch 800, Loss: 0.103993, Train acc 0.781250 \n",
      "Epoch 3, Batch 900, Loss: 0.064648, Train acc 0.828125 \n",
      "Epoch 3, Test Loss: 0.093536, Test acc 0.765800 \n",
      "Epoch 4, Batch 0, Loss: 0.102285, Train acc 0.703125 \n",
      "Epoch 4, Batch 100, Loss: 0.063098, Train acc 0.796875 \n",
      "Epoch 4, Batch 200, Loss: 0.106519, Train acc 0.750000 \n",
      "Epoch 4, Batch 300, Loss: 0.078777, Train acc 0.781250 \n",
      "Epoch 4, Batch 400, Loss: 0.053763, Train acc 0.828125 \n",
      "Epoch 4, Batch 500, Loss: 0.061264, Train acc 0.812500 \n",
      "Epoch 4, Batch 600, Loss: 0.081480, Train acc 0.796875 \n",
      "Epoch 4, Batch 700, Loss: 0.022768, Train acc 0.906250 \n",
      "Epoch 4, Batch 800, Loss: 0.091290, Train acc 0.734375 \n",
      "Epoch 4, Batch 900, Loss: 0.083839, Train acc 0.765625 \n",
      "Epoch 4, Test Loss: 0.088701, Test acc 0.778000 \n",
      "Epoch 5, Batch 0, Loss: 0.076872, Train acc 0.859375 \n",
      "Epoch 5, Batch 100, Loss: 0.085676, Train acc 0.812500 \n",
      "Epoch 5, Batch 200, Loss: 0.081352, Train acc 0.750000 \n",
      "Epoch 5, Batch 300, Loss: 0.069937, Train acc 0.781250 \n",
      "Epoch 5, Batch 400, Loss: 0.063784, Train acc 0.843750 \n",
      "Epoch 5, Batch 500, Loss: 0.070900, Train acc 0.703125 \n",
      "Epoch 5, Batch 600, Loss: 0.121284, Train acc 0.765625 \n",
      "Epoch 5, Batch 700, Loss: 0.120258, Train acc 0.765625 \n",
      "Epoch 5, Batch 800, Loss: 0.094245, Train acc 0.765625 \n",
      "Epoch 5, Batch 900, Loss: 0.082031, Train acc 0.796875 \n",
      "Epoch 5, Test Loss: 0.084938, Test acc 0.781700 \n",
      "Epoch 6, Batch 0, Loss: 0.076039, Train acc 0.750000 \n",
      "Epoch 6, Batch 100, Loss: 0.051686, Train acc 0.843750 \n",
      "Epoch 6, Batch 200, Loss: 0.047772, Train acc 0.859375 \n",
      "Epoch 6, Batch 300, Loss: 0.063086, Train acc 0.812500 \n",
      "Epoch 6, Batch 400, Loss: 0.042267, Train acc 0.875000 \n",
      "Epoch 6, Batch 500, Loss: 0.032848, Train acc 0.921875 \n",
      "Epoch 6, Batch 600, Loss: 0.095574, Train acc 0.687500 \n",
      "Epoch 6, Batch 700, Loss: 0.098550, Train acc 0.734375 \n",
      "Epoch 6, Batch 800, Loss: 0.043178, Train acc 0.875000 \n",
      "Epoch 6, Batch 900, Loss: 0.035668, Train acc 0.890625 \n",
      "Epoch 6, Test Loss: 0.081730, Test acc 0.782000 \n",
      "Epoch 7, Batch 0, Loss: 0.069564, Train acc 0.781250 \n",
      "Epoch 7, Batch 100, Loss: 0.071626, Train acc 0.796875 \n",
      "Epoch 7, Batch 200, Loss: 0.042460, Train acc 0.875000 \n",
      "Epoch 7, Batch 300, Loss: 0.062529, Train acc 0.796875 \n",
      "Epoch 7, Batch 400, Loss: 0.071238, Train acc 0.734375 \n",
      "Epoch 7, Batch 500, Loss: 0.082390, Train acc 0.703125 \n",
      "Epoch 7, Batch 600, Loss: 0.069008, Train acc 0.781250 \n",
      "Epoch 7, Batch 700, Loss: 0.072145, Train acc 0.796875 \n",
      "Epoch 7, Batch 800, Loss: 0.037558, Train acc 0.890625 \n",
      "Epoch 7, Batch 900, Loss: 0.076491, Train acc 0.859375 \n",
      "Epoch 7, Test Loss: 0.080639, Test acc 0.780100 \n",
      "Epoch 8, Batch 0, Loss: 0.086267, Train acc 0.765625 \n",
      "Epoch 8, Batch 100, Loss: 0.101311, Train acc 0.781250 \n",
      "Epoch 8, Batch 200, Loss: 0.047920, Train acc 0.890625 \n",
      "Epoch 8, Batch 300, Loss: 0.068502, Train acc 0.765625 \n",
      "Epoch 8, Batch 400, Loss: 0.063232, Train acc 0.781250 \n",
      "Epoch 8, Batch 500, Loss: 0.085556, Train acc 0.796875 \n",
      "Epoch 8, Batch 600, Loss: 0.134878, Train acc 0.703125 \n",
      "Epoch 8, Batch 700, Loss: 0.086517, Train acc 0.796875 \n",
      "Epoch 8, Batch 800, Loss: 0.081892, Train acc 0.765625 \n",
      "Epoch 8, Batch 900, Loss: 0.054457, Train acc 0.859375 \n",
      "Epoch 8, Test Loss: 0.074857, Test acc 0.795100 \n",
      "Epoch 9, Batch 0, Loss: 0.063883, Train acc 0.828125 \n",
      "Epoch 9, Batch 100, Loss: 0.059420, Train acc 0.828125 \n",
      "Epoch 9, Batch 200, Loss: 0.065991, Train acc 0.828125 \n",
      "Epoch 9, Batch 300, Loss: 0.078642, Train acc 0.812500 \n",
      "Epoch 9, Batch 400, Loss: 0.084807, Train acc 0.765625 \n",
      "Epoch 9, Batch 500, Loss: 0.070647, Train acc 0.859375 \n",
      "Epoch 9, Batch 600, Loss: 0.045565, Train acc 0.890625 \n",
      "Epoch 9, Batch 700, Loss: 0.073186, Train acc 0.781250 \n",
      "Epoch 9, Batch 800, Loss: 0.066234, Train acc 0.765625 \n",
      "Epoch 9, Batch 900, Loss: 0.091640, Train acc 0.750000 \n",
      "Epoch 9, Test Loss: 0.074083, Test acc 0.798500 \n",
      "Epoch 10, Batch 0, Loss: 0.079375, Train acc 0.765625 \n",
      "Epoch 10, Batch 100, Loss: 0.068691, Train acc 0.843750 \n",
      "Epoch 10, Batch 200, Loss: 0.063178, Train acc 0.796875 \n",
      "Epoch 10, Batch 300, Loss: 0.051483, Train acc 0.890625 \n",
      "Epoch 10, Batch 400, Loss: 0.062313, Train acc 0.828125 \n",
      "Epoch 10, Batch 500, Loss: 0.073247, Train acc 0.812500 \n",
      "Epoch 10, Batch 600, Loss: 0.054828, Train acc 0.812500 \n",
      "Epoch 10, Batch 700, Loss: 0.090912, Train acc 0.796875 \n",
      "Epoch 10, Batch 800, Loss: 0.101459, Train acc 0.781250 \n",
      "Epoch 10, Batch 900, Loss: 0.035464, Train acc 0.875000 \n",
      "Epoch 10, Test Loss: 0.071588, Test acc 0.797400 \n",
      "Epoch 11, Batch 0, Loss: 0.054324, Train acc 0.859375 \n",
      "Epoch 11, Batch 100, Loss: 0.066816, Train acc 0.734375 \n",
      "Epoch 11, Batch 200, Loss: 0.091188, Train acc 0.828125 \n",
      "Epoch 11, Batch 300, Loss: 0.048325, Train acc 0.859375 \n",
      "Epoch 11, Batch 400, Loss: 0.084435, Train acc 0.750000 \n",
      "Epoch 11, Batch 500, Loss: 0.065808, Train acc 0.734375 \n",
      "Epoch 11, Batch 600, Loss: 0.046552, Train acc 0.875000 \n",
      "Epoch 11, Batch 700, Loss: 0.054409, Train acc 0.828125 \n",
      "Epoch 11, Batch 800, Loss: 0.049598, Train acc 0.843750 \n",
      "Epoch 11, Batch 900, Loss: 0.072792, Train acc 0.812500 \n",
      "Epoch 11, Test Loss: 0.070307, Test acc 0.802100 \n",
      "Epoch 12, Batch 0, Loss: 0.070543, Train acc 0.859375 \n",
      "Epoch 12, Batch 100, Loss: 0.068679, Train acc 0.796875 \n",
      "Epoch 12, Batch 200, Loss: 0.072709, Train acc 0.812500 \n",
      "Epoch 12, Batch 300, Loss: 0.067134, Train acc 0.828125 \n",
      "Epoch 12, Batch 400, Loss: 0.083909, Train acc 0.750000 \n",
      "Epoch 12, Batch 500, Loss: 0.062812, Train acc 0.859375 \n",
      "Epoch 12, Batch 600, Loss: 0.061121, Train acc 0.875000 \n",
      "Epoch 12, Batch 700, Loss: 0.061535, Train acc 0.796875 \n",
      "Epoch 12, Batch 800, Loss: 0.061697, Train acc 0.828125 \n",
      "Epoch 12, Batch 900, Loss: 0.097245, Train acc 0.734375 \n",
      "Epoch 12, Test Loss: 0.067972, Test acc 0.809200 \n",
      "Epoch 13, Batch 0, Loss: 0.052210, Train acc 0.828125 \n",
      "Epoch 13, Batch 100, Loss: 0.058247, Train acc 0.796875 \n",
      "Epoch 13, Batch 200, Loss: 0.060534, Train acc 0.859375 \n",
      "Epoch 13, Batch 300, Loss: 0.104667, Train acc 0.765625 \n",
      "Epoch 13, Batch 400, Loss: 0.054842, Train acc 0.796875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 500, Loss: 0.051088, Train acc 0.859375 \n",
      "Epoch 13, Batch 600, Loss: 0.025355, Train acc 0.890625 \n",
      "Epoch 13, Batch 700, Loss: 0.077513, Train acc 0.796875 \n",
      "Epoch 13, Batch 800, Loss: 0.047322, Train acc 0.828125 \n",
      "Epoch 13, Batch 900, Loss: 0.071855, Train acc 0.765625 \n",
      "Epoch 13, Test Loss: 0.068268, Test acc 0.806400 \n",
      "Epoch 14, Batch 0, Loss: 0.026761, Train acc 0.921875 \n",
      "Epoch 14, Batch 100, Loss: 0.067886, Train acc 0.765625 \n",
      "Epoch 14, Batch 200, Loss: 0.060385, Train acc 0.796875 \n",
      "Epoch 14, Batch 300, Loss: 0.061576, Train acc 0.812500 \n",
      "Epoch 14, Batch 400, Loss: 0.061684, Train acc 0.859375 \n",
      "Epoch 14, Batch 500, Loss: 0.058525, Train acc 0.828125 \n",
      "Epoch 14, Batch 600, Loss: 0.056437, Train acc 0.828125 \n",
      "Epoch 14, Batch 700, Loss: 0.049873, Train acc 0.859375 \n",
      "Epoch 14, Batch 800, Loss: 0.045839, Train acc 0.812500 \n",
      "Epoch 14, Batch 900, Loss: 0.045442, Train acc 0.828125 \n",
      "Epoch 14, Test Loss: 0.065279, Test acc 0.811800 \n",
      "Epoch 15, Batch 0, Loss: 0.058781, Train acc 0.859375 \n",
      "Epoch 15, Batch 100, Loss: 0.039071, Train acc 0.828125 \n",
      "Epoch 15, Batch 200, Loss: 0.041284, Train acc 0.875000 \n",
      "Epoch 15, Batch 300, Loss: 0.074949, Train acc 0.781250 \n",
      "Epoch 15, Batch 400, Loss: 0.055335, Train acc 0.781250 \n",
      "Epoch 15, Batch 500, Loss: 0.071512, Train acc 0.781250 \n",
      "Epoch 15, Batch 600, Loss: 0.054444, Train acc 0.796875 \n",
      "Epoch 15, Batch 700, Loss: 0.073262, Train acc 0.781250 \n",
      "Epoch 15, Batch 800, Loss: 0.048789, Train acc 0.843750 \n",
      "Epoch 15, Batch 900, Loss: 0.034061, Train acc 0.875000 \n",
      "Epoch 15, Test Loss: 0.064587, Test acc 0.809500 \n",
      "Epoch 16, Batch 0, Loss: 0.033292, Train acc 0.859375 \n",
      "Epoch 16, Batch 100, Loss: 0.073514, Train acc 0.734375 \n",
      "Epoch 16, Batch 200, Loss: 0.033062, Train acc 0.906250 \n",
      "Epoch 16, Batch 300, Loss: 0.065820, Train acc 0.843750 \n",
      "Epoch 16, Batch 400, Loss: 0.031337, Train acc 0.890625 \n",
      "Epoch 16, Batch 500, Loss: 0.068636, Train acc 0.812500 \n",
      "Epoch 16, Batch 600, Loss: 0.040123, Train acc 0.843750 \n",
      "Epoch 16, Batch 700, Loss: 0.108739, Train acc 0.781250 \n",
      "Epoch 16, Batch 800, Loss: 0.070697, Train acc 0.828125 \n",
      "Epoch 16, Batch 900, Loss: 0.034563, Train acc 0.890625 \n",
      "Epoch 16, Test Loss: 0.063318, Test acc 0.814900 \n",
      "Epoch 17, Batch 0, Loss: 0.032553, Train acc 0.875000 \n",
      "Epoch 17, Batch 100, Loss: 0.042255, Train acc 0.859375 \n",
      "Epoch 17, Batch 200, Loss: 0.089075, Train acc 0.734375 \n",
      "Epoch 17, Batch 300, Loss: 0.057692, Train acc 0.812500 \n",
      "Epoch 17, Batch 400, Loss: 0.043185, Train acc 0.843750 \n",
      "Epoch 17, Batch 500, Loss: 0.070020, Train acc 0.812500 \n",
      "Epoch 17, Batch 600, Loss: 0.046314, Train acc 0.843750 \n",
      "Epoch 17, Batch 700, Loss: 0.049343, Train acc 0.843750 \n",
      "Epoch 17, Batch 800, Loss: 0.039879, Train acc 0.921875 \n",
      "Epoch 17, Batch 900, Loss: 0.046456, Train acc 0.859375 \n",
      "Epoch 17, Test Loss: 0.063025, Test acc 0.814400 \n",
      "Epoch 18, Batch 0, Loss: 0.079863, Train acc 0.843750 \n",
      "Epoch 18, Batch 100, Loss: 0.054337, Train acc 0.875000 \n",
      "Epoch 18, Batch 200, Loss: 0.035182, Train acc 0.843750 \n",
      "Epoch 18, Batch 300, Loss: 0.031596, Train acc 0.843750 \n",
      "Epoch 18, Batch 400, Loss: 0.083868, Train acc 0.875000 \n",
      "Epoch 18, Batch 500, Loss: 0.036728, Train acc 0.890625 \n",
      "Epoch 18, Batch 600, Loss: 0.071931, Train acc 0.765625 \n",
      "Epoch 18, Batch 700, Loss: 0.076888, Train acc 0.796875 \n",
      "Epoch 18, Batch 800, Loss: 0.109123, Train acc 0.718750 \n",
      "Epoch 18, Batch 900, Loss: 0.051791, Train acc 0.781250 \n",
      "Epoch 18, Test Loss: 0.062156, Test acc 0.818000 \n",
      "Epoch 19, Batch 0, Loss: 0.030878, Train acc 0.937500 \n",
      "Epoch 19, Batch 100, Loss: 0.036565, Train acc 0.859375 \n",
      "Epoch 19, Batch 200, Loss: 0.052765, Train acc 0.828125 \n",
      "Epoch 19, Batch 300, Loss: 0.052755, Train acc 0.781250 \n",
      "Epoch 19, Batch 400, Loss: 0.023401, Train acc 0.921875 \n",
      "Epoch 19, Batch 500, Loss: 0.062285, Train acc 0.812500 \n",
      "Epoch 19, Batch 600, Loss: 0.072207, Train acc 0.812500 \n",
      "Epoch 19, Batch 700, Loss: 0.078009, Train acc 0.859375 \n",
      "Epoch 19, Batch 800, Loss: 0.090809, Train acc 0.796875 \n",
      "Epoch 19, Batch 900, Loss: 0.051603, Train acc 0.828125 \n",
      "Epoch 19, Test Loss: 0.063254, Test acc 0.815100 \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pdb\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "image_width = image_height = 28\n",
    "num_train_samples = 60000\n",
    "learning_rate = 1e0\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, num_inputs])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "softmax_predict, logit = net(input_placeholder, W, b)\n",
    "loss = cross_entropy(softmax_predict, gt_placeholder)\n",
    "acc = accuracy(softmax_predict, gt_placeholder)\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "train_loss = 0.\n",
    "train_acc = 0.\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], image_height*image_width))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in range(60000/batch_size):\n",
    "        data, label = train_dataset.next_batch(batch_size)\n",
    "        data = np.reshape(data, (batch_size, image_height*image_width))\n",
    "        feed_dict = {input_placeholder: data, gt_placeholder: label}\n",
    "        loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Epoch %d, Batch %d, Loss: %f, Train acc %f \" % (epoch, batch, loss_, acc_))\n",
    "    test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_images_reshape / 255.0, gt_placeholder: test_labels})\n",
    "    print (\"Epoch %d, Test Loss: %f, Test acc %f \" % (epoch, test_loss_, test_acc_))\n",
    "    #test_acc = evaluate_accuracy(test_data, net)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测\n",
    "训练完成后，现在我们可以演示对输入图片的标号的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1225361d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true labels\n",
      "['ankle boot', 'pullover', 'trouser', 'trouser', 'shirt', 'trouser', 'coat', 'shirt', 'sandal']\n",
      "predicted labels\n",
      "['ankle boot', 'pullover', 'trouser', 'trouser', 't-shirt', 'trouser', 'coat', 'coat', 'sandal']\n"
     ]
    }
   ],
   "source": [
    "data, label = test_images[0:9], test_labels[0:9]\n",
    "show_images(data)\n",
    "print('true labels')\n",
    "print(get_text_labels(np.argmax(label,axis=1)))\n",
    "\n",
    "predicted_softmax = sess.run([softmax_predict], feed_dict={input_placeholder:np.reshape(data/255.0,(9, image_height*image_width))})\n",
    "\n",
    "predicted_labels = np.argmax(predicted_softmax[0], axis=1)\n",
    "print('predicted labels')\n",
    "print(get_text_labels(predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
