{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃法（Dropout）— 使用Gluon\n",
    "本章介绍如何使用Gluon在训练和测试深度学习模型中使用丢弃法 (Dropout)。\n",
    "\n",
    "定义模型并添加丢弃层\n",
    "有了Gluon，我们模型的定义工作变得简单了许多。我们只需要在全连接层后添加gluon.nn.Dropout层并指定元素丢弃概率。一般情况下，我们推荐把 更靠近输入层的元素丢弃概率设的更小一点。这个试验中，我们把第一层全连接后的元素丢弃概率设为0.2，把第二层全连接后的元素丢弃概率设为0.5。\n",
    "\n",
    "实际中，此文中用tensorflow来代替，例如tf.nn.dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "num_inputs = 28*28\n",
    "num_outputs = 10\n",
    "\n",
    "num_hidden1 = 256\n",
    "num_hidden2 = 256\n",
    "\n",
    "#与gluon相反，dropout_prob越小代表抛弃的节点越多\n",
    "drop_prob1 = 1\n",
    "drop_prob2 = 1\n",
    "\n",
    "def net(X):\n",
    "    h1 = tf.contrib.layers.fully_connected(X, num_hidden1, activation_fn=tf.nn.relu, scope='hidden_1')\n",
    "    h1 = tf.nn.dropout(h1, drop_prob1)\n",
    "    h2 = tf.contrib.layers.fully_connected(h1, num_hidden2, activation_fn=tf.nn.relu, scope='hidden_2')\n",
    "    h2 = tf.nn.dropout(h2, drop_prob2)\n",
    "    h3 = tf.contrib.layers.fully_connected(h2, num_outputs, activation_fn=None, scope='output')\n",
    "    return h3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据并训练\n",
    "这跟之前没什么不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print train_labels.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "hidden_1/weights\n",
      "hidden_1/biases\n",
      "hidden_2/weights\n",
      "hidden_2/biases\n",
      "output/weights\n",
      "output/biases\n",
      "step 0, train loss 2.389092\n",
      "step 0, train acc 0.062500\n",
      "step 1000, train loss 0.294750\n",
      "step 1000, train acc 0.906250\n",
      "step 2000, train loss 0.467379\n",
      "step 2000, train acc 0.843750\n",
      "step 3000, train loss 0.101088\n",
      "step 3000, train acc 1.000000\n",
      "step 4000, train loss 0.423964\n",
      "step 4000, train acc 0.875000\n",
      "step 5000, train loss 0.294006\n",
      "step 5000, train acc 0.906250\n",
      "step 6000, train loss 0.416803\n",
      "step 6000, train acc 0.812500\n",
      "step 7000, train loss 0.111409\n",
      "step 7000, train acc 0.968750\n",
      "step 8000, train loss 0.302880\n",
      "step 8000, train acc 0.843750\n",
      "step 9000, train loss 0.109781\n",
      "step 9000, train acc 0.968750\n",
      "step 9999, test acc 0.875800\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "learning_rate = 1e-1\n",
    "max_steps = 10000\n",
    "batch_size = 32\n",
    "train_loss = 0.0\n",
    "train_acc = 0.0\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, num_inputs])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "logits = net(input_placeholder)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits,  onehot_labels=gt_placeholder)\n",
    "\n",
    "acc = utils.accuracy(logits , gt_placeholder)\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, num_inputs))\n",
    "    feed_dict = {input_placeholder: data.reshape((-1, num_inputs)), gt_placeholder: label}\n",
    "    loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)\n",
    "    if step % 1000 == 0:\n",
    "        print 'step %d, train loss %f' % (step, loss_)\n",
    "        print 'step %d, train acc %f' % (step, acc_)\n",
    "test_acc = sess.run(acc, feed_dict={input_placeholder: np.squeeze(test_images).reshape((-1, num_inputs)) / 255.0 , gt_placeholder: test_labels})\n",
    "print 'step %d, test acc %f' % (step, test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
