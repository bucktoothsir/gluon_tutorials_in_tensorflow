{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 丢弃法（Dropout）— 从0开始\n",
    "前面我们介绍了多层神经网络，就是包含至少一个隐含层的网络。我们也介绍了正则法来应对过拟合问题。在深度学习中，一个常用的应对过拟合问题的方法叫做丢弃法（Dropout）。本节以多层神经网络为例，从0开始介绍丢弃法。\n",
    "\n",
    "由于丢弃法的概念和实现非常容易，在本节中，我们先介绍丢弃法的概念以及它在现代神经网络中是如何实现的。然后我们一起探讨丢弃法的本质。\n",
    "\n",
    "### 丢弃法的概念\n",
    "在现代神经网络中，我们所指的丢弃法，通常是对输入层或者隐含层做以下操作：\n",
    "\n",
    "随机选择一部分该层的输出作为丢弃元素；\n",
    "把丢弃元素乘以0；\n",
    "把非丢弃元素拉伸。\n",
    "### 丢弃法的实现\n",
    "丢弃法的实现很容易，例如像下面这样。这里的标量`drop_probability`定义了一个`X`（`NDArray`类）中任何一个元素被丢弃的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def np_dropout(X, drop_probability):\n",
    "    keep_probability = 1 - drop_probability\n",
    "    assert 0 <= keep_probability <= 1\n",
    "    # 这种情况下把全部元素都丢弃。\n",
    "    if keep_probability == 0:\n",
    "        return np.zeros(X.shape)\n",
    "\n",
    "    # 随机选择一部分该层的输出作为丢弃元素。\n",
    "    mask = np.random.uniform(0, 1.0, X.shape) < keep_probability\n",
    "    # 保证 E[dropout(X)] == X\n",
    "    scale =  1 / keep_probability\n",
    "    return mask * X * scale, mask\n",
    "\n",
    "def tf_dropout(X, drop_probability):\n",
    "\n",
    "    keep_probability = 1 - drop_probability\n",
    "    shape = X.get_shape().as_list()\n",
    "    assert 0 <= keep_probability <= 1\n",
    "    # 这种情况下把全部元素都丢弃。\n",
    "    if keep_probability == 0:\n",
    "        return tf.zeros(shape)\n",
    "\n",
    "    # 随机选择一部分该层的输出作为丢弃元素。\n",
    "    mask = tf.to_float(np.random.uniform(0, 1.0, shape) < keep_probability)\n",
    "    # 保证 E[dropout(X)] == X\n",
    "    scale =  1 / keep_probability\n",
    "    return mask * X * scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]]), array([[ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(20).reshape((5,4))\n",
    "np_dropout(A, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  4.,  0.],\n",
       "        [ 0., 10.,  0.,  0.],\n",
       "        [16., 18.,  0., 22.],\n",
       "        [ 0., 26., 28., 30.],\n",
       "        [32., 34.,  0.,  0.]]), array([[False, False,  True, False],\n",
       "        [False,  True, False, False],\n",
       "        [ True,  True, False,  True],\n",
       "        [False,  True,  True,  True],\n",
       "        [ True,  True, False, False]]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_dropout(A, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_dropout(A, 1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 丢弃法的本质\n",
    "了解了丢弃法的概念与实现，那你可能对它的本质产生了好奇。\n",
    "\n",
    "如果你了解集成学习，你可能知道它在提升弱分类器准确率上的威力。一般来说，在集成学习里，我们可以对训练数据集有放回地采样若干次并分别训练若干个不同的分类器；测试时，把这些分类器的结果集成一下作为最终分类结果。\n",
    "\n",
    "事实上，丢弃法在模拟集成学习。试想，一个使用了丢弃法的多层神经网络本质上是原始网络的子集（节点和边）。举个例子，它可能长这个样子。\n",
    "![image.png](http://zh.gluon.ai/_images/dropout.png)\n",
    "我们在之前的章节里介绍过随机梯度下降算法：我们在训练神经网络模型时一般随机采样一个批量的训练数 据。丢弃法实质上是对每一个这样的数据集分别训练一个原神经网络子集的分类器。与一般的集成学习不同，这里每个原神经网络子集的分类器用的是同一套参数。因此丢弃法只是在模拟集成学习。\n",
    "\n",
    "我们刚刚强调了，原神经网络子集的分类器在不同的训练数据批量上训练并使用同一套参数。因此，使用丢弃法的神经网络实质上是对输入层和隐含层的参数做了正则化：学到的参数使得原神经网络不同子集在训练数据上都尽可能表现良好。\n",
    "\n",
    "下面我们动手实现一下在多层神经网络里加丢弃层。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据获取\n",
    "我们继续使用FashionMNIST数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print train_labels.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 含两个隐藏层的多层感知机\n",
    "多层感知机已经在之前章节里介绍。与之前章节不同，这里我们定义一个包含两个隐含层的模型，两个隐含层都输出256个节点。我们定义激活函数Relu并直接使用Gluon提供的交叉熵损失函数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_inputs = 28*28\n",
    "num_outputs = 10\n",
    "\n",
    "num_hidden1 = 256\n",
    "num_hidden2 = 256\n",
    "weight_scale = .1\n",
    "\n",
    "with tf.name_scope('multi_layer_percetron'):\n",
    "    W1 = tf.Variable(tf.random_normal([num_inputs, num_hidden1], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32), name='weights_hidden1')\n",
    "    b1 = tf.Variable(tf.constant(0.0, shape=[num_hidden1]), name='bias_hidden')\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden1, num_hidden2], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32), name='weights_hidden2')\n",
    "    b2 = tf.Variable(tf.constant(0.0, shape=[num_hidden2]), name='bias_output')\n",
    "    \n",
    "    W3 = tf.Variable(tf.random_normal([num_hidden2, num_outputs], mean=0.0, stddev=weight_scale, seed=None, dtype=tf.float32), name='weights_output')\n",
    "    b3 = tf.Variable(tf.constant(0.0, shape=[num_outputs]), name='bias_output')\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义包含丢弃层的模型\n",
    "我们的模型就是将层（全连接）和激活函数（Relu）串起来，并在应用激活函数后添加丢弃层。每个丢弃层的元素丢弃概率可以分别设置。一般情况下，我们推荐把更靠近输入层的元素丢弃概率设的更小一点。这个试验中，我们把第一层全连接后的元素丢弃概率设为0.2，把第二层全连接后的元素丢弃概率设为0.5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1 = 0.2\n",
    "drop_prob2 = 0.8\n",
    "\n",
    "def net(X):\n",
    "    is_training = True\n",
    "    # 第一层全连接。\n",
    "    h1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    # 在第一层全连接后添加丢弃层。\n",
    "    #h1 = tf_dropout(h1, drop_prob1)\n",
    "    h1 = tf.map_fn(lambda h: tf_dropout(h, drop_prob1), h1)\n",
    "    # 第二层全连接。\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
    "    # 在第二层全连接后添加丢弃层。\n",
    "    h2 = tf.map_fn(lambda h: tf_dropout(h, drop_prob2), h2)\n",
    "    #h2 = tf_dropout(h2, drop_prob2)\n",
    "    return tf.matmul(h2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练\n",
    "训练跟之前一样。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "multi_layer_percetron/weights_hidden1\n",
      "multi_layer_percetron/bias_hidden\n",
      "multi_layer_percetron/weights_hidden2\n",
      "multi_layer_percetron/bias_output\n",
      "multi_layer_percetron/weights_output\n",
      "multi_layer_percetron/bias_output_1\n",
      "step 0, train loss 4.047252\n",
      "step 0, train acc 0.093750\n",
      "step 1000, train loss 0.756845\n",
      "step 1000, train acc 0.750000\n",
      "step 2000, train loss 0.389689\n",
      "step 2000, train acc 0.875000\n",
      "step 3000, train loss 0.272325\n",
      "step 3000, train acc 0.875000\n",
      "step 4000, train loss 0.428824\n",
      "step 4000, train acc 0.781250\n",
      "step 5000, train loss 0.363320\n",
      "step 5000, train acc 0.843750\n",
      "step 6000, train loss 0.305298\n",
      "step 6000, train acc 0.875000\n",
      "step 7000, train loss 0.329539\n",
      "step 7000, train acc 0.906250\n",
      "step 8000, train loss 0.275392\n",
      "step 8000, train acc 0.906250\n",
      "step 9000, train loss 0.615244\n",
      "step 9000, train acc 0.781250\n",
      "step 9999, test acc 0.859900\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "max_steps = 10000\n",
    "batch_size = 32\n",
    "train_loss = 0.0\n",
    "train_acc = 0.0\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, num_inputs])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "logits = net(input_placeholder)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits,  onehot_labels=gt_placeholder)\n",
    "\n",
    "acc = utils.accuracy(logits , gt_placeholder)\n",
    "var_list = tf.trainable_variables()\n",
    "for var in var_list:\n",
    "    print var.op.name\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, num_inputs))\n",
    "    feed_dict = {input_placeholder: data.reshape((-1, num_inputs)), gt_placeholder: label}\n",
    "    loss_, acc_, _ = sess.run([loss, acc, train_op], feed_dict=feed_dict)\n",
    "    if step % 1000 == 0:\n",
    "        print 'step %d, train loss %f' % (step, loss_)\n",
    "        print 'step %d, train acc %f' % (step, acc_)\n",
    "test_acc = sess.run(acc, feed_dict={input_placeholder: np.squeeze(test_images).reshape((-1, num_inputs)) / 255.0 , gt_placeholder: test_labels})\n",
    "print 'step %d, test acc %f' % (step, test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
