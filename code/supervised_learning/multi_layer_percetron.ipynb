{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机 — 从0开始\n",
    "前面我们介绍了包括线性回归和多类逻辑回归的数个模型，它们的一个共同点是全是只含有一个输入层，一个输出层。这一节我们将介绍多层神经网络，就是包含至少一个隐含层的网络。\n",
    "\n",
    "### 数据获取\n",
    "我们继续使用FashionMNIST数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/fashion_mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/fashion_mnist/t10k-labels-idx1-ubyte.gz\n",
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../utils')\n",
    "import utils\n",
    "\n",
    "data_dir = '../../data/fashion_mnist'\n",
    "train_images, train_labels, test_images, test_labels = utils.load_data_fashion_mnist(data_dir, one_hot=True)\n",
    "print train_images.shape\n",
    "print train_labels.shape\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "train_dataset = DataSet(train_images, train_labels, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机\n",
    "多层感知机与前面介绍的多类逻辑回归非常类似，主要的区别是我们在输入层和输出层之间插入了一个到多个隐含层。\n",
    "\n",
    "![image.png](http://zh.gluon.ai/_images/multilayer-perceptron.png)\n",
    "这里我们定义一个只有一个隐含层的模型，这个隐含层输出256个节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 输入图片是28*28\n",
    "num_inputs = 28*28\n",
    "num_outputs = 10\n",
    "\n",
    "num_hidden = 256\n",
    "    \n",
    "with tf.name_scope('multi_layer_percetron'):\n",
    "    W1 = tf.Variable(tf.random_normal([num_inputs, num_hidden], mean=0.0, stddev=1.0, seed=None, dtype=tf.float32), name='weights_hidden')\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[num_hidden]), name='bias_hidden')\n",
    "    \n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden, num_outputs], mean=0.0, stddev=1.0, seed=None, dtype=tf.float32), name='weights_output')\n",
    "    b2 = tf.Variable(tf.constant(0.1, shape=[num_outputs]), name='bias_output')\n",
    "    \n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数\n",
    "如果我们就用线性操作符来构造多层神经网络，那么整个模型仍然只是一个线性函数。这是因为\n",
    "\n",
    "$\\hat{y} = X \\cdot W_1 \\cdot W_2 = X \\cdot W_3$\n",
    "\n",
    "这里$W_3 = W_1 \\cdot W_2$。为了让我们的模型可以拟合非线性函数，我们需要在层之间插入非线性的激活函数。这里我们使用ReLU\n",
    "\n",
    "$\\textrm{rel}u(x)=\\max(x, 0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return tf.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型\n",
    "我们的模型就是将层（全连接）和激活函数（Relu）串起来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X, params):\n",
    "    X = tf.reshape(X, (-1, num_inputs))\n",
    "    h1 = relu(tf.matmul(X, params[0]) + params[1])\n",
    "    output = tf.matmul(h1, params[2]) + params[3]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax和交叉熵损失函数\n",
    "在多类Logistic回归里我们提到分开实现Softmax和交叉熵损失函数可能导致数值不稳定。这里我们直接使用tensorflow提供的函数\n",
    "\n",
    "### 训练\n",
    "训练跟之前一样。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:718: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-9d64691a74b4>:14: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "multi_layer_percetron/weights_hidden\n",
      "multi_layer_percetron/bias_hidden\n",
      "multi_layer_percetron/weights_output\n",
      "multi_layer_percetron/bias_output\n",
      "Batch 0, Loss: 261.121490, Train acc 0.062500 \n",
      "Batch 100, Loss: 1.234798, Train acc 0.605469 \n",
      "Batch 200, Loss: 1.195346, Train acc 0.613281 \n",
      "Batch 300, Loss: 1.046810, Train acc 0.718750 \n",
      "Batch 400, Loss: 0.972769, Train acc 0.656250 \n",
      "Batch 500, Loss: 0.877846, Train acc 0.640625 \n",
      "Batch 600, Loss: 0.919124, Train acc 0.703125 \n",
      "Batch 700, Loss: 0.750194, Train acc 0.671875 \n",
      "Batch 800, Loss: 0.757320, Train acc 0.707031 \n",
      "Batch 900, Loss: 0.632691, Train acc 0.726562 \n",
      "Batch 1000, Loss: 0.693793, Train acc 0.714844 \n",
      "Batch 1100, Loss: 0.750490, Train acc 0.710938 \n",
      "Batch 1200, Loss: 0.614169, Train acc 0.746094 \n",
      "Batch 1300, Loss: 0.742209, Train acc 0.734375 \n",
      "Batch 1400, Loss: 0.712162, Train acc 0.726562 \n",
      "Batch 1500, Loss: 0.691213, Train acc 0.703125 \n",
      "Batch 1600, Loss: 0.835950, Train acc 0.695312 \n",
      "Batch 1700, Loss: 0.644071, Train acc 0.746094 \n",
      "Batch 1800, Loss: 0.673794, Train acc 0.750000 \n",
      "Batch 1900, Loss: 0.601166, Train acc 0.781250 \n",
      "Batch 2000, Loss: 0.734827, Train acc 0.746094 \n",
      "Batch 2100, Loss: 0.608941, Train acc 0.792969 \n",
      "Batch 2200, Loss: 0.771045, Train acc 0.675781 \n",
      "Batch 2300, Loss: 0.685248, Train acc 0.785156 \n",
      "Batch 2400, Loss: 0.594955, Train acc 0.781250 \n",
      "Batch 2500, Loss: 0.566162, Train acc 0.757812 \n",
      "Batch 2600, Loss: 0.566145, Train acc 0.800781 \n",
      "Batch 2700, Loss: 0.655017, Train acc 0.781250 \n",
      "Batch 2800, Loss: 0.563052, Train acc 0.808594 \n",
      "Batch 2900, Loss: 0.344137, Train acc 0.878906 \n",
      "Batch 3000, Loss: 0.497307, Train acc 0.804688 \n",
      "Batch 3100, Loss: 0.537377, Train acc 0.800781 \n",
      "Batch 3200, Loss: 0.445163, Train acc 0.847656 \n",
      "Batch 3300, Loss: 0.494914, Train acc 0.828125 \n",
      "Batch 3400, Loss: 0.585877, Train acc 0.800781 \n",
      "Batch 3500, Loss: 0.420064, Train acc 0.820312 \n",
      "Batch 3600, Loss: 0.527017, Train acc 0.828125 \n",
      "Batch 3700, Loss: 0.459927, Train acc 0.820312 \n",
      "Batch 3800, Loss: 0.601727, Train acc 0.792969 \n",
      "Batch 3900, Loss: 0.524615, Train acc 0.851562 \n",
      "Batch 4000, Loss: 0.686103, Train acc 0.781250 \n",
      "Batch 4100, Loss: 0.587251, Train acc 0.792969 \n",
      "Batch 4200, Loss: 0.519020, Train acc 0.824219 \n",
      "Batch 4300, Loss: 0.561460, Train acc 0.796875 \n",
      "Batch 4400, Loss: 0.473215, Train acc 0.835938 \n",
      "Batch 4500, Loss: 0.436232, Train acc 0.839844 \n",
      "Batch 4600, Loss: 0.406864, Train acc 0.839844 \n",
      "Batch 4700, Loss: 0.427148, Train acc 0.835938 \n",
      "Batch 4800, Loss: 0.594719, Train acc 0.742188 \n",
      "Batch 4900, Loss: 0.463770, Train acc 0.855469 \n",
      "Batch 5000, Loss: 0.443657, Train acc 0.843750 \n",
      "Batch 5100, Loss: 0.436278, Train acc 0.847656 \n",
      "Batch 5200, Loss: 0.486346, Train acc 0.859375 \n",
      "Batch 5300, Loss: 0.442632, Train acc 0.839844 \n",
      "Batch 5400, Loss: 0.517784, Train acc 0.808594 \n",
      "Batch 5500, Loss: 0.555607, Train acc 0.800781 \n",
      "Batch 5600, Loss: 0.442369, Train acc 0.832031 \n",
      "Batch 5700, Loss: 0.408071, Train acc 0.851562 \n",
      "Batch 5800, Loss: 0.413996, Train acc 0.839844 \n",
      "Batch 5900, Loss: 0.512019, Train acc 0.847656 \n",
      "Batch 6000, Loss: 0.422812, Train acc 0.847656 \n",
      "Batch 6100, Loss: 0.383913, Train acc 0.882812 \n",
      "Batch 6200, Loss: 0.380111, Train acc 0.832031 \n",
      "Batch 6300, Loss: 0.536585, Train acc 0.808594 \n",
      "Batch 6400, Loss: 0.450749, Train acc 0.855469 \n",
      "Batch 6500, Loss: 0.476224, Train acc 0.824219 \n",
      "Batch 6600, Loss: 0.456901, Train acc 0.832031 \n",
      "Batch 6700, Loss: 0.440804, Train acc 0.851562 \n",
      "Batch 6800, Loss: 0.594854, Train acc 0.816406 \n",
      "Batch 6900, Loss: 0.402954, Train acc 0.859375 \n",
      "Batch 7000, Loss: 0.430073, Train acc 0.863281 \n",
      "Batch 7100, Loss: 0.421042, Train acc 0.843750 \n",
      "Batch 7200, Loss: 0.405654, Train acc 0.855469 \n",
      "Batch 7300, Loss: 0.404805, Train acc 0.875000 \n",
      "Batch 7400, Loss: 0.373921, Train acc 0.867188 \n",
      "Batch 7500, Loss: 0.461404, Train acc 0.839844 \n",
      "Batch 7600, Loss: 0.431508, Train acc 0.863281 \n",
      "Batch 7700, Loss: 0.568962, Train acc 0.773438 \n",
      "Batch 7800, Loss: 0.536479, Train acc 0.804688 \n",
      "Batch 7900, Loss: 0.475493, Train acc 0.808594 \n",
      "Batch 8000, Loss: 0.460220, Train acc 0.839844 \n",
      "Batch 8100, Loss: 0.578426, Train acc 0.808594 \n",
      "Batch 8200, Loss: 0.521947, Train acc 0.804688 \n",
      "Batch 8300, Loss: 0.533850, Train acc 0.804688 \n",
      "Batch 8400, Loss: 0.586353, Train acc 0.832031 \n",
      "Batch 8500, Loss: 0.364618, Train acc 0.871094 \n",
      "Batch 8600, Loss: 0.401460, Train acc 0.824219 \n",
      "Batch 8700, Loss: 0.392742, Train acc 0.843750 \n",
      "Batch 8800, Loss: 0.530426, Train acc 0.800781 \n",
      "Batch 8900, Loss: 0.503568, Train acc 0.843750 \n",
      "Batch 9000, Loss: 0.504474, Train acc 0.804688 \n",
      "Batch 9100, Loss: 0.388389, Train acc 0.863281 \n",
      "Batch 9200, Loss: 0.437542, Train acc 0.839844 \n",
      "Batch 9300, Loss: 0.623527, Train acc 0.769531 \n",
      "Batch 9400, Loss: 0.424159, Train acc 0.863281 \n",
      "Batch 9500, Loss: 0.413713, Train acc 0.847656 \n",
      "Batch 9600, Loss: 0.400961, Train acc 0.867188 \n",
      "Batch 9700, Loss: 0.411586, Train acc 0.855469 \n",
      "Batch 9800, Loss: 0.506279, Train acc 0.808594 \n",
      "Batch 9900, Loss: 0.334106, Train acc 0.878906 \n",
      "Test Loss: 0.552969, Test acc 0.833400 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "learning_rate = 1e0\n",
    "max_steps = 10000\n",
    "batch_size = 256\n",
    "\n",
    "input_placeholder = tf.placeholder(tf.float32, [None, num_inputs])\n",
    "gt_placeholder = tf.placeholder(tf.int64, [None, num_outputs])\n",
    "logits = net(input_placeholder, params)\n",
    "loss = tf.losses.softmax_cross_entropy(logits=logits,  onehot_labels=gt_placeholder)\n",
    "acc = utils.accuracy(logits, gt_placeholder)\n",
    "test_images_reshape = np.reshape(np.squeeze(test_images), (test_images.shape[0], num_inputs))\n",
    "\n",
    "for var in tf.all_variables():\n",
    "    print var.op.name\n",
    "    \n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(max_steps):\n",
    "    data, label = train_dataset.next_batch(batch_size)\n",
    "    data = np.reshape(data, (batch_size, num_inputs))\n",
    "    feed_dict = {input_placeholder: data, gt_placeholder: label}\n",
    "    b1_, loss_, acc_, _ = sess.run([b1, loss, acc, train_op], feed_dict=feed_dict)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Batch %d, Loss: %f, Train acc %f \" % (step, loss_, acc_))\n",
    "        test_loss_, test_acc_ = sess.run([loss, acc], feed_dict={input_placeholder: test_images_reshape / 255.0, gt_placeholder: test_labels})\n",
    "        print (\"Test Loss: %f, Test acc %f \" % (test_loss_, test_acc_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
