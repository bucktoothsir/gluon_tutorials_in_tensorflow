{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad——从零开始\n",
    "在我们之前介绍过的优化算法中，无论是梯度下降、随机梯度下降、小批量随机梯度下降还是使用动量法，目标函数自变量的每一个元素在相同时刻都使用同一个学习率来自我迭代。\n",
    "\n",
    "举个例子，假设目标函数为$f$，自变量为一个多维向量$[x_1, x_2]^\\top$，该向量中每一个元素在更新时都使用相同的学习率。例如在学习率为$\\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\\eta$来自我迭代：\n",
    "\n",
    "$\\begin{split}x_1 \\leftarrow x_1 - \\eta \\frac{\\partial{f}}{\\partial{x_1}}, \\\\\n",
    "x_2 \\leftarrow x_2 - \\eta \\frac{\\partial{f}}{\\partial{x_2}}.\\end{split}$.\n",
    "\n",
    "如果让$x_1$和$x_2$使用不同的学习率自我迭代呢？实际上，Adagrad就是一个在迭代过程中不断自我调整学习率，并让模型参数中每个元素都使用不同学习率的优化算法。\n",
    "\n",
    "下面，我们将介绍Adagrad算法。关于本节中涉及到的按元素运算，例如标量与向量计算以及按元素相乘$\\odot$，请参见“数学基础”一节。\n",
    "\n",
    "### Adagrad算法\n",
    "Adagrad的算法会使用一个小批量随机梯度按元素平方的累加变量$\\boldsymbol{s}$，并将其中每个元素初始化为0。在每次迭代中，首先计算小批量随机梯度$\\boldsymbol{g}$，然后将该梯度按元素平方后累加到变量$\\boldsymbol{s}$：\n",
    "\n",
    "$\\boldsymbol{s} \\leftarrow \\boldsymbol{s} + \\boldsymbol{g} \\odot \\boldsymbol{g}.$\n",
    "\n",
    "然后，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：\n",
    "\n",
    "$\\boldsymbol{g}^\\prime \\leftarrow \\frac{\\eta}{\\sqrt{\\boldsymbol{s} + \\epsilon}} \\odot \\boldsymbol{g},$\n",
    "\n",
    "其中$\\eta$是初始学习率且$\\eta>0$，$\\epsilon$是为了维持数值稳定性而添加的常数，例如$10^{-7}$。我们需要注意其中按元素开方、除法和乘法的运算。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。\n",
    "\n",
    "最后，自变量的迭代步骤与小批量随机梯度下降类似。只是这里梯度前的学习率已经被调整过了：\n",
    "\n",
    "$\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\boldsymbol{g}^\\prime.$\n",
    "\n",
    "### Adagrad的特点\n",
    "需要强调的是，小批量随机梯度按元素平方的累加变量$\\boldsymbol{s}$出现在含调整后学习率的梯度$\\boldsymbol{g}^\\prime$的分母项。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么就让该元素的学习率下降快一点；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么就让该元素的学习率下降慢一点。然而，由于$\\boldsymbol{s}$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。\n",
    "\n",
    "### Adagrad的实现\n",
    "Adagrad的实现很简单。我们只需要把上面的数学公式翻译成代码。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(params, grads, sqrs, lr, batch_size):\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    for param, grad, sqr in zip(params, grads, sqrs):\n",
    "        sqr += np.dot(grad, grad)\n",
    "        param[:] -= lr * grad / np.sqrt(sqr + 1e-12)\n",
    "    return params, sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验\n",
    "首先，导入本节中实验所需的包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验中，我们以之前介绍过的线性回归为例。设数据集的样本数为1000，我们使用权重w为[2, -3.4]，偏差b为4.2的线性回归模型来生成数据集。该模型的平方损失函数即所需优化的目标函数，模型参数即目标函数自变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据集。\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "X = np.random.normal(scale=1, size=(num_examples, num_inputs))\n",
    "y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b\n",
    "y += 0.01 * np.random.normal(scale=1, size=y.shape)\n",
    "\n",
    "# 初始化模型参数。\n",
    "def init_params():\n",
    "    w = np.random.normal(scale=1, size=(num_inputs, 1))\n",
    "    b = np.zeros((1,))\n",
    "    sqrs = []\n",
    "    params = [w, b]\n",
    "    for param in params:\n",
    "        sqrs.append(np.zeros(param.shape))\n",
    "    return params, sqrs\n",
    "\n",
    "# 构造迭代器。\n",
    "def data_iter(batch_size):\n",
    "    idx = list(range(num_examples))\n",
    "    random.shuffle(idx)\n",
    "    for batch_i, i in enumerate(range(0, num_examples, batch_size)):\n",
    "        j = np.array(idx[i: min(i + batch_size, num_examples)])\n",
    "        yield batch_i, X[j], y[j]\n",
    "\n",
    "# 线性回归模型。\n",
    "def net(X, w, b):\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "# 损失函数。\n",
    "def square_loss(yhat, y):\n",
    "    return (yhat - np.reshape(y, yhat.shape)) ** 2 / 2\n",
    "\n",
    "def cal_grad(yhat, y, X):\n",
    "    grad_w = np.expand_dims(np.array([-np.dot((yhat - np.reshape(y, yhat.shape)).T, X[:, 0]), -np.dot((yhat - np.reshape(y, yhat.shape)).T, X[:, 1])]), axis=1)\n",
    "    #grad_w = np.array([-np.dot((yhat - np.reshape(y, yhat.shape)).T, X[:, 0])])\n",
    "    grad_b = -np.sum((yhat - np.reshape(y, yhat.shape)))\n",
    "    return [grad_w, grad_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化函数optimize与“梯度下降和随机梯度下降——从零开始”一节中的类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(batch_size, lr, epochs, period):\n",
    "    [w, b], sqrs = init_params()\n",
    "    total_loss = [np.mean(square_loss(net(X, w, b), y))]\n",
    "    # 注意epoch从1开始计数。\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # 学习率自我衰减。\n",
    "        if epoch > 2:\n",
    "            lr *= 0.1\n",
    "        for batch_i, data, label in data_iter(batch_size):\n",
    "            output = net(data, w, b)\n",
    "            loss = square_loss(output, label)\n",
    "            grad =  cal_grad(label, output, data)\n",
    "            [w, b], sqrs = adagrad([w, b], grad, sqrs, lr, batch_size)\n",
    "            if batch_i * batch_size % period == 0:\n",
    "                total_loss.append(\n",
    "                    np.mean(square_loss(net(X, w, b), y)))\n",
    "        print(\"Batch size %d, Learning rate %f, Epoch %d, loss %.4e\" %\n",
    "              (batch_size, lr, epoch, total_loss[-1]))\n",
    "    print('w:', np.reshape(w, (1, -1)),\n",
    "          'b:', b, '\\n')\n",
    "    x_axis = np.linspace(0, epochs, len(total_loss), endpoint=True)\n",
    "    mpl.rcParams['figure.figsize'] = 3.5, 2.5\n",
    "    plt.semilogy(x_axis, total_loss)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终，优化所得的模型参数值与它们的真实值较接近。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-13-3a0ffe61d08f>(4)adagrad()\n",
      "-> for param, grad, sqr in zip(params, grads, sqrs):\n",
      "(Pdb) n\n",
      "> <ipython-input-13-3a0ffe61d08f>(5)adagrad()\n",
      "-> sqr += grad * grad\n",
      "(Pdb) sqr\n",
      "array([[0.],\n",
      "       [0.]])\n",
      "(Pdb) grad\n",
      "array([[-95.23831134],\n",
      "       [132.88585019]])\n",
      "(Pdb) grad * grad.shape\n",
      "array([[-190.47662269,  -95.23831134],\n",
      "       [ 265.77170039,  132.88585019]])\n"
     ]
    }
   ],
   "source": [
    "train(batch_size=10, lr=0.9, epochs=3, period=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
